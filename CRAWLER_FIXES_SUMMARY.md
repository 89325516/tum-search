# çˆ¬è™«ç¼ºé™·ä¿®å¤æ€»ç»“

## âœ… å·²ä¿®å¤çš„ç¼ºé™·

### ğŸ”´ ä¸¥é‡ç¼ºé™·ä¿®å¤

#### 1. **é‡å®šå‘æ— é™å¾ªç¯é£é™©** âœ…
**ä¿®å¤ä½ç½®**: `OptimizedCrawler.fetch()` æ–¹æ³•

**ä¿®å¤å†…å®¹**:
- æ·»åŠ  `max_redirects` å‚æ•°ï¼ˆé»˜è®¤5ï¼‰
- æ·»åŠ é‡å®šå‘æ·±åº¦è·Ÿè¸ªï¼ˆ`redirect_count`ï¼‰
- æ·»åŠ é‡å®šå‘å†å²è®°å½•ï¼ˆ`redirect_history`ï¼‰æ£€æµ‹å¾ªç¯
- è§„èŒƒåŒ–é‡å®šå‘URLå¹¶éªŒè¯æœ‰æ•ˆæ€§

**ä»£ç æ”¹è¿›**:
```python
async def fetch(self, session, url, redirect_count=0, redirect_history=None):
    # æ£€æŸ¥é‡å®šå‘æ·±åº¦
    if redirect_count >= self.max_redirects:
        return None
    
    # æ£€æŸ¥é‡å®šå‘å¾ªç¯
    if url in redirect_history:
        return None
```

---

#### 2. **çº¿ç¨‹å®‰å…¨é—®é¢˜** âœ…
**ä¿®å¤ä½ç½®**: `_rate_limit()` å’Œ `_domain_delay()` æ–¹æ³•

**ä¿®å¤å†…å®¹**:
- ä½¿ç”¨ `asyncio.Lock` ä¿æŠ¤å…±äº«çŠ¶æ€
- æ·»åŠ  `_rate_limit_lock`ã€`_domain_delay_lock`ã€`_last_url_lock`
- ç¡®ä¿å¹¶å‘ç¯å¢ƒä¸‹çš„æ•°æ®ä¸€è‡´æ€§

**ä»£ç æ”¹è¿›**:
```python
self._rate_limit_lock = asyncio.Lock()
self._domain_delay_lock = asyncio.Lock()
self._last_url_lock = asyncio.Lock()

async def _rate_limit(self):
    async with self._rate_limit_lock:
        # çº¿ç¨‹å®‰å…¨çš„é€Ÿç‡é™åˆ¶é€»è¾‘
```

---

#### 3. **SSLéªŒè¯æ§åˆ¶** âœ…
**ä¿®å¤ä½ç½®**: `OptimizedCrawler.__init__()` å’Œ `fetch()` æ–¹æ³•

**ä¿®å¤å†…å®¹**:
- æ·»åŠ  `verify_ssl` å‚æ•°ï¼ˆé»˜è®¤Trueï¼‰
- ç”Ÿäº§ç¯å¢ƒé»˜è®¤å¯ç”¨SSLéªŒè¯
- å¼€å‘ç¯å¢ƒå¯ä»¥ç¦ç”¨ï¼ˆé€šè¿‡å‚æ•°æ§åˆ¶ï¼‰

**ä»£ç æ”¹è¿›**:
```python
def __init__(self, ..., verify_ssl=True):
    self.verify_ssl = verify_ssl

async def fetch(...):
    async with session.get(..., ssl=self.verify_ssl, ...):
```

---

#### 4. **äº‹ä»¶å¾ªç¯å†²çª** âœ…
**ä¿®å¤ä½ç½®**: `OptimizedCrawler.parse()` æ–¹æ³•

**ä¿®å¤å†…å®¹**:
- ä½¿ç”¨ `asyncio.get_running_loop()` æ›¿ä»£ `asyncio.get_event_loop()`
- æ­£ç¡®å¤„ç†å·²æœ‰äº‹ä»¶å¾ªç¯çš„æƒ…å†µ
- æ·»åŠ è¶…æ—¶ä¿æŠ¤

**ä»£ç æ”¹è¿›**:
```python
try:
    loop = asyncio.get_running_loop()
    # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†
except RuntimeError:
    # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
    results = asyncio.run(self.run([url]))
```

---

#### 5. **èµ„æºæ¸…ç†æ”¹è¿›** âœ…
**ä¿®å¤ä½ç½®**: æ·»åŠ  `close()` æ–¹æ³•å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¯æŒ

**ä¿®å¤å†…å®¹**:
- æ·»åŠ æ˜¾å¼çš„ `close()` æ–¹æ³•
- å®ç°ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆ`__enter__` å’Œ `__exit__`ï¼‰
- æ”¹è¿› `__del__` æ–¹æ³•çš„é”™è¯¯å¤„ç†

**ä»£ç æ”¹è¿›**:
```python
def close(self):
    if hasattr(self, 'executor'):
        self.executor.shutdown(wait=True)

def __enter__(self):
    return self

def __exit__(self, exc_type, exc_val, exc_tb):
    self.close()
    return False
```

---

### ğŸŸ¡ ä¸­ç­‰ç¼ºé™·ä¿®å¤

#### 6. **URLè§„èŒƒåŒ–** âœ…
**ä¿®å¤ä½ç½®**: æ·»åŠ  `_normalize_url()` æ–¹æ³•

**ä¿®å¤å†…å®¹**:
- ç§»é™¤URL fragmentï¼ˆ#ï¼‰
- å¤„ç†ç›¸å¯¹è·¯å¾„ï¼ˆ`./` å’Œ `../`ï¼‰
- è§„èŒƒåŒ–è·¯å¾„ç»“æ„
- ç»Ÿä¸€URLæ ¼å¼

**ä»£ç æ”¹è¿›**:
```python
def _normalize_url(self, url):
    # ç§»é™¤fragment
    url = url.split('#')[0]
    # è§„èŒƒåŒ–è·¯å¾„
    # å¤„ç†./å’Œ../
    # é‡å»ºURL
```

---

#### 7. **é“¾æ¥è¿‡æ»¤æ”¹è¿›** âœ…
**ä¿®å¤ä½ç½®**: `extract_content_smart()` å’Œ `SmartCrawler.parse()`

**ä¿®å¤å†…å®¹**:
- è¿‡æ»¤ `javascript:`, `mailto:`, `tel:`, `data:`, `file:` ç­‰æ— æ•ˆé“¾æ¥
- éªŒè¯URLæœ‰æ•ˆæ€§
- è§„èŒƒåŒ–æ‰€æœ‰æå–çš„é“¾æ¥

**ä»£ç æ”¹è¿›**:
```python
# è¿‡æ»¤æ— æ•ˆåè®®
if href.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
    continue
```

---

#### 8. **BeautifulSoupè§£æå™¨å›é€€** âœ…
**ä¿®å¤ä½ç½®**: `_parse_sync()` å’Œ `SmartCrawler.parse()`

**ä¿®å¤å†…å®¹**:
- ä¼˜å…ˆä½¿ç”¨ `lxml` è§£æå™¨ï¼ˆæ›´å¿«ï¼‰
- å¦‚æœ `lxml` ä¸å¯ç”¨ï¼Œè‡ªåŠ¨å›é€€åˆ° `html.parser`
- ç¡®ä¿åœ¨æ‰€æœ‰ç¯å¢ƒä¸‹éƒ½èƒ½å·¥ä½œ

**ä»£ç æ”¹è¿›**:
```python
try:
    soup = BeautifulSoup(html, 'lxml')
except Exception:
    logger.debug("lxml parser failed, falling back to html.parser")
    soup = BeautifulSoup(html, 'html.parser')
```

---

#### 9. **è¾“å…¥éªŒè¯** âœ…
**ä¿®å¤ä½ç½®**: æ‰€æœ‰å…¬å…±æ–¹æ³•

**ä¿®å¤å†…å®¹**:
- éªŒè¯URLæ ¼å¼
- éªŒè¯URLé•¿åº¦ï¼ˆæœ€å¤§2048å­—ç¬¦ï¼‰
- éªŒè¯å‚æ•°ç±»å‹
- å¤„ç†Noneå’Œç©ºå­—ç¬¦ä¸²

**ä»£ç æ”¹è¿›**:
```python
def _is_valid_url(self, url):
    if not url or len(url) > 2048:
        return False
    # éªŒè¯scheme
    # è¿‡æ»¤æ— æ•ˆåè®®
```

---

#### 10. **å›¾ç‰‡æ‰©å±•åæ£€æŸ¥æ”¹è¿›** âœ…
**ä¿®å¤ä½ç½®**: `extract_content_smart()` å’Œ `SmartCrawler.parse()` æ–¹æ³•

**ä¿®å¤å†…å®¹**:
- æ”¹è¿›æ‰©å±•åæå–é€»è¾‘
- æ­£ç¡®å¤„ç†URLå‚æ•°å’Œfragmentï¼ˆä½¿ç”¨ `.split('?')[0].split('#')[0]`ï¼‰
- æ”¯æŒæ›´å¤šå›¾ç‰‡æ ¼å¼

**ä»£ç æ”¹è¿›**:
```python
# æ”¹è¿›çš„æ‰©å±•åæå–ï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œfragment
ext = full_url.split('.')[-1].lower().split('?')[0].split('#')[0]
```

---

#### 11. **ç¼–ç æ£€æµ‹æ”¹è¿›** âœ…
**ä¿®å¤ä½ç½®**: `SmartCrawler.parse()` å’Œ `OptimizedCrawler.fetch()` æ–¹æ³•

**ä¿®å¤å†…å®¹**:
- SmartCrawler: æ”¹è¿›ç¼–ç æ£€æµ‹ï¼Œå°è¯•å¤šç§å¸¸è§ç¼–ç ï¼ˆutf-8, latin-1, iso-8859-1, cp1252ï¼‰
- OptimizedCrawler: æ·»åŠ ç¼–ç é”™è¯¯å¤„ç†ï¼Œå¦‚æœaiohttpè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œæ‰‹åŠ¨å°è¯•å¤šç§ç¼–ç 
- ä½¿ç”¨ `errors='replace'` æ›¿ä»£ `errors='ignore'`ï¼Œé¿å…é™é»˜å¿½ç•¥é”™è¯¯

**ä»£ç æ”¹è¿›**:
```python
# SmartCrawler: å°è¯•å¤šç§ç¼–ç 
encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
for encoding in encodings:
    try:
        html = response.content.decode(encoding)
        break
    except (UnicodeDecodeError, LookupError):
        continue

# OptimizedCrawler: aiohttpç¼–ç é”™è¯¯å¤„ç†
try:
    return await response.text()
except UnicodeDecodeError:
    # æ‰‹åŠ¨å°è¯•å¤šç§ç¼–ç 
    content = await response.read()
    # ... å°è¯•å¤šç§ç¼–ç  ...
```

---

#### 12. **åˆ é™¤æœªä½¿ç”¨çš„å˜é‡** âœ…
**ä¿®å¤ä½ç½®**: `OptimizedCrawler.__init__()` æ–¹æ³•

**ä¿®å¤å†…å®¹**:
- åˆ é™¤æœªä½¿ç”¨çš„ `MIN_TEXT_DENSITY` å˜é‡
- æ¸…ç†å†—ä½™ä»£ç 

---

## ğŸ“Š ä¿®å¤ç»Ÿè®¡

- âœ… **ä¸¥é‡ç¼ºé™·**: 5ä¸ªå…¨éƒ¨ä¿®å¤
- âœ… **ä¸­ç­‰ç¼ºé™·**: 9ä¸ªå…¨éƒ¨ä¿®å¤ï¼ˆåŒ…æ‹¬ç¼–ç æ£€æµ‹ã€å›¾ç‰‡æ‰©å±•åæ£€æŸ¥ç­‰ï¼‰
- âœ… **è½»å¾®ç¼ºé™·**: 2ä¸ªä¿®å¤ï¼ˆåˆ é™¤æœªä½¿ç”¨å˜é‡ã€èµ„æºæ¸…ç†æ”¹è¿›ï¼‰

## ğŸ¯ æ–°å¢åŠŸèƒ½

1. **ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¯æŒ**
   ```python
   with OptimizedCrawler() as crawler:
       results = await crawler.run(urls)
   ```

2. **å¯é…ç½®çš„SSLéªŒè¯**
   ```python
   crawler = OptimizedCrawler(verify_ssl=True)  # ç”Ÿäº§ç¯å¢ƒ
   crawler = OptimizedCrawler(verify_ssl=False)  # å¼€å‘ç¯å¢ƒ
   ```

3. **å¯é…ç½®çš„é‡å®šå‘æ·±åº¦**
   ```python
   crawler = OptimizedCrawler(max_redirects=10)
   ```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **SSLéªŒè¯**: ç”Ÿäº§ç¯å¢ƒå»ºè®®ä¿æŒ `verify_ssl=True`ï¼ˆé»˜è®¤å€¼ï¼‰
2. **é‡å®šå‘æ·±åº¦**: é»˜è®¤5æ¬¡ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
3. **èµ„æºæ¸…ç†**: æ¨èä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨æˆ–æ˜¾å¼è°ƒç”¨ `close()`
4. **å¹¶å‘å®‰å…¨**: ç°åœ¨æ‰€æœ‰å…±äº«çŠ¶æ€éƒ½æœ‰é”ä¿æŠ¤ï¼Œå¯ä»¥å®‰å…¨å¹¶å‘ä½¿ç”¨

## ğŸ”„ å‘åå…¼å®¹æ€§

æ‰€æœ‰ä¿®å¤éƒ½ä¿æŒäº†å‘åå…¼å®¹æ€§ï¼š
- `SmartCrawler` æ¥å£å®Œå…¨ä¸å˜
- `OptimizedCrawler` çš„é»˜è®¤è¡Œä¸ºä¸å˜
- æ–°å¢å‚æ•°éƒ½æœ‰åˆç†çš„é»˜è®¤å€¼

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨ï¼ˆä¿®å¤åï¼‰
```python
from crawler import OptimizedCrawler
import asyncio

# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ¨èï¼‰
with OptimizedCrawler(concurrency=3, delay=1.5, max_rate=3.0) as crawler:
    results = asyncio.run(crawler.run(urls))
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®
```python
crawler = OptimizedCrawler(
    concurrency=3,
    delay=1.5,
    max_rate=3.0,
    verify_ssl=True,      # å¯ç”¨SSLéªŒè¯
    max_redirects=5       # é™åˆ¶é‡å®šå‘æ·±åº¦
)
```

### å¼€å‘ç¯å¢ƒé…ç½®
```python
crawler = OptimizedCrawler(
    concurrency=5,
    delay=0.5,
    verify_ssl=False,     # å¼€å‘ç¯å¢ƒå¯ç¦ç”¨
    max_redirects=10
)
```

## âœ… æµ‹è¯•å»ºè®®

å»ºè®®æµ‹è¯•ä»¥ä¸‹åœºæ™¯ï¼š
1. é‡å®šå‘å¾ªç¯æ£€æµ‹
2. å¹¶å‘ç¯å¢ƒä¸‹çš„é€Ÿç‡é™åˆ¶
3. å¤§é‡URLçš„æ‰¹é‡å¤„ç†
4. æ— æ•ˆURLçš„å¤„ç†
5. èµ„æºæ¸…ç†ï¼ˆå†…å­˜æ³„æ¼æ£€æŸ¥ï¼‰

