import numpy as np
import uuid
import time
import random
import logging
from typing import Optional, Dict, List
from qdrant_client import QdrantClient
from qdrant_client.http import models
import google.generativeai as genai
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
from crawler import SmartCrawler  # ç¡®ä¿ crawler.py åœ¨åŒçº§ç›®å½•ä¸‹

logger = logging.getLogger(__name__)

# ================= é…ç½®åŒº =================
# ğŸ”´ ä½ çš„çœŸå®é…ç½®
import os
from dotenv import load_dotenv

load_dotenv()

# ä¿®æ”¹å‰ï¼š
# QDRANT_URL = "https://..."
# QDRANT_API_KEY = "ey..."

# ä¿®æ”¹åï¼š
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

# Configure Gemini
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    print("âš ï¸ GOOGLE_API_KEY not found in .env. Summarization will be disabled.")

SPACE_R = "tum_space_r"
SPACE_X = "tum_space_x"

# é˜ˆå€¼è®¾å®š
NOVELTY_THRESHOLD = 0.2  # è·ç¦»å¤§äº 0.2 (ç›¸ä¼¼åº¦ < 0.8) è§†ä¸ºç‹¬ç‰¹ï¼Œè‡ªåŠ¨æ™‹å‡
# =========================================

print("ğŸ› ï¸System Initialization: Connecting to database & loading models...")
client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
crawler = SmartCrawler()
from interaction_manager import InteractionManager

def get_embedding(text=None, image_path=None):
    inputs = None
    if text:
        inputs = clip_processor(text=[text], return_tensors="pt", padding=True, truncation=True, max_length=77)
        feat = clip_model.get_text_features(**inputs)
    elif image_path:
        try:
            # å¦‚æœæ˜¯URLå›¾ç‰‡ï¼Œéœ€è¦å…ˆä¸‹è½½ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå…¼å®¹æœ¬åœ°è·¯å¾„
            image = Image.open(image_path).convert("RGB")
            inputs = clip_processor(images=image, return_tensors="pt")
            feat = clip_model.get_image_features(**inputs)
        except Exception as e:
            return None
    if inputs is not None:
        feat = feat / feat.norm(p=2, dim=-1, keepdim=True)
        return feat[0].detach().numpy().tolist()
    return None


class SystemManager:
    def __init__(self):
        self.client = client
        self.r_cache = []
        self.r_ranks = {}
        # HNSW ç«‹ä½“ç»“æ„å‚æ•°
        self.max_level = 3
        self.m_neighbors = 5
        self.interaction_mgr = InteractionManager()
        self.crawler = crawler
        
        # Initialize Gemini Model
        print("ğŸ§  Initializing Gemini API...")
        self.model = genai.GenerativeModel('gemini-pro')
        
        self._init_collections()
        self._ensure_indices()

    def _init_collections(self):
        """åˆå§‹åŒ– Qdrant é›†åˆ"""
        for name in [SPACE_X, SPACE_R]:
            if not self.client.collection_exists(name):
                self.client.create_collection(
                    collection_name=name,
                    vectors_config={
                        "clip": models.VectorParams(size=512, distance=models.Distance.COSINE)
                    }
                )
                print(f"âœ… Collection {name} created successfully!")

    def _ensure_indices(self):
        """Ensure necessary payload indices exist."""
        try:
            self.client.create_payload_index(
                collection_name=SPACE_X,
                field_name="url",
                field_schema=models.PayloadSchemaType.KEYWORD
            )
            print(f"âœ… Index ensured for {SPACE_X}: url")
        except Exception as e:
            # Index might already exist
            pass
            
        try:
            self.client.create_payload_index(
                collection_name=SPACE_R,
                field_name="url",
                field_schema=models.PayloadSchemaType.KEYWORD
            )
            print(f"âœ… Index ensured for {SPACE_R}: url")
        except Exception as e:
            pass
            
        try:
            self.client.create_payload_index(
                collection_name=SPACE_X,
                field_name="is_summarized",
                field_schema=models.PayloadSchemaType.KEYWORD
            )
            print("âœ… Index ensured for tum_space_x: is_summarized")
        except Exception:
            pass

    def get_text_embedding(self, text):
        """Wrapper for global get_embedding function."""
        return get_embedding(text=text)

    def trigger_global_recalculation(self):
        """è§¦å‘åŸºäº HNSW ç»“æ„çš„ç«‹ä½“ PageRank è®¡ç®—"""
        print("\nâš¡ï¸ Triggering 3D Network Recalculation (HNSW-based Recalculation) âš¡ï¸")

        # 1. æ‹‰å– R ç©ºé—´æ•°æ®
        r_points = []
        offset = None
        while True:
            batch, offset = client.scroll(collection_name=SPACE_R, limit=100, with_vectors=True, offset=offset)
            r_points.extend(batch)
            if offset is None: break

        if not r_points:
            print("   âš ï¸ Space R is empty, no calculation needed.")
            return

        self.r_cache = r_points
        print(f"   -> Space R Total Nodes: {len(r_points)}")

        # 2. æ„å»ºç«‹ä½“å›¾å¹¶è®¡ç®— PR
        self._calculate_hnsw_pagerank(r_points)

        # 3. æ›´æ–° Space X (æŠ•å½±)
        self._update_space_x_scores()

    def _calculate_hnsw_pagerank(self, points):
        """
        æ„å»º HNSW ç«‹ä½“åˆ†å±‚å›¾å¹¶è®¡ç®— PageRank (Rust Accelerated)
        """
        try:
            import visual_rank_engine
        except ImportError:
            print("âŒ Error: visual_rank_engine not found. Please build the Rust extension.")
            return

        n = len(points)
        if n == 0: return
        
        print(f"   âš¡ï¸ Rust Engine: Calculating PageRank for {n} nodes...")
        start_time = time.time()

        # 1. Prepare Data
        # Ensure IDs are strings
        ids = [str(p.id) for p in points]
        vectors = [p.vector['clip'] for p in points]
        
        # Cold start check
        if not self.interaction_mgr.interactions:
            self.interaction_mgr.simulate_cold_start_data(points)

        # Interaction Weights (Fix: Use ID instead of URL)
        interaction_weights = {}
        for p in points:
            w = self.interaction_mgr.get_interaction_weight(str(p.id))
            interaction_weights[str(p.id)] = w
            
        # Transitions (Convert defaultdict to dict for safety)
        # InteractionManager.transitions is defaultdict(lambda: defaultdict(int))
        transitions = {k: dict(v) for k, v in self.interaction_mgr.transitions.items()}

        # 2. Call Rust
        try:
            ranks = visual_rank_engine.calculate_hnsw_pagerank(
                ids,
                vectors,
                interaction_weights,
                transitions,
                self.m_neighbors,
                0.85, # damping
                30    # iterations
            )
            
            self.r_ranks = ranks
            print(f"   -> âœ… Rust calculation finished in {time.time() - start_time:.4f}s")
            
        except Exception as e:
            print(f"   âŒ Rust Engine Failed: {e}")
            import traceback
            traceback.print_exc()

    def _check_novelty(self, vector):
        """
        ç‹¬ç‰¹æ€§æ£€æµ‹ï¼šè®¡ç®—å‘é‡ä¸ R ç©ºé—´ä¸­æœ€è¿‘é”šç‚¹çš„è·ç¦»ã€‚
        è¿”å›: (is_novel, min_distance)
        """
        if not self.r_cache:
            # å¦‚æœ R ä¸ºç©ºï¼Œç¬¬ä¸€ä¸ªè¿›æ¥çš„è‚¯å®šæ˜¯æ–°çš„
            return True, 1.0

        r_vecs = np.array([p.vector['clip'] for p in self.r_cache])
        # è®¡ç®—ä¸ç°æœ‰é”šç‚¹çš„ç›¸ä¼¼åº¦
        sims = np.dot(r_vecs, np.array(vector))
        max_sim = np.max(sims)
        min_dist = 1.0 - max_sim

        is_novel = min_dist > NOVELTY_THRESHOLD
        return is_novel, min_dist

    def process_url_and_add(self, url, trigger_recalc=True, check_db_first=True):
        """
        å…¨è‡ªåŠ¨æµæ°´çº¿ï¼šæ£€æŸ¥æ•°æ®åº“ -> çˆ¬å–ï¼ˆå¦‚éœ€è¦ï¼‰-> æ¸…æ´—(ç†µ) -> å‘é‡åŒ– -> ç‹¬ç‰¹æ€§æ£€æµ‹ -> æ™‹å‡/å…¥åº“
        Args:
            url: ç›®æ ‡ URL
            trigger_recalc: æ˜¯å¦ç«‹å³è§¦å‘å…¨å±€é‡ç®— (æ‰¹é‡å¯¼å…¥æ—¶å»ºè®®è®¾ä¸º False)
            check_db_first: æ˜¯å¦å…ˆæ£€æŸ¥æ•°æ®åº“ï¼Œå¦‚æœå­˜åœ¨åˆ™è·³è¿‡çˆ¬å–
        """
        print(f"\nğŸ¤– Processing URL: {url}")
        
        # 0. æ£€æŸ¥æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if check_db_first:
            if self.check_url_exists(url, SPACE_X):
                print(f"   âœ… URLå·²åœ¨æ•°æ®åº“ä¸­ï¼Œè·³è¿‡çˆ¬å–: {url}")
                # è¿”å›æ•°æ®åº“ä¸­å·²æœ‰çš„æ•°æ®ä¿¡æ¯
                existing_data = self.get_url_from_db(url, SPACE_X)
                if existing_data:
                    print(f"   ğŸ“¦ ä½¿ç”¨å·²æœ‰æ•°æ® (ID: {existing_data['id'][:8]}...)")
                    return existing_data
                else:
                    print(f"   âš ï¸  æ•°æ®åº“ä¸­å­˜åœ¨ä½†æ— æ³•è·å–æ•°æ®ï¼Œç»§ç»­çˆ¬å–")

        # 1. çˆ¬å–
        data = crawler.parse(url)
        if not data:
            print("   âŒ Crawl failed or content filtered")
            return

        print(f"   -> âœ…ğŸ›ğŸ•¸ï¸Crawl successful! Retrieved {len(data['texts'])} valid text blocks (Entropy Cleaned).")

        promoted_count = 0

        # 2. å¤„ç†æ–‡æœ¬
        for text in data['texts']:
            vec = get_embedding(text=text)
            if not vec: continue

            # --- ç‹¬ç‰¹æ€§æ£€æµ‹ ---
            is_novel, dist = self._check_novelty(vec)
            promotion_status = False

            if is_novel:
                # åªæœ‰è¶³å¤Ÿç‹¬ç‰¹çš„çŸ¥è¯†æ‰ä¼šè¢«æ™‹å‡åˆ° R ç©ºé—´
                print(f"   ğŸŒŸ [NOVELTY DETECTED] New knowledge found (Distance {dist:.3f} > {NOVELTY_THRESHOLD}) -> Promoted to Space R")
                print(f"      Content Summary: {text[:40]}...")

                pt_id = str(uuid.uuid4())
                client.upsert(
                    collection_name=SPACE_R,
                    points=[models.PointStruct(id=pt_id, vector={"clip": vec}, payload={"content": text, "url": url})]
                )
                promotion_status = True
                promoted_count += 1
                
                # å¦‚æœå¼€å¯äº†å®æ—¶é‡ç®—
                if trigger_recalc:
                    self.trigger_global_recalculation()

            # æ— è®ºå¦‚ä½•ï¼Œéƒ½è¦æ·»åŠ åˆ° X (æœç´¢æ± )
            payload = {"url": url, "type": "text", "content_preview": text[:100], "pr_score": 0.0}
            
            # å¦‚æœæœ‰é“¾æ¥ä¿¡æ¯ï¼Œå­˜å‚¨åˆ°payloadä¸­
            if 'links' in data and data['links']:
                payload['links'] = data['links'][:50]  # å­˜å‚¨å‰50ä¸ªé“¾æ¥
            
            client.upsert(
                collection_name=SPACE_X,
                points=[models.PointStruct(
                    id=str(uuid.uuid4()),
                    vector={"clip": vec},
                    payload=payload
                )]
            )

        print(f"   âœ… URL processing complete. {promoted_count} items promoted to Anchors.")

    def add_to_space_x(self, text, url=None, promote_to_r=False, is_summarized=False, **kwargs):
        """
        æ·»åŠ å†…å®¹åˆ° Space X (æµ·é‡ä¿¡æ¯åº“)
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            url: URLåœ°å€
            promote_to_r: æ˜¯å¦å¼ºåˆ¶æ™‹å‡åˆ°Space R
            is_summarized: æ˜¯å¦å·²æ‘˜è¦
            **kwargs: å…¶ä»–å‚æ•°ï¼ŒåŒ…æ‹¬:
                - full_text: å®Œæ•´åŸæ–‡
                - links: é“¾æ¥åˆ—è¡¨ï¼ˆç”¨äºæ•°æ®åº“ç¼“å­˜ä¼˜åŒ–ï¼‰
        """
        if not text: return
        
        print(f"ğŸ“¥ Adding content to Space X: {url or 'Text Upload'}")

        # 1. ç”Ÿæˆå‘é‡ (CLIP Text Encoder)
        vec = self.get_text_embedding(text)
        if not vec: return

        # 2. æ„é€  payload
        payload = {
            "url": url,
            "type": "text",
            "content": text,
            "full_text": kwargs.get("full_text", text), # Store original text
            "content_preview": text[:100],
            "pr_score": 0.0,
            "is_summarized": is_summarized
        }
        
        # å¦‚æœæœ‰é“¾æ¥ä¿¡æ¯ï¼Œå­˜å‚¨åˆ°payloadä¸­ï¼ˆç”¨äºæ•°æ®åº“ç¼“å­˜ä¼˜åŒ–ï¼‰
        if 'links' in kwargs and kwargs['links']:
            payload['links'] = kwargs['links'][:50]  # å­˜å‚¨å‰50ä¸ªé“¾æ¥

        # 3. æ’å…¥åˆ° X
        pt_id = str(uuid.uuid4())
        client.upsert(
            collection_name=SPACE_X,
            points=[models.PointStruct(id=pt_id, vector={"clip": vec}, payload=payload)]
        )
        print(f"   âœ… Added to Space X (ID: {pt_id})")

        # 4. (å¯é€‰) æ™‹å‡åˆ° R
        if promote_to_r:
            print("   -> ğŸš€ Force promotion to Space R")
            client.upsert(
                collection_name=SPACE_R,
                points=[models.PointStruct(id=pt_id, vector={"clip": vec}, payload=payload)]
            )
            self.trigger_global_recalculation()

    def _update_space_x_scores(self):
        # ç®€å•çš„æŠ•å½±æ›´æ–°é€»è¾‘
        if not self.r_cache: return

        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºä¸æ‰“å°å¤ªå¤šåˆ·å±
        # print("   -> Updating Space X scores (projection calculation)...")

        r_vecs = np.array([p.vector['clip'] for p in self.r_cache])
        r_scores = np.array([self.r_ranks[p.id] for p in self.r_cache])

        offset = None
        while True:
            batch, offset = client.scroll(collection_name=SPACE_X, limit=50, with_vectors=True, offset=offset)
            if not batch: break

            points_to_update = []
            for point in batch:
                x_vec = np.array(point.vector['clip'])
                sims = np.dot(r_vecs, x_vec)
                sims[sims < 0] = 0
                new_score = float(np.sum(sims * r_scores))

                points_to_update.append(models.PointStruct(
                    id=point.id, vector={"clip": x_vec.tolist()},
                    payload={**point.payload, "pr_score": new_score}
                ))
            client.upsert(collection_name=SPACE_X, points=points_to_update)
            if offset is None: break

    # ... (ä¿ç•™ä¹‹å‰çš„ __init__, trigger_global_recalculation ç­‰æ‰€æœ‰ä»£ç ) ...

    # [æ–°å¢] åˆ†é¡µæµè§ˆæ¥å£ (ç”¨äº Admin é¢æ¿)
    def browse_collection(self, collection_name, limit=50, offset_id=None):
        """
        æµè§ˆæ•°æ®åº“å†…å®¹ã€‚
        Qdrant çš„ scroll API ä½¿ç”¨ offset æŒ‡é’ˆã€‚
        """
        points, next_offset = client.scroll(
            collection_name=collection_name,
            limit=limit,
            with_payload=True,
            with_vectors=False,  # æµè§ˆæ—¶ä¸éœ€è¦çœ‹å·¨å¤§çš„å‘é‡æ•°æ®
            offset=offset_id
        )

        results = []
        for p in points:
            results.append({
                "id": p.id,
                "payload": p.payload,
                "score": p.payload.get("pr_score", 0.0)
            })

        return {
            "items": results,
            "next_offset": next_offset
        }

    def check_url_exists(self, url: str, collection_name: str = SPACE_X) -> bool:
        """
        æ£€æŸ¥URLæ˜¯å¦å·²ç»åœ¨æ•°æ®åº“ä¸­å­˜åœ¨
        
        Args:
            url: è¦æ£€æŸ¥çš„URL
            collection_name: è¦æŸ¥è¯¢çš„é›†åˆåç§°ï¼ˆé»˜è®¤SPACE_Xï¼‰
            
        Returns:
            bool: å¦‚æœURLå­˜åœ¨è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            points, _ = self.client.scroll(
                collection_name=collection_name,
                scroll_filter=models.Filter(
                    must=[
                        models.FieldCondition(
                            key="url",
                            match=models.MatchValue(value=url)
                        )
                    ]
                ),
                limit=1
            )
            return len(points) > 0
        except Exception as e:
            print(f"âš ï¸ Error checking URL existence: {e}")
            return False
    
    def get_url_from_db(self, url: str, collection_name: str = SPACE_X) -> Optional[Dict]:
        """
        ä»æ•°æ®åº“è·å–URLçš„æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        
        Args:
            url: è¦æŸ¥è¯¢çš„URL
            collection_name: è¦æŸ¥è¯¢çš„é›†åˆåç§°ï¼ˆé»˜è®¤SPACE_Xï¼‰
            
        Returns:
            Dict: åŒ…å«idå’Œpayloadçš„å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        try:
            points, _ = self.client.scroll(
                collection_name=collection_name,
                scroll_filter=models.Filter(
                    must=[
                        models.FieldCondition(
                            key="url",
                            match=models.MatchValue(value=url)
                        )
                    ]
                ),
                limit=1,
                with_payload=True,
                with_vectors=True
            )
            if points:
                return {
                    'id': points[0].id,
                    'payload': points[0].payload,
                    'vector': points[0].vector
                }
            return None
        except Exception as e:
            print(f"âš ï¸ Error getting URL from DB: {e}")
            return None
    
    def batch_check_urls(self, urls: List[str], collection_name: str = SPACE_X) -> Dict[str, bool]:
        """
        æ‰¹é‡æ£€æŸ¥å¤šä¸ªURLæ˜¯å¦å­˜åœ¨
        
        Args:
            urls: URLåˆ—è¡¨
            collection_name: è¦æŸ¥è¯¢çš„é›†åˆåç§°ï¼ˆé»˜è®¤SPACE_Xï¼‰
            
        Returns:
            Dict[str, bool]: URLåˆ°å­˜åœ¨æ€§çš„æ˜ å°„å­—å…¸
        """
        result = {}
        
        # æ‰¹é‡æŸ¥è¯¢ä»¥æé«˜æ•ˆç‡
        for url in urls:
            result[url] = self.check_url_exists(url, collection_name)
        
        return result

    # [æ–°å¢] åˆ é™¤æ¥å£ (ç”¨äº Admin é¢æ¿)
    def delete_item(self, collection_name, point_id):
        client.delete(
            collection_name=collection_name,
            points_selector=models.PointIdsList(points=[point_id])
        )
        print(f"ğŸ—‘ï¸ Deleted ID from {collection_name}: {point_id}")
        # å¦‚æœåˆ çš„æ˜¯ R ç©ºé—´ï¼Œå¿…é¡»è§¦å‘é‡ç®—
        if collection_name == SPACE_R:
            self.trigger_global_recalculation()

    # [æ–°å¢] ä» X å¤åˆ¶åˆ° R (ç”¨äº Admin æ‰‹åŠ¨ä¼˜åŒ–)
    def promote_from_x_to_r(self, point_id):
        # 1. å…ˆä» X æ‹¿æ•°æ®
        points = client.retrieve(
            collection_name=SPACE_X,
            ids=[point_id],
            with_vectors=True,
            with_payload=True
        )
        if not points: return False

        point = points[0]

        # 2. å†™å…¥ R
        client.upsert(
            collection_name=SPACE_R,
            points=[models.PointStruct(
                id=point.id,  # ä¿æŒ ID ä¸€è‡´
                vector=point.vector,
                payload={**point.payload, "promoted_by_admin": True}
            )]
        )
        print(f"â¬†ï¸ Admin manually promoted ID: {point_id}")

        # 3. è§¦å‘é‡ç®—
        self.trigger_global_recalculation()
        return True


    def summarize_text_api(self, text):
        """Use Gemini API to summarize text."""
        if not GOOGLE_API_KEY:
            return text
            
        try:
            # Enforce 200 word limit and ignore child page content
            prompt = f"Please summarize the following content in strictly under 200 words. Focus ONLY on the main content of the current page. Ignore any lists of sub-pages, navigation menus, or teasers for other articles. Make it concise:\n\n{text[:15000]}"
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            print(f"âš ï¸ API Summarization failed: {e}")
            return text

    def backfill_summaries(self, force=False):
        """Iterate through all items in Space X and summarize."""
        print(f"ğŸ”„ Starting backfill of summaries (Force={force})...")
        offset = None
        count = 0
        while True:
            batch, offset = self.client.scroll(
                collection_name=SPACE_X, 
                limit=50, 
                with_payload=True, 
                with_vectors=True,
                offset=offset
            )
            if not batch: break
            
            points_to_update = []
            for point in batch:
                payload = point.payload
                
                # If already summarized and not forced, skip
                if payload.get("is_summarized") and not force:
                    continue
                
                # Use full_text if available, otherwise try to extract from content
                full_text = payload.get("full_text", "")
                if not full_text:
                    if "Original Content:" in content:
                        parts = content.split("Original Content:\n")
                        if len(parts) > 1:
                            full_text = parts[1].strip()
                    else:
                        full_text = content

                if not full_text or len(full_text) < 100:
                    continue
                    
                print(f"   ğŸ“ Summarizing item: {payload.get('url')}")
                summary = self.summarize_text_api(full_text)
                
                # Update payload
                new_payload = payload.copy()
                new_payload["content"] = summary # Store ONLY summary
                new_payload["full_text"] = full_text # Ensure full_text is preserved
                new_payload["is_summarized"] = True
                
                points_to_update.append(models.PointStruct(
                    id=point.id,
                    vector=point.vector,
                    payload=new_payload
                ))
                count += 1
                
            if points_to_update:
                self.client.upsert(collection_name=SPACE_X, points=points_to_update)
                
            if offset is None: break
            
        print(f"âœ… Backfill complete. Updated {count} items.")

    def process_url_recursive(self, start_url, max_depth=1, callback=None, check_db_first=True):
        """
        Recursively crawl and process URLs up to max_depth.
        callback(count, url): function to call on successful addition.
        check_db_first: æ˜¯å¦å…ˆæ£€æŸ¥æ•°æ®åº“ï¼Œå¦‚æœURLå·²å­˜åœ¨åˆ™è·³è¿‡çˆ¬å–
        """
        print(f"ğŸ•¸ï¸ Starting recursive crawl: {start_url} (Depth: {max_depth})")
        if check_db_first:
            print(f"   âœ… å·²å¯ç”¨æ•°æ®åº“æ£€æŸ¥ï¼Œå°†è·³è¿‡å·²å­˜åœ¨çš„URL")
        
        visited = set()
        queue = [(start_url, 0)]
        
        # æ‰¹é‡æ£€æŸ¥URLæ˜¯å¦å­˜åœ¨ï¼ˆç”¨äºä¼˜åŒ–ï¼‰
        urls_to_check = [] # (url, depth)
        count = 0
        
        while queue:
            current_url, depth = queue.pop(0)
            
            if current_url in visited:
                continue
            visited.add(current_url)
            
            # æ£€æŸ¥æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if check_db_first:
                if self.check_url_exists(current_url, SPACE_X):
                    print(f"   â­ï¸  è·³è¿‡ï¼ˆæ•°æ®åº“ä¸­å·²å­˜åœ¨ï¼‰: {current_url}")
                    count += 1
                    if callback:
                        callback(count, current_url)
                    # å°è¯•ä»æ•°æ®åº“ä¸­è·å–é“¾æ¥ä¿¡æ¯
                    if depth < max_depth:
                        from urllib.parse import urlparse
                        start_domain = urlparse(start_url).netloc
                        existing_data = self.get_url_from_db(current_url, SPACE_X)
                        if existing_data and 'links' in existing_data.get('payload', {}):
                            # å¦‚æœæ•°æ®åº“ä¸­æœ‰é“¾æ¥ä¿¡æ¯ï¼Œä½¿ç”¨å®ƒä»¬
                            stored_links = existing_data['payload'].get('links', [])
                            for link in stored_links:
                                if urlparse(link).netloc == start_domain:
                                    if link not in visited:
                                        queue.append((link, depth + 1))
                            continue  # è·³è¿‡çˆ¬å–ï¼Œç›´æ¥ä½¿ç”¨å­˜å‚¨çš„é“¾æ¥
                        else:
                            # å¦‚æœæ²¡æœ‰å­˜å‚¨é“¾æ¥ï¼Œä»ç„¶éœ€è¦çˆ¬å–ä»¥è·å–é“¾æ¥
                            # ä½†å¯ä»¥è®¾ç½®ä¸€ä¸ªæ ‡å¿—ï¼Œåªçˆ¬å–é“¾æ¥ï¼Œä¸æ›´æ–°æ•°æ®åº“
                            pass  # ç»§ç»­çˆ¬å–
            
            # Process current URL
            try:
                # 1. Crawl
                data = self.crawler.parse(current_url)
                if not data:
                    continue
                    
                # 2. Add to DB (Space X)
                # Combine texts for content
                raw_content = "\n\n".join(data['texts'])
                if not raw_content:
                    continue
                
                # Summarize using API
                final_content = raw_content
                is_summarized = False
                
                if len(raw_content) > 300:
                    summary = self.summarize_text_api(raw_content)
                    if summary != raw_content:
                        # ONLY store the summary to keep it clean
                        final_content = summary
                        is_summarized = True
                        print(f"   âœ¨ API Summarized content for {current_url}")
                    
                # ä¿å­˜æ•°æ®æ—¶ä¹Ÿä¿å­˜é“¾æ¥ä¿¡æ¯ï¼ˆç”¨äºåç»­ä¼˜åŒ–ï¼‰
                self.add_to_space_x(
                    text=final_content, 
                    url=current_url, 
                    promote_to_r=False, 
                    is_summarized=is_summarized, 
                    full_text=raw_content,
                    links=data.get('links', [])  # ä¼ é€’é“¾æ¥ä¿¡æ¯
                )
                count += 1
                
                # Trigger callback
                if callback:
                    callback(count, current_url)
                
                # 3. Enqueue children if depth allows
                if depth < max_depth:
                    # Filter links to stay on same domain or be relevant?
                    # For now, let's stick to same domain to avoid exploding
                    from urllib.parse import urlparse
                    start_domain = urlparse(start_url).netloc
                    
                    for link in data.get('links', []):
                        if urlparse(link).netloc == start_domain:
                            if link not in visited:
                                queue.append((link, depth + 1))
                                
            except Exception as e:
                print(f"âš ï¸ Error processing {current_url}: {e}")
                
        print(f"âœ… Recursive crawl finished. Processed {count} pages.")
        return count


# --- æ¨¡æ‹Ÿæµ‹è¯• ---
if __name__ == "__main__":
    mgr = SystemManager()

    print("\nğŸ§ª Injecting mock data to verify Rust Engine...")
    # Inject some mock data into Space R to trigger calculation
    mock_vec = [0.1] * 512
    id1 = str(uuid.uuid4())
    id2 = str(uuid.uuid4())
    id3 = str(uuid.uuid4())
    
    mgr.client.upsert(
        collection_name=SPACE_R,
        points=[
            models.PointStruct(id=id1, vector={"clip": mock_vec}, payload={"url": "http://a.com", "content": "A"}),
            models.PointStruct(id=id2, vector={"clip": mock_vec}, payload={"url": "http://b.com", "content": "B"}),
            models.PointStruct(id=id3, vector={"clip": mock_vec}, payload={"url": "http://c.com", "content": "C"}),
        ]
    )
    
    # Add interactions to verify weight passing
    mgr.interaction_mgr.record_interaction(id1, "click")
    
    # Trigger Recalculation
    mgr.trigger_global_recalculation()