from fastapi import FastAPI, WebSocket, WebSocketDisconnect, BackgroundTasks, UploadFile, File, Form, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from typing import List
import shutil
import os
import time
import datetime
import asyncio
from qdrant_client import models
import argparse
import sys
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¼•å…¥æ ¸å¿ƒæ¨¡å—
from system_manager import SystemManager, SPACE_R, SPACE_X
from search_engine import search
from xml_dump_processor import MediaWikiDumpProcessor

# ä»ç¯å¢ƒå˜é‡è¯»å–çˆ¬å–å¯†ç 
CRAWL_PASSWORD = os.getenv("CRAWL_PASSWORD", "")

# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description="TUM Search Engine Server")
parser.add_argument("--mode", type=str, choices=["user", "admin"], default="user", help="Server mode: user or admin")
parser.add_argument("--port", type=int, default=8000, help="Port to run the server on")
# é¿å…ä¸ uvicorn çš„å‚æ•°å†²çªï¼Œåªè§£æå·²çŸ¥çš„
args, unknown = parser.parse_known_args()

app = FastAPI(title=f"TUM Search Engine ({args.mode.upper()})")

# æŒ‚è½½é™æ€æ–‡ä»¶ (å‰ç«¯é¡µé¢)
app.mount("/static", StaticFiles(directory="static"), name="static")

# åˆå§‹åŒ–æ ¸å¿ƒç®¡ç†å™¨
mgr = SystemManager()


# --- WebSocket è¿æ¥ç®¡ç†å™¨ (ç”¨äºå®æ—¶é€šçŸ¥) ---
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except:
                pass


ws_manager = ConnectionManager()


# --- å¼‚æ­¥åå°ä»»åŠ¡ (è€—æ—¶æ“ä½œåœ¨è¿™é‡Œåš) ---
def background_process_content(task_type: str, content: str = None, file_path: str = None, url: str = None):
    """
    åå°æ‰§è¡Œï¼šçˆ¬å–/å…¥åº“ -> ç‹¬ç‰¹æ€§æ£€æµ‹ -> (å¯èƒ½) HNSWé‡ç®— -> å‘é€é€šçŸ¥
    """
    start_time = time.time()
    print(f"â³ [AsyncTask] Starting task: {task_type}")

    try:
        # æ‰§è¡Œå…·ä½“é€»è¾‘
        if task_type == "url":
            # Define callback to send progress via WebSocket
            async def progress_callback(count, current_url):
                await ws_manager.broadcast({
                    "type": "progress",
                    "count": count,
                    "message": f"Processed: {current_url}"
                })
            
            # Run recursive crawl (å¯ç”¨æ•°æ®åº“æ£€æŸ¥ä»¥è·³è¿‡å·²å­˜åœ¨çš„URL)
            mgr.process_url_recursive(url, max_depth=1, callback=lambda c, u: asyncio.run(progress_callback(c, u)), check_db_first=True)
            
            # Get total count
            total_count = mgr.client.count(collection_name=SPACE_X).count
            
            asyncio.run(ws_manager.broadcast({
                "type": "system_update",
                "message": f"âœ… Recursive crawl finished. Processed {total_count} pages in total."
            }))
        elif task_type == "text":
            # ç®€å•æ–‡æœ¬å¤„ç†ï¼Œå¤ç”¨ add_to_space_x
            mgr.add_to_space_x(text=content, url="User Upload", promote_to_r=False)
        elif task_type == "image":
            mgr.add_to_space_x(image_path=file_path, url="User Image Upload", promote_to_r=False)
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(file_path):
                os.remove(file_path)
        # ä»»åŠ¡å®Œæˆï¼Œå‡†å¤‡é€šçŸ¥æ¶ˆæ¯
        duration = time.time() - start_time
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # æ„é€ ç”¨æˆ·è¦æ±‚çš„è‹±æ–‡é€šçŸ¥
        notification_msg = (
            f"Update Completed at {timestamp}. "
            f"The Database (Space X & R) and the Representative Network have been synchronized. "
            f"Search results may vary as the system evolves. "
            f"Processing time: {duration:.2f}s."
        )

        # é€šè¿‡ WebSocket å¹¿æ’­ç»™æ‰€æœ‰åœ¨çº¿ç”¨æˆ·
        asyncio.run(ws_manager.broadcast({
            "type": "system_update",
            "message": notification_msg,
            "timestamp": timestamp
        }))
        print("âœ… [AsyncTask] Notification sent.")

    except Exception as e:
        print(f"âŒ [AsyncTask] Error: {e}")
        import traceback
        traceback.print_exc()
        asyncio.run(ws_manager.broadcast({
            "type": "error",
            "message": f"Processing failed: {str(e)}"
        }))


def background_process_xml_dump(file_path: str, base_url: str = "", max_pages: int = None):
    """
    åå°å¤„ç†XML dumpå¯¼å…¥ä»»åŠ¡
    """
    start_time = time.time()
    print(f"â³ [XML Dump Import] Starting XML dump import from {file_path}")
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = MediaWikiDumpProcessor(
            base_url=base_url,
            wiki_type="auto"  # è‡ªåŠ¨æ£€æµ‹Wikiç±»å‹
        )
        
        # è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(current: int, total: int, message: str):
            progress = int((current / total) * 100) if total > 0 else 0
            asyncio.run(ws_manager.broadcast({
                "type": "progress",
                "count": current,
                "total": total,
                "message": f"XML Dumpå¤„ç†è¿›åº¦: {current}/{total} ({progress}%) - {message}"
            }))
        
        # å¤„ç†dumpæ–‡ä»¶
        processor.process_dump(file_path, max_pages=max_pages, progress_callback=progress_callback)
        
        # å¯¼å…¥åˆ°æ•°æ®åº“
        asyncio.run(ws_manager.broadcast({
            "type": "progress",
            "message": "æ­£åœ¨å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“..."
        }))
        
        mgr_instance = SystemManager()
        stats = processor.import_to_database(
            mgr_instance,
            url_prefix=base_url or processor.base_url,
            batch_size=50,
            import_edges=False,  # æš‚æ—¶ä¸é€šè¿‡CSVå¯¼å…¥è¾¹
            edges_csv_path=None,
            check_db_first=True  # æ£€æŸ¥æ•°æ®åº“ï¼Œè·³è¿‡å·²å­˜åœ¨çš„URL
        )
        
        # å¯¼å…¥è¾¹ï¼ˆé“¾æ¥å…³ç³»ï¼‰- é€šè¿‡ç”Ÿæˆä¸´æ—¶CSVç„¶åå¯¼å…¥
        edge_count = 0
        if processor.links:
            asyncio.run(ws_manager.broadcast({
                "type": "progress",
                "message": "æ­£åœ¨å¯¼å…¥é“¾æ¥å…³ç³»..."
            }))
            
            # ç”Ÿæˆä¸´æ—¶è¾¹CSVæ–‡ä»¶
            import tempfile
            import csv
            temp_edges_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8')
            edges_writer = csv.writer(temp_edges_file)
            edges_writer.writerow(['source_title', 'target_title'])
            
            for source_title, target_titles in processor.links.items():
                for target_title in target_titles:
                    if source_title in processor.pages and target_title in processor.pages:
                        edges_writer.writerow([source_title, target_title])
            
            temp_edges_file.close()
            
            # ä½¿ç”¨import_edgesæ¨¡å—å¯¼å…¥
            try:
                from import_edges import import_edges_from_csv
                url_prefix_for_edges = base_url or processor.base_url
                import_edges_from_csv(temp_edges_file.name, mgr_instance, base_url=url_prefix_for_edges)
                # è®¡ç®—è¾¹çš„æ€»æ•°
                edge_count = sum(len(target_titles) for target_titles in processor.links.values())
            except Exception as e:
                print(f"âš ï¸  è¾¹å¯¼å…¥å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_edges_file.name):
                    os.remove(temp_edges_file.name)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(file_path):
            os.remove(file_path)
        
        # å‘é€å®Œæˆé€šçŸ¥
        duration = time.time() - start_time
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        success_msg = (
            f"âœ… XML Dumpå¯¼å…¥å®Œæˆ ({timestamp})\n"
            f"å¤„ç†é¡µé¢: {processor.stats['processed_pages']}\n"
            f"æ€»é¡µé¢: {processor.stats['total_pages']}\n"
            f"æˆåŠŸå¯¼å…¥: {stats['success']}\n"
            f"è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {stats.get('skipped', 0)}\n"
            f"å¤±è´¥: {stats['failed']}\n"
            f"é“¾æ¥å…³ç³»: {edge_count}\n"
            f"æ™‹å‡åˆ°Space R: {stats['promoted']}\n"
            f"å¤„ç†æ—¶é—´: {duration:.2f}ç§’"
        )
        
        asyncio.run(ws_manager.broadcast({
            "type": "system_update",
            "message": success_msg,
            "timestamp": timestamp
        }))
        
        print(f"âœ… [XML Dump Import] Completed: {stats['success']} items, {edge_count} edges imported")
        
    except Exception as e:
        print(f"âŒ [XML Dump Import] Error: {e}")
        import traceback
        traceback.print_exc()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(file_path):
            os.remove(file_path)
        
        asyncio.run(ws_manager.broadcast({
            "type": "error",
            "message": f"XML Dumpå¯¼å…¥å¤±è´¥: {str(e)}"
        }))


# ================= è·¯ç”±å®šä¹‰ =================

# 1. é€šç”¨è·¯ç”± (User & Admin)
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await ws_manager.connect(websocket)
    try:
        while True:
            await websocket.receive_text()  # ä¿æŒè¿æ¥ï¼Œè™½ä¸æ¥æ”¶æ¶ˆæ¯
    except WebSocketDisconnect:
        ws_manager.disconnect(websocket)

@app.get("/api/search")
async def api_search(q: str):
    results = search(q, top_k=20)
    return {"results": results}

@app.get("/api/search/graph")
async def api_search_graph(q: str, max_nodes: int = 30):
    """
    è¿”å›æœç´¢ç»“æœçš„ç½‘ç»œå›¾æ•°æ®
    æ„å»ºä»¥æŸ¥è¯¢ç»“æœä¸ºä¸­å¿ƒçš„èŠ‚ç‚¹ç½‘ç»œå›¾
    """
    from search_engine import search
    from urllib.parse import urlparse
    
    # 1. è·å–æœç´¢ç»“æœ
    search_results = search(q, top_k=min(10, max_nodes // 3))
    
    if not search_results:
        return {"nodes": [], "edges": []}
    
    # 2. æ„å»ºèŠ‚ç‚¹å’Œè¾¹çš„é›†åˆ
    nodes_dict = {}  # id -> node data
    edges_list = []  # List of (source_id, target_id, weight) tuples
    
    # æå–èŠ‚ç‚¹æ ‡é¢˜çš„è¾…åŠ©å‡½æ•°
    def extract_title(url, content_preview="", node_id=""):
        if url:
            # æå–URLè·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†
            url_part = url.split('/')[-1].split('?')[0]  # ç§»é™¤æŸ¥è¯¢å‚æ•°
            url_part = url_part.replace('_', ' ').replace('-', ' ')
            if url_part and url_part != url:
                title = url_part[:50]
                return title if len(title) <= 50 else title[:47] + "..."
        
        # å¦‚æœURLä¸å¯ç”¨ï¼Œå°è¯•ä»å†…å®¹ä¸­æå–
        if content_preview:
            words = content_preview.split()[:5]  # å–å‰5ä¸ªè¯
            title = ' '.join(words)[:50]
            return title if len(title) <= 50 else title[:47] + "..."
        
        # æœ€åä½¿ç”¨èŠ‚ç‚¹ID
        return f"Node {node_id[:8]}" if node_id else "Unknown Node"
    
    # 3. ä¸ºæ¯ä¸ªæœç´¢ç»“æœæ·»åŠ èŠ‚ç‚¹ï¼Œå¹¶æ‰¾åˆ°ç›¸å…³èŠ‚ç‚¹
    for result in search_results:
        result_id = result['id']
        result_url = result.get('url', '')
        
        node_title = extract_title(result_url, result.get('content', ''), result_id)
        
        # æ·»åŠ ä¸­å¿ƒèŠ‚ç‚¹ï¼ˆæœç´¢ç»“æœï¼‰
        nodes_dict[result_id] = {
            "id": result_id,
            "name": node_title,
            "url": result_url,
            "content": result.get('content', '')[:100],
            "score": result.get('score', 0.0),
            "category": result.get('type', 'unknown'),
            "value": result.get('score', 0.0) * 100,  # èŠ‚ç‚¹å¤§å°
            "isCenter": True  # æ ‡è®°ä¸ºä¸­å¿ƒèŠ‚ç‚¹
        }
        
        # 4. æŸ¥æ‰¾ç›¸å…³èŠ‚ç‚¹ï¼ˆé€šè¿‡å‘é‡ç›¸ä¼¼åº¦ï¼‰
        try:
            # ä»æ•°æ®åº“ä¸­è·å–è¯¥èŠ‚ç‚¹çš„å‘é‡
            points = mgr.client.retrieve(
                collection_name=SPACE_X,
                ids=[result_id],
                with_vectors=True,
                with_payload=True
            )
            
            if points:
                point = points[0]
                
                # æŸ¥æ‰¾ç›¸ä¼¼çš„èŠ‚ç‚¹
                related_hits = mgr.client.query_points(
                    collection_name=SPACE_X,
                    query=point.vector['clip'],
                    using="clip",
                    limit=5  # æ¯ä¸ªä¸­å¿ƒèŠ‚ç‚¹æœ€å¤š5ä¸ªç›¸å…³èŠ‚ç‚¹
                ).points
                
                for hit in related_hits:
                    if hit.id == result_id:
                        continue
                    
                    # é™åˆ¶èŠ‚ç‚¹æ•°é‡
                    if len(nodes_dict) >= max_nodes:
                        break
                    
                    # æå–ç›¸å…³èŠ‚ç‚¹æ ‡é¢˜
                    related_url = hit.payload.get('url', '')
                    related_content = hit.payload.get('content_preview', '')
                    related_title = extract_title(related_url, related_content, hit.id)
                    
                    # æ·»åŠ ç›¸å…³èŠ‚ç‚¹
                    if hit.id not in nodes_dict:
                        nodes_dict[hit.id] = {
                            "id": hit.id,
                            "name": related_title,
                            "url": related_url,
                            "content": hit.payload.get('content_preview', '')[:100],
                            "score": float(hit.score),
                            "category": hit.payload.get('type', 'unknown'),
                            "value": float(hit.score) * 50,  # ç›¸å…³èŠ‚ç‚¹è¾ƒå°
                            "isCenter": False
                        }
                    
                    # æ·»åŠ è¾¹ï¼ˆä¸­å¿ƒèŠ‚ç‚¹ -> ç›¸å…³èŠ‚ç‚¹ï¼‰
                    edge_tuple = (result_id, hit.id, float(hit.score))
                    if edge_tuple not in edges_list:
                        edges_list.append(edge_tuple)
        
        except Exception as e:
            print(f"âš ï¸ Error finding related nodes for {result_id}: {e}")
        
        # 5. æŸ¥æ‰¾åä½œè¿‡æ»¤èŠ‚ç‚¹ï¼ˆé€šè¿‡transitionsï¼‰
        try:
            top_transitions = mgr.interaction_mgr.get_top_transitions(result_id, limit=3)
            
            for target_id, count in top_transitions:
                if len(nodes_dict) >= max_nodes:
                    break
                
                # å¦‚æœèŠ‚ç‚¹å·²å­˜åœ¨ï¼Œåªæ·»åŠ è¾¹
                if target_id not in nodes_dict:
                    try:
                        target_points = mgr.client.retrieve(
                            collection_name=SPACE_X,
                            ids=[target_id],
                            with_payload=True
                        )
                        
                        if target_points:
                            target_point = target_points[0]
                            target_url = target_point.payload.get('url', '')
                            target_content = target_point.payload.get('content_preview', '')
                            target_title = extract_title(target_url, target_content, target_id)
                            
                            nodes_dict[target_id] = {
                                "id": target_id,
                                "name": target_title,
                                "url": target_url,
                                "content": target_point.payload.get('content_preview', '')[:100],
                                "score": 0.5,  # åä½œè¿‡æ»¤èŠ‚ç‚¹ä¸­ç­‰æƒé‡
                                "category": target_point.payload.get('type', 'unknown'),
                                "value": 30.0,
                                "isCenter": False
                            }
                    except Exception:
                        continue
                
                # æ·»åŠ åä½œè¾¹ï¼ˆæƒé‡åŸºäºtransition countï¼‰
                edge_weight = 0.3 + (count * 0.1)  # åŸºäºtransitionæ¬¡æ•°
                edge_tuple = (result_id, target_id, edge_weight)
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„è¾¹ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°æƒé‡
                existing_edge = next((e for e in edges_list if e[0] == result_id and e[1] == target_id), None)
                if existing_edge:
                    edges_list.remove(existing_edge)
                    edge_weight = max(edge_weight, existing_edge[2])  # ä½¿ç”¨è¾ƒå¤§çš„æƒé‡
                edges_list.append((result_id, target_id, edge_weight))
        
        except Exception as e:
            print(f"âš ï¸ Error finding collaborative nodes for {result_id}: {e}")
    
    # 6. è½¬æ¢æ•°æ®æ ¼å¼
    nodes = list(nodes_dict.values())
    edges = [{"source": src, "target": tgt, "value": weight} for src, tgt, weight in edges_list]
    
    return {
        "nodes": nodes,
        "edges": edges,
        "query": q
    }

@app.post("/api/feedback")
async def api_feedback(item_id: str = Form(...), action: str = Form(...), source_id: str = Form(None)):
    """
    Record user feedback (click, impression, etc.)
    """
    mgr.interaction_mgr.record_interaction(item_id, action, source_id)
    mgr.interaction_mgr.record_interaction(item_id, action, source_id)
    return {"status": "recorded", "item_id": item_id}

@app.get("/api/trending")
async def api_trending(limit: int = 5):
    """
    Get trending items based on clicks.
    """
    trending_ids = mgr.interaction_mgr.get_trending_items(limit)
    print(f"ğŸ”¥ Trending IDs: {trending_ids}")
    
    results = []
    if trending_ids:
        # Retrieve details from Space X
        try:
            points = mgr.client.retrieve(
                collection_name=SPACE_X,
                ids=trending_ids,
                with_payload=True
            )
        except Exception as e:
            print(f"âŒ Error retrieving trending items: {e}")
            return {"results": []}
        
        # Map back to preserve order (retrieve might not preserve order)
        points_map = {p.id: p for p in points}
        
        for tid in trending_ids:
            if tid in points_map:
                p = points_map[tid]
                results.append({
                    "id": p.id,
                    "payload": p.payload,
                    "clicks": mgr.interaction_mgr.interactions[tid]["clicks"]
                })
                
    return {"results": results}

@app.get("/view/{item_id}")
async def view_item(item_id: str):
    return FileResponse('static/view.html')

@app.get("/api/item/{item_id}")
async def get_item_details(item_id: str):
    # 1. Retrieve item from Space X
    points = mgr.client.retrieve(
        collection_name=SPACE_X,
        ids=[item_id],
        with_payload=True,
        with_vectors=True
    )
    if not points:
        raise HTTPException(status_code=404, message="Item not found")
    
    item = points[0]
    
    # 2. Find related items (Internal Navigation Links)
    # Use the item's vector to find similar items
    related_hits = mgr.client.query_points(
        collection_name=SPACE_X,
        query=item.vector['clip'],
        using="clip",
        limit=6 # Top 5 related (excluding self)
    ).points
    
    related = []
    for hit in related_hits:
        if hit.id != item_id:
            related.append({
                "id": hit.id,
                "score": hit.score,
                "payload": hit.payload
            })
            
    # 3. Collaborative Filtering Recommendations (People also visited)
    # Based on transitions from this item
    collab_recs = []
    # Use UUID (item.id) for transitions
    top_transitions = mgr.interaction_mgr.get_top_transitions(item.id, limit=3)
    
    if top_transitions:
        target_ids = [t[0] for t in top_transitions]
        # Retrieve target items by ID
        target_points = mgr.client.retrieve(
            collection_name=SPACE_X,
            ids=target_ids,
            with_payload=True
        )
        
        # Create a map for easy lookup
        target_map = {p.id: p for p in target_points}
        
        for target_id, count in top_transitions:
            if target_id in target_map:
                hit = target_map[target_id]
                collab_recs.append({
                    "id": hit.id,
                    "count": count,
                    "payload": hit.payload
                })

    return {
        "item": {
            "id": item.id,
            "payload": item.payload
        },
        "related": related[:5],
        "collaborative": collab_recs
    }

# 2. æ¨¡å¼ç‰¹å®šè·¯ç”±
if args.mode == "user":
    print("ğŸš€ Server starting in USER mode")
    
    @app.get("/")
    async def get_user_ui():
        response = FileResponse('static/index.html')
        # æ·»åŠ ç¼“å­˜æ§åˆ¶å¤´ï¼Œç¡®ä¿é¡µé¢æ›´æ–°åèƒ½ç«‹å³çœ‹åˆ°æ•ˆæœ
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"
        return response

    @app.get("/api/feed")
    async def api_feed(limit: int = 20, offset: str = None):
        """
        User Feed: Browse Space X (Public Content) only.
        """
        # å¼ºåˆ¶æŒ‡å®š SPACE_Xï¼Œé˜²æ­¢ç”¨æˆ·è®¿é—® R
        offset_val = offset if offset and offset != "null" else None
        return mgr.browse_collection(SPACE_X, limit, offset_val)

    # ç”¨æˆ·ä¸Šä¼ æ¥å£
    @app.post("/api/admin/backfill")
    async def trigger_backfill(background_tasks: BackgroundTasks, force: bool = False):
        """Trigger background backfill of summaries."""
        background_tasks.add_task(mgr.backfill_summaries, force=force)
        return {"status": "started", "message": f"Backfill process started (Force={force})."}

    @app.post("/api/upload/url")
    async def upload_url(url: str = Form(...), password: str = Form(None), background_tasks: BackgroundTasks = None):
        # éªŒè¯å¯†ç 
        if not CRAWL_PASSWORD:
            raise HTTPException(status_code=500, detail="æœåŠ¡å™¨æœªé…ç½®çˆ¬å–å¯†ç ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
        
        if not password or password != CRAWL_PASSWORD:
            raise HTTPException(status_code=403, detail="å¯†ç é”™è¯¯ï¼Œçˆ¬å–è¢«æ‹’ç»")
        
        # å¯†ç éªŒè¯é€šè¿‡ï¼Œå¼€å§‹å¤„ç†
        background_tasks.add_task(background_process_content, "url", url=url)
        return {"status": "processing", "message": "URL received. Processing..."}

    @app.post("/api/upload/text")
    async def upload_text(text: str = Form(...), background_tasks: BackgroundTasks = None):
        background_tasks.add_task(background_process_content, "text", content=text)
        return {"status": "processing", "message": "Text received. Processing..."}

    @app.post("/api/upload/image")
    async def upload_image(file: UploadFile = File(...), background_tasks: BackgroundTasks = None):
        os.makedirs("temp_uploads", exist_ok=True)
        file_path = f"temp_uploads/{file.filename}"
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        background_tasks.add_task(background_process_content, "image", file_path=file_path)
        return {"status": "processing", "message": "Image received. Processing..."}

    @app.post("/api/upload/xml-dump")
    async def upload_xml_dump(
        file: UploadFile = File(...),
        base_url: str = Form(""),
        max_pages: int = Form(None),
        background_tasks: BackgroundTasks = None
    ):
        """
        ä¸Šä¼ XML Dumpæ–‡ä»¶ï¼ˆMediaWiki/Wikipediaæ ¼å¼ï¼‰
        è‡ªåŠ¨è§£æå¹¶å¯¼å…¥åˆ°æ•°æ®åº“ï¼Œæ— éœ€å€ŸåŠ©çˆ¬è™«
        """
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        filename_lower = file.filename.lower()
        if not (filename_lower.endswith('.xml') or filename_lower.endswith('.xml.bz2') or filename_lower.endswith('.xml.gz')):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒXMLæ ¼å¼çš„dumpæ–‡ä»¶ï¼ˆ.xml, .xml.bz2, .xml.gzï¼‰")
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        os.makedirs("temp_uploads", exist_ok=True)
        file_path = f"temp_uploads/{file.filename}"
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # å¼‚æ­¥å¤„ç†XML dumpå¯¼å…¥
        background_tasks.add_task(
            background_process_xml_dump,
            file_path=file_path,
            base_url=base_url,
            max_pages=max_pages if max_pages else None
        )
        return {"status": "processing", "message": f"XML Dumpæ–‡ä»¶å·²æ¥æ”¶ï¼Œå¼€å§‹è§£æå’Œå¯¼å…¥..."}


elif args.mode == "admin":
    print("ğŸ›¡ï¸ Server starting in ADMIN mode")
    
    @app.get("/")
    async def get_admin_ui():
        return FileResponse('static/admin.html')

    # Admin æµè§ˆæ¥å£ (å¯çœ‹ X å’Œ R)
    @app.get("/api/admin/browse")
    async def admin_browse(space: str, limit: int = 50, offset: str = None):
        collection = SPACE_R if space == "R" else SPACE_X
        offset_val = offset if offset and offset != "null" else None
        return mgr.browse_collection(collection, limit, offset_val)

    @app.post("/api/admin/promote")
    async def admin_promote(id: str = Form(...)):
        success = mgr.promote_from_x_to_r(id)
        return {"success": success}

    @app.delete("/api/admin/delete")
    async def admin_delete(space: str, id: str):
        collection = SPACE_R if space == "R" else SPACE_X
        mgr.delete_item(collection, id)
        return {"success": True}


if __name__ == "__main__":
    import uvicorn
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„ç«¯å£
    uvicorn.run(app, host="0.0.0.0", port=args.port)