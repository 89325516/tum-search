"""
ä¼˜åŒ–çš„çˆ¬è™«æ¨¡å— - æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§æ¨¡å¼
å…¼å®¹åŸæœ‰ SmartCrawler æ¥å£ï¼ŒåŒæ—¶æä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥æ‰¹é‡å¤„ç†èƒ½åŠ›
"""
import asyncio
import aiohttp
import math
import logging
import re
import requests
import time
import traceback
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup, Comment
from collections import Counter
from typing import List, Dict, Optional, Set
from concurrent.futures import ThreadPoolExecutor

try:
    from fake_useragent import UserAgent
    HAS_FAKE_USERAGENT = True
except ImportError:
    HAS_FAKE_USERAGENT = False
    logging.warning("fake_useragent not installed, using default User-Agent")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SmartCrawler:
    """
    åŒæ­¥çˆ¬è™«ç±» - ä¿æŒå‘åå…¼å®¹
    è¿™æ˜¯åŸæœ‰æ¥å£ï¼Œç¡®ä¿ç°æœ‰ä»£ç å¯ä»¥æ­£å¸¸å·¥ä½œ
    """
    def __init__(self):
        # ç†µå€¼é˜ˆå€¼ï¼šæ ¹æ®ç»éªŒï¼Œè‹±æ–‡/å¾·æ–‡è‡ªç„¶è¯­è¨€é€šå¸¸åœ¨ 3.5 åˆ° 5.8 ä¹‹é—´
        self.MIN_ENTROPY = 3.5
        self.MAX_ENTROPY = 6.0
        self.MIN_LENGTH = 30

    def calculate_shannon_entropy(self, text):
        """è®¡ç®—æ–‡æœ¬çš„é¦™å†œç†µ (Shannon Entropy)"""
        if not text:
            return 0
        prob = [float(text.count(c)) / len(text) for c in dict.fromkeys(list(text))]
        entropy = -sum([p * math.log(p) / math.log(2.0) for p in prob if p > 0])
        return entropy

    def is_valid_text(self, text):
        """æ ¸å¿ƒè¿‡æ»¤å™¨ï¼šåŸºäºé•¿åº¦å’Œç†µå€¼æ’é™¤æ— æ•ˆæ–‡æœ¬"""
        if len(text) < self.MIN_LENGTH:
            return False, "Too Short"

        entropy = self.calculate_shannon_entropy(text)

        if entropy < self.MIN_ENTROPY:
            return False, f"Low Entropy ({entropy:.2f}) - Likely menu/nav/ad"
        if entropy > self.MAX_ENTROPY:
            return False, f"High Entropy ({entropy:.2f}) - Likely code/hash"

        return True, entropy

    def _normalize_url(self, url):
        """è§„èŒƒåŒ–URLï¼šç§»é™¤fragmentï¼Œå¤„ç†æœ«å°¾æ–œæ ç­‰"""
        if not url:
            return None
        
        # ç§»é™¤fragment
        url = url.split('#')[0]
        
        # è§£æå¹¶é‡å»ºURLä»¥è§„èŒƒåŒ–
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return None
        
        # è§„èŒƒåŒ–è·¯å¾„ï¼ˆç§»é™¤./å’Œ../ï¼‰
        path = parsed.path
        if path:
            parts = path.split('/')
            normalized_parts = []
            for part in parts:
                if part == '..':
                    if normalized_parts:
                        normalized_parts.pop()
                elif part and part != '.':
                    normalized_parts.append(part)
            path = '/' + '/'.join(normalized_parts)
        
        # é‡å»ºURL
        normalized = f"{parsed.scheme}://{parsed.netloc}{path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        
        return normalized
    
    def _is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or len(url) > 2048:  # URLé•¿åº¦é™åˆ¶
            return False
        
        parsed = urlparse(url)
        # åªæ¥å—httpå’Œhttps
        if parsed.scheme not in ['http', 'https']:
            return False
        
        # è¿‡æ»¤æ— æ•ˆåè®®
        if url.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
            return False
        
        return True
    
    def parse(self, url):
        """
        çˆ¬å–å¹¶æ‹†åˆ†å›¾æ–‡ - åŒæ­¥æ¥å£ï¼ˆå‘åå…¼å®¹ï¼‰
        è¿”å›æ ¼å¼: {"url": str, "texts": List[str], "images": List[str], "links": List[str]}
        """
        # è¾“å…¥éªŒè¯
        if not url or not isinstance(url, str):
            logger.error(f"Invalid URL input: {url}")
            return None
        
        if not self._is_valid_url(url):
            logger.error(f"Invalid URL format: {url}")
            return None
        
        # è§„èŒƒåŒ–URL
        url = self._normalize_url(url)
        if not url:
            logger.error(f"Failed to normalize URL: {url}")
            return None
        
        try:
            # æ”¹è¿›çš„HTTP Headersï¼Œæ›´åƒçœŸå®æµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,de;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
            response.raise_for_status()

            # æ”¹è¿›çš„ç¼–ç æ£€æµ‹ï¼šä¼˜å…ˆä½¿ç”¨å“åº”å£°æ˜çš„ç¼–ç ï¼Œå¦åˆ™å°è¯•æ£€æµ‹
            if response.encoding:
                try:
                    html = response.text
                except UnicodeDecodeError:
                    # å¦‚æœå£°æ˜çš„ç¼–ç å¤±è´¥ï¼Œå°è¯•UTF-8
                    html = response.content.decode('utf-8', errors='replace')
            else:
                # å°è¯•å¤šç§å¸¸è§ç¼–ç 
                encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
                html = None
                for encoding in encodings:
                    try:
                        html = response.content.decode(encoding)
                        break
                    except (UnicodeDecodeError, LookupError):
                        continue
                if html is None:
                    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶æ›¿æ¢é”™è¯¯å­—ç¬¦
                    html = response.content.decode('utf-8', errors='replace')

            # å°è¯•ä½¿ç”¨lxmlï¼ˆæ›´å¿«ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°html.parser
            try:
                soup = BeautifulSoup(html, 'lxml')
            except Exception:
                logger.debug(f"lxml parser failed for {url}, falling back to html.parser")
                soup = BeautifulSoup(html, 'html.parser')

            # 1. æå–å›¾åƒ
            images = []
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src')
                if src and not src.startswith('data:'):
                    full_url = urljoin(url, src)
                    # æ”¹è¿›çš„æ‰©å±•åæå–ï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œfragment
                    ext = full_url.split('.')[-1].lower().split('?')[0].split('#')[0]
                    if ext in ['jpg', 'jpeg', 'png', 'webp', 'gif', 'svg']:
                        images.append(full_url)

            # 1.5 Remove Noise (Navigation, Footer, Scripts, etc.)
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'noscript', 'iframe', 'svg']):
                element.decompose()
            
            # Remove elements with specific classes/ids indicating noise
            noise_keywords = ['menu', 'cookie', 'popup', 'banner', 'sidebar', 'search', 'language', 'login', 'copyright']
            for tag in list(soup.find_all(True)):
                if not hasattr(tag, 'attrs') or tag.attrs is None:
                    continue
                    
                # Check class and id
                classes = tag.get('class', [])
                if isinstance(classes, list):
                    classes = ' '.join(classes)
                ids = tag.get('id', '')
                
                combined = (str(classes) + " " + str(ids)).lower()
                if any(keyword in combined for keyword in noise_keywords):
                    tag.decompose()

            # ç§»é™¤æ³¨é‡Š
            for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):
                comment.extract()

            # 2. æå–å¹¶æ¸…æ´—æ–‡æœ¬
            text_blocks = []
            # ä¼˜å…ˆæå–æ­£æ–‡ç›¸å…³çš„æ ‡ç­¾
            for tag in soup.find_all(['p', 'article', 'main', 'section', 'div']):
                text = tag.get_text(strip=True, separator=' ')
                
                # Filter out common UI text
                ui_phrases = [
                    "close menu", "search navigation", "reset search", 
                    "all rights reserved", "privacy policy", "legal notice",
                    "cookie", "accept", "decline", "skip to content"
                ]
                if any(phrase in text.lower() for phrase in ui_phrases):
                    continue
                    
                valid, reason = self.is_valid_text(text)
                if valid:
                    text_blocks.append(text)
            
            # å»é‡ä½†ä¿ç•™é¡ºåº
            text_blocks = list(dict.fromkeys(text_blocks))

            # 3. Extract Links (Recursive Crawling) - æ”¹è¿›ï¼šè¿‡æ»¤æ— æ•ˆé“¾æ¥
            links = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                
                # è¿‡æ»¤æ— æ•ˆåè®®
                if href.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
                    continue
                
                full_url = urljoin(url, href)
                
                # è§„èŒƒåŒ–URL
                parsed = urlparse(full_url)
                if parsed.scheme in ['http', 'https']:
                    # è§„èŒƒåŒ–URL
                    normalized = self._normalize_url(full_url)
                    if normalized and len(normalized) <= 2048:  # URLé•¿åº¦é™åˆ¶
                        links.append(normalized)
            
            # Deduplicate
            links = list(dict.fromkeys(links))

            return {
                "url": url,
                "texts": text_blocks,
                "images": images[:5],  # é™åˆ¶æ¯é¡µæœ€å¤šå–å‰5å¼ å›¾
                "links": links
            }

        except requests.exceptions.Timeout:
            logger.error(f"Timeout for {url}")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error for {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"Crawler Error for {url}: {e}")
            traceback.print_exc()
            return None


class OptimizedCrawler:
    """
    ä¼˜åŒ–çš„å¼‚æ­¥çˆ¬è™«ç±» - æ”¯æŒæ‰¹é‡å¹¶å‘å¤„ç†
    æä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥çˆ¬å–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒä¸ SmartCrawler å…¼å®¹çš„è¿”å›æ ¼å¼
    """
    def __init__(self, concurrency=5, timeout=10, delay=1.0, max_rate=None, max_redirects=5, verify_ssl=True):
        """
        Args:
            concurrency: å¹¶å‘æ•°ï¼Œé˜²æ­¢å°IP
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            delay: è¯·æ±‚ä¹‹é—´çš„æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹
            max_rate: å…¨å±€æœ€å¤§è¯·æ±‚é€Ÿç‡ï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            max_redirects: æœ€å¤§é‡å®šå‘æ·±åº¦ï¼Œé˜²æ­¢æ— é™å¾ªç¯
            verify_ssl: æ˜¯å¦éªŒè¯SSLè¯ä¹¦ï¼ˆé»˜è®¤Trueï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®å¯ç”¨ï¼‰
        """
        if HAS_FAKE_USERAGENT:
            self.ua = UserAgent()
        else:
            self.ua = None
        
        self.semaphore = asyncio.Semaphore(concurrency)
        self.timeout = aiohttp.ClientTimeout(total=timeout)
        self.executor = ThreadPoolExecutor(max_workers=4)  # ç”¨äºCPUå¯†é›†å‹ä»»åŠ¡
        
        # åçˆ¬è™«ï¼šè¯·æ±‚å»¶è¿Ÿå’Œé€Ÿç‡é™åˆ¶
        self.delay = delay  # è¯·æ±‚ä¹‹é—´çš„æœ€å°å»¶è¿Ÿ
        self.last_request_time = {}  # æŒ‰åŸŸåè®°å½•æœ€åè¯·æ±‚æ—¶é—´
        self.max_rate = max_rate  # å…¨å±€é€Ÿç‡é™åˆ¶
        self.max_redirects = max_redirects  # æœ€å¤§é‡å®šå‘æ·±åº¦
        self.verify_ssl = verify_ssl  # SSLéªŒè¯
        
        # çº¿ç¨‹å®‰å…¨ï¼šä½¿ç”¨é”ä¿æŠ¤å…±äº«çŠ¶æ€
        self._rate_limit_lock = asyncio.Lock()
        self._domain_delay_lock = asyncio.Lock()
        self._last_url_lock = asyncio.Lock()
        
        self.rate_limiter = None
        if max_rate:
            # ä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•å®ç°é€Ÿç‡é™åˆ¶
            self.rate_limiter = {
                'tokens': max_rate,
                'last_update': time.time(),
                'max_tokens': max_rate
            }
        
        # ä¼˜åŒ–åçš„é˜ˆå€¼
        # ç†µå€¼é˜ˆå€¼ï¼šæ ¹æ®ç»éªŒï¼Œè‹±æ–‡/å¾·æ–‡è‡ªç„¶è¯­è¨€é€šå¸¸åœ¨ 3.5 åˆ° 5.8 ä¹‹é—´
        self.MIN_LENGTH = 30
        self.MIN_ENTROPY = 3.5
        self.MAX_ENTROPY = 6.5
        
        # é¢„ç¼–è¯‘æ­£åˆ™ï¼Œæå‡é€Ÿåº¦
        self.noise_pattern = re.compile(
            r'menu|cookie|popup|banner|sidebar|search|language|login|copyright|footer|header', 
            re.IGNORECASE
        )
        self.clean_tags = ['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'noscript', 'iframe', 'svg']
        
        # UIçŸ­è¯­è¿‡æ»¤
        self.ui_phrases = re.compile(
            r'close menu|search navigation|reset search|all rights reserved|privacy policy|legal notice|cookie|accept|decline',
            re.IGNORECASE
        )

    def _get_user_agent(self):
        """è·å–User-Agent"""
        if self.ua:
            try:
                return self.ua.random
            except:
                pass
        return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    
    def _normalize_url(self, url):
        """è§„èŒƒåŒ–URLï¼šç§»é™¤fragmentï¼Œå¤„ç†æœ«å°¾æ–œæ ç­‰"""
        if not url:
            return None
        
        # ç§»é™¤fragment
        url = url.split('#')[0]
        
        # è§£æå¹¶é‡å»ºURLä»¥è§„èŒƒåŒ–
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return None
        
        # è§„èŒƒåŒ–è·¯å¾„ï¼ˆç§»é™¤./å’Œ../ï¼‰
        path = parsed.path
        if path:
            # ç®€å•çš„è·¯å¾„è§„èŒƒåŒ–
            parts = path.split('/')
            normalized_parts = []
            for part in parts:
                if part == '..':
                    if normalized_parts:
                        normalized_parts.pop()
                elif part and part != '.':
                    normalized_parts.append(part)
            path = '/' + '/'.join(normalized_parts)
        
        # é‡å»ºURL
        normalized = f"{parsed.scheme}://{parsed.netloc}{path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        
        return normalized
    
    def _is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or len(url) > 2048:  # URLé•¿åº¦é™åˆ¶
            return False
        
        parsed = urlparse(url)
        # åªæ¥å—httpå’Œhttps
        if parsed.scheme not in ['http', 'https']:
            return False
        
        # è¿‡æ»¤æ— æ•ˆåè®®
        if url.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
            return False
        
        return True
    
    async def _get_headers(self, url=None):
        """è·å–å®Œæ•´çš„HTTP Headersï¼Œæ›´åƒçœŸå®æµè§ˆå™¨"""
        headers = {
            'User-Agent': self._get_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,de;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        
        # å¦‚æœæœ‰Refererï¼Œæ·»åŠ Refererï¼ˆæ¨¡æ‹Ÿä»å…¶ä»–é¡µé¢è·³è½¬ï¼‰
        async with self._last_url_lock:
            if url and hasattr(self, '_last_url') and self._last_url:
                parsed_current = urlparse(url)
                parsed_last = urlparse(self._last_url)
                if parsed_current.netloc == parsed_last.netloc:
                    headers['Referer'] = self._last_url
        
        return headers
    
    async def _rate_limit(self):
        """å…¨å±€é€Ÿç‡é™åˆ¶ï¼ˆä»¤ç‰Œæ¡¶ç®—æ³•ï¼‰- çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        if not self.max_rate:
            return
        
        async with self._rate_limit_lock:
            now = time.time()
            rate_limiter = self.rate_limiter
            
            # æ›´æ–°ä»¤ç‰Œ
            elapsed = now - rate_limiter['last_update']
            rate_limiter['tokens'] = min(
                rate_limiter['max_tokens'],
                rate_limiter['tokens'] + elapsed * self.max_rate
            )
            rate_limiter['last_update'] = now
            
            # å¦‚æœæ²¡æœ‰ä»¤ç‰Œï¼Œç­‰å¾…
            if rate_limiter['tokens'] < 1:
                wait_time = (1 - rate_limiter['tokens']) / self.max_rate
                # é‡Šæ”¾é”åç­‰å¾…ï¼Œé¿å…é˜»å¡å…¶ä»–è¯·æ±‚
                await asyncio.sleep(wait_time)
                # é‡æ–°è·å–é”å¹¶æ›´æ–°
                async with self._rate_limit_lock:
                    rate_limiter['tokens'] = 0
            
            # æ¶ˆè€—ä¸€ä¸ªä»¤ç‰Œ
            rate_limiter['tokens'] -= 1
    
    async def _domain_delay(self, url):
        """æŒ‰åŸŸåå»¶è¿Ÿï¼Œé˜²æ­¢å¯¹åŒä¸€åŸŸåè¯·æ±‚è¿‡äºé¢‘ç¹ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        if self.delay <= 0:
            return
        
        parsed = urlparse(url)
        domain = parsed.netloc
        
        async with self._domain_delay_lock:
            now = time.time()
            
            if domain in self.last_request_time:
                elapsed = now - self.last_request_time[domain]
                if elapsed < self.delay:
                    wait_time = self.delay - elapsed
                    # é‡Šæ”¾é”åç­‰å¾…
                    await asyncio.sleep(wait_time)
                    # é‡æ–°è·å–é”å¹¶æ›´æ–°
                    async with self._domain_delay_lock:
                        self.last_request_time[domain] = time.time()
                else:
                    self.last_request_time[domain] = now
            else:
                self.last_request_time[domain] = now

    async def fetch(self, session, url, redirect_count=0, redirect_history=None):
        """
        å¼‚æ­¥è·å–é¡µé¢å†…å®¹ï¼Œå¸¦è‡ªåŠ¨é‡è¯•å’Œåçˆ¬è™«æªæ–½
        
        Args:
            session: aiohttpä¼šè¯
            url: ç›®æ ‡URL
            redirect_count: å½“å‰é‡å®šå‘æ·±åº¦
            redirect_history: é‡å®šå‘å†å²ï¼ˆç”¨äºæ£€æµ‹å¾ªç¯ï¼‰
        """
        # è¾“å…¥éªŒè¯
        if not url or not self._is_valid_url(url):
            logger.warning(f"Invalid URL: {url}")
            return None
        
        # è§„èŒƒåŒ–URL
        url = self._normalize_url(url)
        if not url:
            logger.warning(f"Failed to normalize URL: {url}")
            return None
        
        # æ£€æŸ¥é‡å®šå‘æ·±åº¦
        if redirect_count >= self.max_redirects:
            logger.warning(f"Max redirects ({self.max_redirects}) reached for {url}")
            return None
        
        # åˆå§‹åŒ–é‡å®šå‘å†å²
        if redirect_history is None:
            redirect_history = set()
        
        # æ£€æŸ¥é‡å®šå‘å¾ªç¯
        if url in redirect_history:
            logger.warning(f"Redirect loop detected: {url} -> {redirect_history}")
            return None
        
        # åçˆ¬è™«ï¼šé€Ÿç‡é™åˆ¶å’ŒåŸŸåå»¶è¿Ÿ
        await self._rate_limit()
        await self._domain_delay(url)
        
        retries = 3
        for i in range(retries):
            try:
                headers = await self._get_headers(url)
                async with self.semaphore:
                    async with session.get(
                        url, 
                        headers=headers, 
                        timeout=self.timeout, 
                        ssl=self.verify_ssl, 
                        allow_redirects=False
                    ) as response:
                        if response.status == 200:
                            # è®°å½•æœ€åè®¿é—®çš„URLï¼ˆç”¨äºRefererï¼‰
                            async with self._last_url_lock:
                                self._last_url = url
                            # aiohttpä¼šè‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œä½†æ·»åŠ é”™è¯¯å¤„ç†
                            try:
                                return await response.text()
                            except UnicodeDecodeError:
                                # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨è§£ç 
                                content = await response.read()
                                # å°è¯•å¸¸è§ç¼–ç 
                                encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
                                for encoding in encodings:
                                    try:
                                        return content.decode(encoding)
                                    except (UnicodeDecodeError, LookupError):
                                        continue
                                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶æ›¿æ¢é”™è¯¯å­—ç¬¦
                                return content.decode('utf-8', errors='replace')
                        elif response.status in [301, 302, 303, 307, 308]:
                            # å¤„ç†é‡å®šå‘
                            redirect_url = response.headers.get('Location')
                            if redirect_url:
                                logger.info(f"Redirecting {url} -> {redirect_url} (depth: {redirect_count + 1})")
                                # å¤„ç†ç›¸å¯¹å’Œç»å¯¹URL
                                absolute_redirect = urljoin(url, redirect_url)
                                # è§„èŒƒåŒ–é‡å®šå‘URL
                                absolute_redirect = self._normalize_url(absolute_redirect)
                                
                                if absolute_redirect and self._is_valid_url(absolute_redirect):
                                    # æ›´æ–°é‡å®šå‘å†å²
                                    new_history = redirect_history | {url}
                                    # é€’å½’å¤„ç†é‡å®šå‘
                                    return await self.fetch(
                                        session, 
                                        absolute_redirect, 
                                        redirect_count + 1, 
                                        new_history
                                    )
                                else:
                                    logger.warning(f"Invalid redirect URL: {redirect_url}")
                        else:
                            logger.warning(f"Status {response.status} for {url}")
            except asyncio.TimeoutError:
                logger.debug(f"Timeout {i+1}/{retries} for {url}")
            except aiohttp.ClientError as e:
                logger.debug(f"Client error {i+1}/{retries} for {url}: {e}")
            except Exception as e:
                logger.debug(f"Retry {i+1}/{retries} for {url}: {e}")
            
            if i < retries - 1:
                await asyncio.sleep(2 ** i)  # æŒ‡æ•°é€€é¿ç­–ç•¥
        
        return None

    def fast_entropy(self, text):
        """ä¼˜åŒ–çš„é¦™å†œç†µè®¡ç®— (ä½¿ç”¨äº† Counter)"""
        if not text or len(text) < 2:
            return 0
        length = len(text)
        counts = Counter(text)
        probs = (count / length for count in counts.values())
        return -sum(p * math.log2(p) for p in probs if p > 0)

    def clean_dom(self, soup):
        """æ¸…æ´— DOM æ ‘ï¼Œç§»é™¤å™ªå£°èŠ‚ç‚¹"""
        # 1. ç§»é™¤æ— ç”¨æ ‡ç­¾
        for tag in soup(self.clean_tags):
            tag.decompose()
        
        # 2. ç§»é™¤æ³¨é‡Š
        for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):
            comment.extract()

        # 3. åŸºäº Class/ID çš„å¯å‘å¼ç§»é™¤
        for tag in list(soup.find_all(True)):
            attr_str = str(tag.get('class', '')) + " " + str(tag.get('id', ''))
            if self.noise_pattern.search(attr_str):
                tag.decompose()

    def extract_content_smart(self, soup, url):
        """
        æ™ºèƒ½å†…å®¹æå–ï¼šä¿ç•™æ®µè½ç»“æ„ï¼Œè€Œä¸æ˜¯æ‰“æ•£çš„å¥å­
        è¿”å›æ ¼å¼ä¸ SmartCrawler.parse() å…¼å®¹
        """
        self.clean_dom(soup)

        # 1. æå–å›¾ç‰‡
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if not src or src.startswith('data:'):
                continue
            
            full_url = urljoin(url, src)
            # æ”¹è¿›çš„æ‰©å±•åæå–ï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œfragment
            ext = full_url.split('.')[-1].lower().split('?')[0].split('#')[0]
            if ext in ['jpg', 'jpeg', 'png', 'webp', 'gif', 'svg']:
                images.append(full_url)

        # 2. æå–é“¾æ¥ï¼ˆæ”¹è¿›ï¼šè¿‡æ»¤æ— æ•ˆé“¾æ¥ï¼‰
        links = set()
        for a in soup.find_all('a', href=True):
            href = a['href']
            
            # è¿‡æ»¤æ— æ•ˆåè®®
            if href.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
                continue
            
            full_link = urljoin(url, href)
            
            # è§„èŒƒåŒ–URL
            normalized = self._normalize_url(full_link)
            if normalized and self._is_valid_url(normalized):
                links.add(normalized)

        # 3. æå–æ–‡æœ¬ (æ ¸å¿ƒä¼˜åŒ–ï¼šåŸºäºå—çš„æå–)
        text_blocks = []
        
        # ä¼˜å…ˆæå–æ­£æ–‡ç›¸å…³çš„æ ‡ç­¾
        for tag in soup.find_all(['p', 'article', 'main', 'section', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = tag.get_text(strip=True, separator=' ')
            
            # è¿‡æ»¤UIçŸ­è¯­
            if self.ui_phrases.search(text):
                continue
            
            # é•¿åº¦æ£€æŸ¥
            if len(text) < self.MIN_LENGTH:
                continue
            
            # ç†µå€¼æ£€æŸ¥
            entropy = self.fast_entropy(text)
            if self.MIN_ENTROPY <= entropy <= self.MAX_ENTROPY:
                text_blocks.append(text)

        # å»é‡ä½†ä¿ç•™é¡ºåº
        text_blocks = list(dict.fromkeys(text_blocks))

        # è¿”å›æ ¼å¼ä¸ SmartCrawler å…¼å®¹
        return {
            "url": url,
            "title": soup.title.string.strip() if soup.title and soup.title.string else "",
            "texts": text_blocks,  # å…³é”®ï¼šä½¿ç”¨ texts è€Œä¸æ˜¯ content_blocks
            "images": images[:5],
            "links": list(links)
        }

    async def process_url(self, session, url):
        """å•ä¸ª URL çš„å¤„ç†æµ"""
        html = await self.fetch(session, url)
        if not html:
            return None

        # å°† CPU å¯†é›†å‹çš„è§£æä»»åŠ¡æ”¾åˆ°çº¿ç¨‹æ± ä¸­ï¼Œé¿å…é˜»å¡ Event Loop
        loop = asyncio.get_running_loop()
        try:
            result = await loop.run_in_executor(self.executor, self._parse_sync, html, url)
            return result
        except Exception as e:
            logger.error(f"Parse error {url}: {e}")
            return None

    def _parse_sync(self, html, url):
        """åŒæ­¥è§£æé€»è¾‘ (è¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­) - å¸¦è§£æå™¨å›é€€"""
        try:
            # å°è¯•ä½¿ç”¨lxmlï¼ˆæ›´å¿«ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°html.parser
            try:
                soup = BeautifulSoup(html, 'lxml')
            except Exception:
                logger.debug(f"lxml parser failed for {url}, falling back to html.parser")
                soup = BeautifulSoup(html, 'html.parser')
            
            return self.extract_content_smart(soup, url)
        except Exception as e:
            logger.error(f"BeautifulSoup parse error for {url}: {e}")
            return None

    async def run(self, urls: List[str]) -> List[Dict]:
        """
        ä¸»å…¥å£ - å¼‚æ­¥æ‰¹é‡å¤„ç†URLåˆ—è¡¨
        Args:
            urls: URLåˆ—è¡¨
        Returns:
            æˆåŠŸçˆ¬å–çš„ç»“æœåˆ—è¡¨
        """
        async with aiohttp.ClientSession() as session:
            tasks = [self.process_url(session, url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è¿‡æ»¤æ‰å¼‚å¸¸å’ŒNone
            valid_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Error processing {urls[i]}: {result}")
                elif result is not None:
                    valid_results.append(result)
            
            return valid_results

    def parse(self, url: str) -> Optional[Dict]:
        """
        åŒæ­¥æ¥å£ - å…¼å®¹ SmartCrawler.parse()
        ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾›åŒæ­¥æ¥å£
        """
        # è¾“å…¥éªŒè¯
        if not url or not isinstance(url, str):
            logger.error(f"Invalid URL input: {url}")
            return None
        
        if not self._is_valid_url(url):
            logger.error(f"Invalid URL format: {url}")
            return None
        
        try:
            # ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨å¼‚æ­¥æ–¹æ³•
            try:
                loop = asyncio.get_running_loop()
                # å¦‚æœäº‹ä»¶å¾ªç¯å·²ç»åœ¨è¿è¡Œï¼Œä½¿ç”¨çº¿ç¨‹æ± 
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.run([url]))
                    results = future.result(timeout=60)  # æ·»åŠ è¶…æ—¶
                    return results[0] if results else None
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥ä½¿ç”¨asyncio.run
                results = asyncio.run(self.run([url]))
                return results[0] if results else None
        except Exception as e:
            logger.error(f"Error in parse({url}): {e}")
            return None

    def close(self):
        """æ˜¾å¼å…³é—­èµ„æºï¼ˆæ¨èä½¿ç”¨ï¼‰"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
    
    def __del__(self):
        """æ¸…ç†èµ„æºï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
        except:
            pass  # å¿½ç•¥æ¸…ç†æ—¶çš„é”™è¯¯
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()
        return False


# ä¸ºäº†å‘åå…¼å®¹ï¼Œé»˜è®¤å¯¼å‡º SmartCrawler
# å¦‚æœé¡¹ç›®éœ€è¦é«˜æ€§èƒ½ï¼Œå¯ä»¥æ”¹ç”¨ OptimizedCrawler
if __name__ == "__main__":
    import time
    
    # æµ‹è¯• SmartCrawler (åŒæ­¥)
    print("=" * 60)
    print("Testing SmartCrawler (Synchronous)")
    print("=" * 60)
    crawler_sync = SmartCrawler()
    start = time.time()
    result = crawler_sync.parse("https://www.tum.de/en/")
    end = time.time()
    
    if result:
        print(f"\nâœ… Crawled in {end - start:.2f} seconds")
        print(f"ğŸ“„ Title: {result.get('title', 'N/A')}")
        print(f"ğŸ“ Text blocks: {len(result['texts'])}")
        print(f"ğŸ–¼ï¸ Images: {len(result['images'])}")
        print(f"ğŸ”— Links: {len(result['links'])}")
        print("\nğŸ“ Sample texts:")
        for i, text in enumerate(result['texts'][:3], 1):
            print(f"   {i}. {text[:80]}...")
    
    # æµ‹è¯• OptimizedCrawler (å¼‚æ­¥)
    print("\n" + "=" * 60)
    print("Testing OptimizedCrawler (Asynchronous)")
    print("=" * 60)
    crawler_async = OptimizedCrawler(concurrency=3)
    
    target_urls = [
        "https://www.tum.de/en/",
        "https://www.tum.de/en/studies/",
        "https://www.tum.de/en/research/"
    ]
    
    start = time.time()
    results = asyncio.run(crawler_async.run(target_urls))
    end = time.time()
    
    print(f"\nâœ… Crawled {len(results)} pages in {end - start:.2f} seconds")
    for result in results:
        if result:
            print(f"\nğŸ“„ {result['url']}")
            print(f"   Texts: {len(result['texts'])} | Images: {len(result['images'])} | Links: {len(result['links'])}")
