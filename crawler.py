"""
ä¼˜åŒ–çš„çˆ¬è™«æ¨¡å— - æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§æ¨¡å¼
å…¼å®¹åŸæœ‰ SmartCrawler æ¥å£ï¼ŒåŒæ—¶æä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥æ‰¹é‡å¤„ç†èƒ½åŠ›
"""
import asyncio
import aiohttp
import math
import logging
import re
import requests
import time
import traceback
import threading
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup, Comment
from collections import Counter
from typing import List, Dict, Optional, Set
from concurrent.futures import ThreadPoolExecutor

try:
    from fake_useragent import UserAgent
    HAS_FAKE_USERAGENT = True
except ImportError:
    HAS_FAKE_USERAGENT = False
    logging.warning("fake_useragent not installed, using default User-Agent")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SmartCrawler:
    """
    åŒæ­¥çˆ¬è™«ç±» - ä¿æŒå‘åå…¼å®¹
    è¿™æ˜¯åŸæœ‰æ¥å£ï¼Œç¡®ä¿ç°æœ‰ä»£ç å¯ä»¥æ­£å¸¸å·¥ä½œ
    """
    def __init__(self):
        # ç†µå€¼é˜ˆå€¼ï¼šæ ¹æ®ç»éªŒï¼Œè‹±æ–‡/å¾·æ–‡è‡ªç„¶è¯­è¨€é€šå¸¸åœ¨ 3.5 åˆ° 5.8 ä¹‹é—´
        self.MIN_ENTROPY = 3.5
        self.MAX_ENTROPY = 6.0
        self.MIN_LENGTH = 30

    def calculate_shannon_entropy(self, text):
        """è®¡ç®—æ–‡æœ¬çš„é¦™å†œç†µ (Shannon Entropy)"""
        if not text:
            return 0
        prob = [float(text.count(c)) / len(text) for c in dict.fromkeys(list(text))]
        entropy = -sum([p * math.log(p) / math.log(2.0) for p in prob if p > 0])
        return entropy

    def is_valid_text(self, text):
        """æ ¸å¿ƒè¿‡æ»¤å™¨ï¼šåŸºäºé•¿åº¦å’Œç†µå€¼æ’é™¤æ— æ•ˆæ–‡æœ¬"""
        if len(text) < self.MIN_LENGTH:
            return False, "Too Short"

        entropy = self.calculate_shannon_entropy(text)

        if entropy < self.MIN_ENTROPY:
            return False, f"Low Entropy ({entropy:.2f}) - Likely menu/nav/ad"
        if entropy > self.MAX_ENTROPY:
            return False, f"High Entropy ({entropy:.2f}) - Likely code/hash"

        return True, entropy

    def _normalize_url(self, url):
        """è§„èŒƒåŒ–URLï¼šç§»é™¤fragmentï¼Œå¤„ç†æœ«å°¾æ–œæ ç­‰"""
        if not url:
            return None
        
        # ç§»é™¤fragment
        url = url.split('#')[0]
        
        # è§£æå¹¶é‡å»ºURLä»¥è§„èŒƒåŒ–
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return None
        
        # è§„èŒƒåŒ–è·¯å¾„ï¼ˆç§»é™¤./å’Œ../ï¼‰
        path = parsed.path
        if path:
            parts = path.split('/')
            normalized_parts = []
            for part in parts:
                if part == '..':
                    if normalized_parts:
                        normalized_parts.pop()
                elif part and part != '.':
                    normalized_parts.append(part)
            path = '/' + '/'.join(normalized_parts)
        
        # é‡å»ºURL
        normalized = f"{parsed.scheme}://{parsed.netloc}{path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        
        return normalized
    
    def _is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or len(url) > 2048:  # URLé•¿åº¦é™åˆ¶
            return False
        
        parsed = urlparse(url)
        # åªæ¥å—httpå’Œhttps
        if parsed.scheme not in ['http', 'https']:
            return False
        
        # è¿‡æ»¤æ— æ•ˆåè®®
        if url.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
            return False
        
        return True
    
    def parse(self, url):
        """
        çˆ¬å–å¹¶æ‹†åˆ†å›¾æ–‡ - åŒæ­¥æ¥å£ï¼ˆå‘åå…¼å®¹ï¼‰
        è¿”å›æ ¼å¼: {"url": str, "texts": List[str], "images": List[str], "links": List[str]}
        """
        # è¾“å…¥éªŒè¯
        if not url or not isinstance(url, str):
            logger.error(f"Invalid URL input: {url}")
            return None
        
        if not self._is_valid_url(url):
            logger.error(f"Invalid URL format: {url}")
            return None
        
        # è§„èŒƒåŒ–URL
        url = self._normalize_url(url)
        if not url:
            logger.error(f"Failed to normalize URL: {url}")
            return None
        
        try:
            # æ”¹è¿›çš„HTTP Headersï¼Œæ›´åƒçœŸå®æµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,de;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
            response.raise_for_status()

            # æ”¹è¿›çš„ç¼–ç æ£€æµ‹ï¼šä¼˜å…ˆä½¿ç”¨å“åº”å£°æ˜çš„ç¼–ç ï¼Œå¦åˆ™å°è¯•æ£€æµ‹
            if response.encoding:
                try:
                    html = response.text
                except UnicodeDecodeError:
                    # å¦‚æœå£°æ˜çš„ç¼–ç å¤±è´¥ï¼Œå°è¯•UTF-8
                    html = response.content.decode('utf-8', errors='replace')
            else:
                # å°è¯•å¤šç§å¸¸è§ç¼–ç 
                encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
                html = None
                for encoding in encodings:
                    try:
                        html = response.content.decode(encoding)
                        break
                    except (UnicodeDecodeError, LookupError):
                        continue
                if html is None:
                    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶æ›¿æ¢é”™è¯¯å­—ç¬¦
                    html = response.content.decode('utf-8', errors='replace')

            # å°è¯•ä½¿ç”¨lxmlï¼ˆæ›´å¿«ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°html.parser
            try:
                soup = BeautifulSoup(html, 'lxml')
            except Exception:
                logger.debug(f"lxml parser failed for {url}, falling back to html.parser")
                soup = BeautifulSoup(html, 'html.parser')

            # 1. æå–å›¾åƒ
            images = []
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src')
                if src and not src.startswith('data:'):
                    full_url = urljoin(url, src)
                    # æ”¹è¿›çš„æ‰©å±•åæå–ï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œfragment
                    ext = full_url.split('.')[-1].lower().split('?')[0].split('#')[0]
                    if ext in ['jpg', 'jpeg', 'png', 'webp', 'gif', 'svg']:
                        images.append(full_url)

            # 1.5 Remove Noise (Navigation, Footer, Scripts, etc.)
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'noscript', 'iframe', 'svg']):
                element.decompose()
            
            # Remove elements with specific classes/ids indicating noise
            noise_keywords = ['menu', 'cookie', 'popup', 'banner', 'sidebar', 'search', 'language', 'login', 'copyright']
            for tag in list(soup.find_all(True)):
                if not hasattr(tag, 'attrs') or tag.attrs is None:
                    continue
                    
                # Check class and id
                classes = tag.get('class', [])
                if isinstance(classes, list):
                    classes = ' '.join(classes)
                ids = tag.get('id', '')
                
                combined = (str(classes) + " " + str(ids)).lower()
                if any(keyword in combined for keyword in noise_keywords):
                    tag.decompose()

            # ç§»é™¤æ³¨é‡Š
            for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):
                comment.extract()

            # 2. æå–å¹¶æ¸…æ´—æ–‡æœ¬
            text_blocks = []
            # ä¼˜å…ˆæå–æ­£æ–‡ç›¸å…³çš„æ ‡ç­¾
            for tag in soup.find_all(['p', 'article', 'main', 'section', 'div']):
                text = tag.get_text(strip=True, separator=' ')
                
                # Filter out common UI text
                ui_phrases = [
                    "close menu", "search navigation", "reset search", 
                    "all rights reserved", "privacy policy", "legal notice",
                    "cookie", "accept", "decline", "skip to content"
                ]
                if any(phrase in text.lower() for phrase in ui_phrases):
                    continue
                    
                valid, reason = self.is_valid_text(text)
                if valid:
                    text_blocks.append(text)
            
            # å»é‡ä½†ä¿ç•™é¡ºåº
            text_blocks = list(dict.fromkeys(text_blocks))

            # 3. Extract Links (Recursive Crawling) - æ”¹è¿›ï¼šè¿‡æ»¤æ— æ•ˆé“¾æ¥
            links = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                
                # è¿‡æ»¤æ— æ•ˆåè®®
                if href.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
                    continue
                
                full_url = urljoin(url, href)
                
                # è§„èŒƒåŒ–URL
                parsed = urlparse(full_url)
                if parsed.scheme in ['http', 'https']:
                    # è§„èŒƒåŒ–URL
                    normalized = self._normalize_url(full_url)
                    if normalized and len(normalized) <= 2048:  # URLé•¿åº¦é™åˆ¶
                        links.append(normalized)
            
            # Deduplicate
            links = list(dict.fromkeys(links))

            return {
                "url": url,
                "texts": text_blocks,
                "images": images[:5],  # é™åˆ¶æ¯é¡µæœ€å¤šå–å‰5å¼ å›¾
                "links": links
            }

        except requests.exceptions.Timeout:
            logger.error(f"Timeout for {url}")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error for {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"Crawler Error for {url}: {e}")
            traceback.print_exc()
            return None


class OptimizedCrawler:
    """
    ä¼˜åŒ–çš„å¼‚æ­¥çˆ¬è™«ç±» - æ”¯æŒæ‰¹é‡å¹¶å‘å¤„ç†å’Œæ·±åº¦é€’å½’çˆ¬å–
    æä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥çˆ¬å–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒä¸ SmartCrawler å…¼å®¹çš„è¿”å›æ ¼å¼
    """
    def __init__(self, concurrency=5, timeout=10, delay=1.0, max_rate=None, max_redirects=5, verify_ssl=True, 
                 enable_cache=True, max_cache_size=3000, same_domain_only=True, max_path_depth=None,
                 exclude_static=True, exclude_extensions=None, enable_link_prioritization=True):
        """
        Args:
            concurrency: å¹¶å‘æ•°ï¼Œé˜²æ­¢å°IP
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            delay: è¯·æ±‚ä¹‹é—´çš„æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹
            max_rate: å…¨å±€æœ€å¤§è¯·æ±‚é€Ÿç‡ï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            max_redirects: æœ€å¤§é‡å®šå‘æ·±åº¦ï¼Œé˜²æ­¢æ— é™å¾ªç¯
            verify_ssl: æ˜¯å¦éªŒè¯SSLè¯ä¹¦ï¼ˆé»˜è®¤Trueï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®å¯ç”¨ï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨URLç¼“å­˜ï¼Œé¿å…é‡å¤çˆ¬å–
            max_cache_size: æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆURLæ•°é‡ï¼‰ï¼Œé»˜è®¤å¢åŠ åˆ°3000ä»¥æ”¯æŒæ›´æ·±çˆ¬å–
            same_domain_only: æ˜¯å¦åªçˆ¬å–åŒä¸€åŸŸåï¼ˆæ·±åº¦çˆ¬å–æ—¶ï¼‰
            max_path_depth: æœ€å¤§è·¯å¾„æ·±åº¦é™åˆ¶ï¼ˆNoneè¡¨ç¤ºæ™ºèƒ½åˆ¤æ–­ï¼ŒåŸºäºURLè¯­ä¹‰ï¼Œé«˜è´¨é‡URLå…è®¸æœ€å¤š12å±‚ï¼‰
            exclude_static: æ˜¯å¦æ’é™¤é™æ€èµ„æºæ–‡ä»¶
            exclude_extensions: è¦æ’é™¤çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ˆé»˜è®¤: pdf, jpg, png, gif, css, jsç­‰ï¼‰
            enable_link_prioritization: æ˜¯å¦å¯ç”¨é“¾æ¥ä¼˜å…ˆçº§è¯„åˆ†ç³»ç»Ÿï¼ˆé»˜è®¤Trueï¼‰
        """
        if HAS_FAKE_USERAGENT:
            self.ua = UserAgent()
        else:
            self.ua = None
        
        self.semaphore = asyncio.Semaphore(concurrency)
        self.timeout = aiohttp.ClientTimeout(total=timeout)
        self.executor = ThreadPoolExecutor(max_workers=4)  # ç”¨äºCPUå¯†é›†å‹ä»»åŠ¡
        
        # åçˆ¬è™«ï¼šè¯·æ±‚å»¶è¿Ÿå’Œé€Ÿç‡é™åˆ¶
        self.delay = delay  # è¯·æ±‚ä¹‹é—´çš„æœ€å°å»¶è¿Ÿ
        self.last_request_time = {}  # æŒ‰åŸŸåè®°å½•æœ€åè¯·æ±‚æ—¶é—´
        self.max_rate = max_rate  # å…¨å±€é€Ÿç‡é™åˆ¶
        self.max_redirects = max_redirects  # æœ€å¤§é‡å®šå‘æ·±åº¦
        self.verify_ssl = verify_ssl  # SSLéªŒè¯
        
        # çº¿ç¨‹å®‰å…¨ï¼šä½¿ç”¨é”ä¿æŠ¤å…±äº«çŠ¶æ€
        self._rate_limit_lock = asyncio.Lock()
        self._domain_delay_lock = asyncio.Lock()
        self._last_url_lock = asyncio.Lock()
        
        self.rate_limiter = None
        if max_rate:
            # ä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•å®ç°é€Ÿç‡é™åˆ¶
            self.rate_limiter = {
                'tokens': max_rate,
                'last_update': time.time(),
                'max_tokens': max_rate
            }
        
        # ä¼˜åŒ–åçš„é˜ˆå€¼
        # ç†µå€¼é˜ˆå€¼ï¼šæ ¹æ®ç»éªŒï¼Œè‹±æ–‡/å¾·æ–‡è‡ªç„¶è¯­è¨€é€šå¸¸åœ¨ 3.5 åˆ° 5.8 ä¹‹é—´
        self.MIN_LENGTH = 30
        self.MIN_ENTROPY = 3.5
        self.MAX_ENTROPY = 6.5
        
        # é¢„ç¼–è¯‘æ­£åˆ™ï¼Œæå‡é€Ÿåº¦
        self.noise_pattern = re.compile(
            r'menu|cookie|popup|banner|sidebar|search|language|login|copyright|footer|header', 
            re.IGNORECASE
        )
        self.clean_tags = ['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'noscript', 'iframe', 'svg']
        
        # UIçŸ­è¯­è¿‡æ»¤
        self.ui_phrases = re.compile(
            r'close menu|search navigation|reset search|all rights reserved|privacy policy|legal notice|cookie|accept|decline',
            re.IGNORECASE
        )
        
        # æ·±åº¦çˆ¬å–ç›¸å…³é…ç½®
        self.enable_cache = enable_cache
        self.max_cache_size = max_cache_size
        self.same_domain_only = same_domain_only
        self.max_path_depth = max_path_depth
        self.exclude_static = exclude_static
        self.enable_link_prioritization = enable_link_prioritization
        
        # é»˜è®¤æ’é™¤çš„é™æ€èµ„æºæ‰©å±•å
        if exclude_extensions is None:
            exclude_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', 
                                  '.css', '.js', '.zip', '.tar', '.gz', '.xml', '.json',
                                  '.mp4', '.mp3', '.avi', '.mov', '.wmv', '.flv',
                                  '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx']
        self.exclude_extensions = set(ext.lower() for ext in exclude_extensions)
        
        # é“¾æ¥ä¼˜å…ˆçº§è¯„åˆ†ï¼šé«˜è´¨é‡URLæ¨¡å¼ï¼ˆæ­£åˆ†ï¼‰å’Œä½è´¨é‡URLæ¨¡å¼ï¼ˆè´Ÿåˆ†ï¼‰
        self.high_quality_patterns = [
            (re.compile(r'/(article|post|news|blog|page|content|detail|view|show)/', re.I), 3.0),
            (re.compile(r'/(course|program|study|education|research|faculty|department)/', re.I), 2.5),
            (re.compile(r'/(about|info|overview|introduction)/', re.I), 2.0),
            (re.compile(r'/\d{4}/|\d{2}/', re.I), 1.0),  # æ—¥æœŸè·¯å¾„é€šå¸¸è¡¨ç¤ºæ–‡ç« 
        ]
        self.low_quality_patterns = [
            (re.compile(r'/(tag|category|author|archive|feed|rss|atom)/', re.I), -2.0),
            (re.compile(r'/(print|pdf|download|export|share|embed)/', re.I), -3.0),
            (re.compile(r'/(search|result|filter|sort)/', re.I), -1.5),
            (re.compile(r'/api/|/ajax/|/json/', re.I), -3.0),
        ]
        
        # é«˜è´¨é‡é“¾æ¥æ–‡æœ¬å…³é”®è¯
        self.high_quality_link_texts = re.compile(
            r'(learn|read|more|details|information|about|study|research|course|program|article|news)',
            re.I
        )
        
        # URLç¼“å­˜ï¼ˆç”¨äºé¿å…é‡å¤çˆ¬å–ï¼‰
        self.url_cache = {}  # {url: result}
        # é”æœºåˆ¶è¯´æ˜ï¼š
        # - cache_lock (asyncio.Lock): ç”¨äºå¼‚æ­¥æ–¹æ³•ï¼Œä¿æŠ¤å¼‚æ­¥ä»£ç ä¹‹é—´çš„å¹¶å‘è®¿é—®
        # - cache_lock_sync (threading.Lock): ç”¨äºåŒæ­¥æ–¹æ³•ï¼Œä¿æŠ¤è·¨çº¿ç¨‹è®¿é—®
        # æ³¨æ„ï¼šè™½ç„¶ä¸¤ä¸ªé”ä¸èƒ½äº’ç›¸ä¿æŠ¤ï¼Œä½†åœ¨å®é™…ä½¿ç”¨ä¸­ï¼š
        #   - å¼‚æ­¥ä»£ç ä¸»è¦åœ¨å•çº¿ç¨‹äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œï¼Œä½¿ç”¨ asyncio.Lock ä¿æŠ¤å¼‚æ­¥å¹¶å‘
        #   - åŒæ­¥æ–¹æ³•å¯èƒ½åœ¨å¦ä¸€ä¸ªçº¿ç¨‹ä¸­ï¼Œä½¿ç”¨ threading.Lock ä¿æŠ¤è·¨çº¿ç¨‹è®¿é—®
        self.cache_lock = asyncio.Lock()  # å¼‚æ­¥é”ï¼Œç”¨äºå¼‚æ­¥æ–¹æ³•
        self.cache_lock_sync = threading.Lock()  # åŒæ­¥é”ï¼Œç”¨äºåŒæ­¥æ–¹æ³•ï¼ˆä¿®å¤ç«æ€æ¡ä»¶ï¼‰
        
        # çˆ¬å–ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'failed_requests': 0
        }

    def _get_user_agent(self):
        """è·å–User-Agent"""
        if self.ua:
            try:
                return self.ua.random
            except:
                pass
        return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    
    def _normalize_url(self, url):
        """è§„èŒƒåŒ–URLï¼šç§»é™¤fragmentï¼Œå¤„ç†æœ«å°¾æ–œæ ç­‰"""
        if not url:
            return None
        
        # ç§»é™¤fragment
        url = url.split('#')[0]
        
        # è§£æå¹¶é‡å»ºURLä»¥è§„èŒƒåŒ–
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return None
        
        # è§„èŒƒåŒ–è·¯å¾„ï¼ˆç§»é™¤./å’Œ../ï¼‰
        path = parsed.path
        if path:
            # ç®€å•çš„è·¯å¾„è§„èŒƒåŒ–
            parts = path.split('/')
            normalized_parts = []
            for part in parts:
                if part == '..':
                    if normalized_parts:
                        normalized_parts.pop()
                elif part and part != '.':
                    normalized_parts.append(part)
            path = '/' + '/'.join(normalized_parts)
        
        # é‡å»ºURL
        normalized = f"{parsed.scheme}://{parsed.netloc}{path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        
        return normalized
    
    def _is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or len(url) > 2048:  # URLé•¿åº¦é™åˆ¶
            return False
        
        parsed = urlparse(url)
        # åªæ¥å—httpå’Œhttps
        if parsed.scheme not in ['http', 'https']:
            return False
        
        # è¿‡æ»¤æ— æ•ˆåè®®
        if url.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
            return False
        
        return True
    
    def _score_link_quality(self, url, link_text=None, link_context=None):
        """
        é“¾æ¥è´¨é‡è¯„åˆ†ç³»ç»Ÿ - æ ¹æ®URLæ¨¡å¼ã€é“¾æ¥æ–‡æœ¬å’Œä¸Šä¸‹æ–‡è¯„åˆ†
        è¿”å›è¯„åˆ†ï¼ˆ0-10ï¼‰ï¼Œåˆ†æ•°è¶Šé«˜è¡¨ç¤ºé“¾æ¥è´¨é‡è¶Šå¥½ï¼Œåº”è¯¥ä¼˜å…ˆçˆ¬å–
        
        Args:
            url: é“¾æ¥URL
            link_text: é“¾æ¥æ–‡æœ¬å†…å®¹ï¼ˆå¯é€‰ï¼‰
            link_context: é“¾æ¥ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚æ‰€å±æ ‡ç­¾ï¼ˆå¯é€‰ï¼Œå¦‚ 'nav', 'content', 'footer'ï¼‰
        
        Returns:
            è¯„åˆ†ï¼ˆfloatï¼‰ï¼ŒèŒƒå›´é€šå¸¸åœ¨ -5 åˆ° 10
        """
        score = 5.0  # åŸºç¡€åˆ†
        
        # URLæ¨¡å¼è¯„åˆ†
        for pattern, points in self.high_quality_patterns:
            if pattern.search(url):
                score += points
        
        for pattern, points in self.low_quality_patterns:
            if pattern.search(url):
                score += points  # points æ˜¯è´Ÿæ•°
        
        # é“¾æ¥æ–‡æœ¬è¯„åˆ†
        if link_text:
            text_lower = link_text.lower().strip()
            if len(text_lower) > 3:  # æœ‰æ„ä¹‰çš„é“¾æ¥æ–‡æœ¬
                if self.high_quality_link_texts.search(text_lower):
                    score += 1.0
                # é“¾æ¥æ–‡æœ¬å¤ªçŸ­æˆ–å¤ªé€šç”¨ï¼Œé™ä½åˆ†æ•°
                if len(text_lower) < 5:
                    score -= 0.5
                if text_lower in ['more', 'click', 'here', 'link', 'read more']:
                    score -= 1.0
        
        # ä¸Šä¸‹æ–‡ä½ç½®è¯„åˆ†
        if link_context:
            context_lower = link_context.lower()
            if 'content' in context_lower or 'main' in context_lower or 'article' in context_lower:
                score += 1.5
            elif 'nav' in context_lower:
                score += 0.5  # å¯¼èˆªé“¾æ¥æœ‰ä¸€å®šä»·å€¼
            elif 'footer' in context_lower or 'sidebar' in context_lower:
                score -= 0.5
        
        # è·¯å¾„æ·±åº¦è°ƒæ•´ï¼šé€‚åº¦æ·±åº¦ï¼ˆ2-6å±‚ï¼‰é€šå¸¸è´¨é‡æ›´é«˜ï¼Œä½†å…è®¸æ›´æ·±çš„è·¯å¾„
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        depth = len(path_parts)
        if 2 <= depth <= 6:
            score += 0.5  # é€‚åº¦æ·±åº¦åŠ åˆ†
        elif 7 <= depth <= 10:
            score += 0.0  # è¾ƒæ·±è·¯å¾„ä¸æ‰£åˆ†ï¼Œå…è®¸æ¢ç´¢
        elif depth > 10:
            score -= 0.5  # éå¸¸æ·±çš„è·¯å¾„è½»å¾®æ‰£åˆ†ï¼Œä½†ä»å…è®¸
        
        return max(0.0, min(10.0, score))  # é™åˆ¶åœ¨ 0-10 èŒƒå›´
    
    def _is_valid_link_for_crawl(self, url, start_domain=None):
        """
        æ·±åº¦çˆ¬å–æ—¶çš„é“¾æ¥è¿‡æ»¤ - æ›´ä¸¥æ ¼çš„éªŒè¯ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒæ™ºèƒ½è·¯å¾„æ·±åº¦åˆ¤æ–­ï¼‰
        æ£€æŸ¥é™æ€èµ„æºã€è·¯å¾„æ·±åº¦ã€åŸŸåç­‰
        """
        if not self._is_valid_url(url):
            return False
        
        parsed = urlparse(url)
        
        # åŸŸåè¿‡æ»¤
        if self.same_domain_only and start_domain:
            if parsed.netloc != start_domain:
                return False
        
        # æ™ºèƒ½è·¯å¾„æ·±åº¦é™åˆ¶
        path_parts = [p for p in parsed.path.split('/') if p]
        depth = len(path_parts)
        
        if self.max_path_depth is not None:
            # ç¡¬æ€§é™åˆ¶
            if depth > self.max_path_depth:
                return False
        else:
            # æ™ºèƒ½åˆ¤æ–­ï¼šåŸºäºURLè¯­ä¹‰è€Œéç®€å•å±‚çº§
            # å¯¹äºåŒ…å«é«˜è´¨é‡å…³é”®è¯çš„URLï¼Œå…è®¸æ›´æ·±è·¯å¾„
            is_high_quality = any(
                pattern.search(url) for pattern, _ in self.high_quality_patterns
            )
            # å¤§å¹…æ”¾å®½è·¯å¾„æ·±åº¦é™åˆ¶ï¼Œæ”¯æŒæ›´æ·±çš„çˆ¬å–
            max_allowed_depth = 12 if is_high_quality else 10
            if depth > max_allowed_depth:
                return False
        
        # é™æ€èµ„æºè¿‡æ»¤
        if self.exclude_static:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            path_lower = parsed.path.lower()
            for ext in self.exclude_extensions:
                if path_lower.endswith(ext):
                    return False
            
            # æ£€æŸ¥å¸¸è§çš„é™æ€èµ„æºè·¯å¾„æ¨¡å¼
            static_patterns = ['/static/', '/assets/', '/media/', '/files/', 
                             '/downloads/', '/images/', '/img/', '/css/', '/js/']
            if any(pattern in path_lower for pattern in static_patterns):
                return False
        
        return True
    
    async def _get_from_cache(self, url):
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if not self.enable_cache:
            return None
        
        async with self.cache_lock:
            if url in self.url_cache:
                self.stats['cache_hits'] += 1
                return self.url_cache[url]
        
        self.stats['cache_misses'] += 1
        return None
    
    async def _add_to_cache(self, url, result):
        """æ·»åŠ åˆ°ç¼“å­˜"""
        if not self.enable_cache or result is None:
            return
        
        async with self.cache_lock:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆç®€å•çš„FIFOç­–ç•¥ï¼‰
            if len(self.url_cache) >= self.max_cache_size:
                # åˆ é™¤ç¬¬ä¸€ä¸ªï¼ˆæœ€æ—§çš„ï¼‰æ¡ç›®
                if self.url_cache:
                    oldest_url = next(iter(self.url_cache))
                    del self.url_cache[oldest_url]
            
            self.url_cache[url] = result
    
    async def _get_headers(self, url=None):
        """è·å–å®Œæ•´çš„HTTP Headersï¼Œæ›´åƒçœŸå®æµè§ˆå™¨"""
        headers = {
            'User-Agent': self._get_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,de;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        
        # å¦‚æœæœ‰Refererï¼Œæ·»åŠ Refererï¼ˆæ¨¡æ‹Ÿä»å…¶ä»–é¡µé¢è·³è½¬ï¼‰
        async with self._last_url_lock:
            if url and hasattr(self, '_last_url') and self._last_url:
                parsed_current = urlparse(url)
                parsed_last = urlparse(self._last_url)
                if parsed_current.netloc == parsed_last.netloc:
                    headers['Referer'] = self._last_url
        
        return headers
    
    async def _rate_limit(self):
        """å…¨å±€é€Ÿç‡é™åˆ¶ï¼ˆä»¤ç‰Œæ¡¶ç®—æ³•ï¼‰- çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        if not self.max_rate:
            return
        
        async with self._rate_limit_lock:
            now = time.time()
            rate_limiter = self.rate_limiter
            
            # æ›´æ–°ä»¤ç‰Œ
            elapsed = now - rate_limiter['last_update']
            rate_limiter['tokens'] = min(
                rate_limiter['max_tokens'],
                rate_limiter['tokens'] + elapsed * self.max_rate
            )
            rate_limiter['last_update'] = now
            
            # å¦‚æœæ²¡æœ‰ä»¤ç‰Œï¼Œç­‰å¾…
            if rate_limiter['tokens'] < 1:
                wait_time = (1 - rate_limiter['tokens']) / self.max_rate
                # é‡Šæ”¾é”åç­‰å¾…ï¼Œé¿å…é˜»å¡å…¶ä»–è¯·æ±‚
                await asyncio.sleep(wait_time)
                # é‡æ–°è·å–é”å¹¶æ›´æ–°
                async with self._rate_limit_lock:
                    rate_limiter['tokens'] = 0
            
            # æ¶ˆè€—ä¸€ä¸ªä»¤ç‰Œ
            rate_limiter['tokens'] -= 1
    
    async def _domain_delay(self, url):
        """æŒ‰åŸŸåå»¶è¿Ÿï¼Œé˜²æ­¢å¯¹åŒä¸€åŸŸåè¯·æ±‚è¿‡äºé¢‘ç¹ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        if self.delay <= 0:
            return
        
        parsed = urlparse(url)
        domain = parsed.netloc
        
        async with self._domain_delay_lock:
            now = time.time()
            
            if domain in self.last_request_time:
                elapsed = now - self.last_request_time[domain]
                if elapsed < self.delay:
                    wait_time = self.delay - elapsed
                    # é‡Šæ”¾é”åç­‰å¾…
                    await asyncio.sleep(wait_time)
                    # é‡æ–°è·å–é”å¹¶æ›´æ–°
                    async with self._domain_delay_lock:
                        self.last_request_time[domain] = time.time()
                else:
                    self.last_request_time[domain] = now
            else:
                self.last_request_time[domain] = now

    async def fetch(self, session, url, redirect_count=0, redirect_history=None):
        """
        å¼‚æ­¥è·å–é¡µé¢å†…å®¹ï¼Œå¸¦è‡ªåŠ¨é‡è¯•å’Œåçˆ¬è™«æªæ–½
        
        Args:
            session: aiohttpä¼šè¯
            url: ç›®æ ‡URL
            redirect_count: å½“å‰é‡å®šå‘æ·±åº¦
            redirect_history: é‡å®šå‘å†å²ï¼ˆç”¨äºæ£€æµ‹å¾ªç¯ï¼‰
        """
        # è¾“å…¥éªŒè¯
        if not url or not self._is_valid_url(url):
            logger.warning(f"Invalid URL: {url}")
            return None
        
        # è§„èŒƒåŒ–URL
        url = self._normalize_url(url)
        if not url:
            logger.warning(f"Failed to normalize URL: {url}")
            return None
        
        # æ£€æŸ¥é‡å®šå‘æ·±åº¦
        if redirect_count >= self.max_redirects:
            logger.warning(f"Max redirects ({self.max_redirects}) reached for {url}")
            return None
        
        # åˆå§‹åŒ–é‡å®šå‘å†å²
        if redirect_history is None:
            redirect_history = set()
        
        # æ£€æŸ¥é‡å®šå‘å¾ªç¯
        if url in redirect_history:
            logger.warning(f"Redirect loop detected: {url} -> {redirect_history}")
            return None
        
        # åçˆ¬è™«ï¼šé€Ÿç‡é™åˆ¶å’ŒåŸŸåå»¶è¿Ÿ
        await self._rate_limit()
        await self._domain_delay(url)
        
        retries = 3
        for i in range(retries):
            try:
                headers = await self._get_headers(url)
                async with self.semaphore:
                    async with session.get(
                        url, 
                        headers=headers, 
                        timeout=self.timeout, 
                        ssl=self.verify_ssl, 
                        allow_redirects=False
                    ) as response:
                        if response.status == 200:
                            # è®°å½•æœ€åè®¿é—®çš„URLï¼ˆç”¨äºRefererï¼‰
                            async with self._last_url_lock:
                                self._last_url = url
                            # aiohttpä¼šè‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œä½†æ·»åŠ é”™è¯¯å¤„ç†
                            try:
                                return await response.text()
                            except UnicodeDecodeError:
                                # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨è§£ç 
                                content = await response.read()
                                # å°è¯•å¸¸è§ç¼–ç 
                                encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
                                for encoding in encodings:
                                    try:
                                        return content.decode(encoding)
                                    except (UnicodeDecodeError, LookupError):
                                        continue
                                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶æ›¿æ¢é”™è¯¯å­—ç¬¦
                                return content.decode('utf-8', errors='replace')
                        elif response.status in [301, 302, 303, 307, 308]:
                            # å¤„ç†é‡å®šå‘
                            redirect_url = response.headers.get('Location')
                            if redirect_url:
                                logger.info(f"Redirecting {url} -> {redirect_url} (depth: {redirect_count + 1})")
                                # å¤„ç†ç›¸å¯¹å’Œç»å¯¹URL
                                absolute_redirect = urljoin(url, redirect_url)
                                # è§„èŒƒåŒ–é‡å®šå‘URL
                                absolute_redirect = self._normalize_url(absolute_redirect)
                                
                                if absolute_redirect and self._is_valid_url(absolute_redirect):
                                    # æ›´æ–°é‡å®šå‘å†å²
                                    new_history = redirect_history | {url}
                                    # é€’å½’å¤„ç†é‡å®šå‘
                                    return await self.fetch(
                                        session, 
                                        absolute_redirect, 
                                        redirect_count + 1, 
                                        new_history
                                    )
                                else:
                                    logger.warning(f"Invalid redirect URL: {redirect_url}")
                        else:
                            logger.warning(f"Status {response.status} for {url}")
            except asyncio.TimeoutError:
                logger.debug(f"Timeout {i+1}/{retries} for {url}")
            except aiohttp.ClientError as e:
                logger.debug(f"Client error {i+1}/{retries} for {url}: {e}")
            except Exception as e:
                logger.debug(f"Retry {i+1}/{retries} for {url}: {e}")
            
            if i < retries - 1:
                await asyncio.sleep(2 ** i)  # æŒ‡æ•°é€€é¿ç­–ç•¥
        
        return None

    def fast_entropy(self, text):
        """ä¼˜åŒ–çš„é¦™å†œç†µè®¡ç®— (ä½¿ç”¨äº† Counter)"""
        if not text or len(text) < 2:
            return 0
        length = len(text)
        counts = Counter(text)
        probs = (count / length for count in counts.values())
        return -sum(p * math.log2(p) for p in probs if p > 0)

    def clean_dom(self, soup):
        """æ¸…æ´— DOM æ ‘ï¼Œç§»é™¤å™ªå£°èŠ‚ç‚¹"""
        # 1. ç§»é™¤æ— ç”¨æ ‡ç­¾
        for tag in soup(self.clean_tags):
            tag.decompose()
        
        # 2. ç§»é™¤æ³¨é‡Š
        for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):
            comment.extract()

        # 3. åŸºäº Class/ID çš„å¯å‘å¼ç§»é™¤
        for tag in list(soup.find_all(True)):
            attr_str = str(tag.get('class', '')) + " " + str(tag.get('id', ''))
            if self.noise_pattern.search(attr_str):
                tag.decompose()

    def extract_content_smart(self, soup, url):
        """
        æ™ºèƒ½å†…å®¹æå–ï¼šä¿ç•™æ®µè½ç»“æ„ï¼Œè€Œä¸æ˜¯æ‰“æ•£çš„å¥å­
        è¿”å›æ ¼å¼ä¸ SmartCrawler.parse() å…¼å®¹
        """
        self.clean_dom(soup)

        # 1. æå–å›¾ç‰‡
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if not src or src.startswith('data:'):
                continue
            
            full_url = urljoin(url, src)
            # æ”¹è¿›çš„æ‰©å±•åæå–ï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œfragment
            ext = full_url.split('.')[-1].lower().split('?')[0].split('#')[0]
            if ext in ['jpg', 'jpeg', 'png', 'webp', 'gif', 'svg']:
                images.append(full_url)

        # 2. æå–é“¾æ¥ï¼ˆå¢å¼ºç‰ˆï¼šä»æ›´å¤šä½ç½®æå–ï¼Œé™„å¸¦å…ƒæ•°æ®ç”¨äºä¼˜å…ˆçº§è¯„åˆ†ï¼‰
        links = []  # æ”¹ä¸ºåˆ—è¡¨ï¼Œå­˜å‚¨ (url, metadata) å…ƒç»„
        links_set = set()  # ç”¨äºå»é‡
        
        # ä»ä¸åŒåŒºåŸŸæå–é“¾æ¥ï¼Œå¹¶æ ‡è®°ä¸Šä¸‹æ–‡
        # ä¿®å¤ï¼šè¯­ä¹‰æ ‡ç­¾ï¼ˆarticle, main, sectionç­‰ï¼‰åº”è¯¥æ— æ¡ä»¶æŸ¥æ‰¾ï¼Œdivæ ‡ç­¾å¯ä»¥è¦æ±‚åŒ¹é…class
        def find_content_containers():
            """æŸ¥æ‰¾å†…å®¹å®¹å™¨ï¼šè¯­ä¹‰æ ‡ç­¾æ— æ¡ä»¶æŸ¥æ‰¾ï¼Œdivæ ‡ç­¾è¦æ±‚åŒ¹é…class"""
            containers = []
            # è¯­ä¹‰æ ‡ç­¾ï¼šæ— æ¡ä»¶æŸ¥æ‰¾ï¼ˆè¿™äº›æ ‡ç­¾æœ¬èº«å°±è¡¨ç¤ºå†…å®¹åŒºåŸŸï¼‰
            semantic_tags = soup.find_all(['article', 'main', 'section'])
            containers.extend(semantic_tags)
            # divæ ‡ç­¾ï¼šè¦æ±‚åŒ¹é…class
            div_with_class = soup.find_all('div', class_=re.compile(r'content|main|article|body', re.I))
            containers.extend(div_with_class)
            return containers
        
        def find_nav_containers():
            """æŸ¥æ‰¾å¯¼èˆªå®¹å™¨ï¼šnavå’Œheaderæ ‡ç­¾æ— æ¡ä»¶æŸ¥æ‰¾ï¼Œdivæ ‡ç­¾è¦æ±‚åŒ¹é…class"""
            containers = []
            # è¯­ä¹‰æ ‡ç­¾ï¼šæ— æ¡ä»¶æŸ¥æ‰¾
            semantic_tags = soup.find_all(['nav', 'header'])
            containers.extend(semantic_tags)
            # divæ ‡ç­¾ï¼šè¦æ±‚åŒ¹é…class
            div_with_class = soup.find_all('div', class_=re.compile(r'nav|menu|header', re.I))
            containers.extend(div_with_class)
            return containers
        
        def find_sidebar_containers():
            """æŸ¥æ‰¾ä¾§è¾¹æ å®¹å™¨ï¼šasideæ ‡ç­¾æ— æ¡ä»¶æŸ¥æ‰¾ï¼Œdivæ ‡ç­¾è¦æ±‚åŒ¹é…class"""
            containers = []
            # è¯­ä¹‰æ ‡ç­¾ï¼šæ— æ¡ä»¶æŸ¥æ‰¾
            semantic_tags = soup.find_all('aside')
            containers.extend(semantic_tags)
            # divæ ‡ç­¾ï¼šè¦æ±‚åŒ¹é…class
            div_with_class = soup.find_all('div', class_=re.compile(r'sidebar|aside', re.I))
            containers.extend(div_with_class)
            return containers
        
        def find_footer_containers():
            """æŸ¥æ‰¾é¡µè„šå®¹å™¨ï¼šfooteræ ‡ç­¾æ— æ¡ä»¶æŸ¥æ‰¾"""
            # footeræ˜¯è¯­ä¹‰æ ‡ç­¾ï¼Œæ— æ¡ä»¶æŸ¥æ‰¾
            return soup.find_all('footer')
        
        link_sources = [
            ('content', find_content_containers()),
            ('nav', find_nav_containers()),
            ('sidebar', find_sidebar_containers()),
            ('footer', find_footer_containers()),
        ]
        
        # ä»å„åŒºåŸŸæå–é“¾æ¥
        for context, containers in link_sources:
            for container in containers:
                for a in container.find_all('a', href=True):
                    href = a['href']
                    
                    # è¿‡æ»¤æ— æ•ˆåè®®
                    if href.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
                        continue
                    
                    full_link = urljoin(url, href)
                    
                    # è§„èŒƒåŒ–URL
                    normalized = self._normalize_url(full_link)
                    if normalized and self._is_valid_url(normalized):
                        if normalized not in links_set:
                            links_set.add(normalized)
                            # æå–é“¾æ¥æ–‡æœ¬
                            link_text = a.get_text(strip=True)
                            # å­˜å‚¨é“¾æ¥åŠå…¶å…ƒæ•°æ®
                            links.append((normalized, {'text': link_text, 'context': context}))
        
        # å¦‚æœä»ä¸Šè¿°åŒºåŸŸæ²¡æ‰¾åˆ°è¶³å¤Ÿé“¾æ¥ï¼Œä»æ•´ä¸ªé¡µé¢æå–ï¼ˆå‘åå…¼å®¹ï¼‰
        if len(links) < 10:
            for a in soup.find_all('a', href=True):
                href = a['href']
                
                if href.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
                    continue
                
                full_link = urljoin(url, href)
                normalized = self._normalize_url(full_link)
                if normalized and self._is_valid_url(normalized):
                    if normalized not in links_set:
                        links_set.add(normalized)
                        link_text = a.get_text(strip=True)
                        # å°è¯•æ¨æ–­ä¸Šä¸‹æ–‡
                        parent_tag = a.find_parent()
                        context = 'general'
                        if parent_tag:
                            parent_class = str(parent_tag.get('class', ''))
                            if any(x in parent_class.lower() for x in ['nav', 'menu']):
                                context = 'nav'
                            elif any(x in parent_class.lower() for x in ['content', 'main', 'article']):
                                context = 'content'
                            elif 'footer' in parent_tag.name.lower() or 'footer' in parent_class.lower():
                                context = 'footer'
                        links.append((normalized, {'text': link_text, 'context': context}))
        
        # å­˜å‚¨é“¾æ¥å…ƒæ•°æ®ç”¨äºä¼˜å…ˆçº§æ’åºï¼ˆå­˜å‚¨åœ¨ç»“æœä¸­ï¼‰
        links_metadata = {url: metadata for url, metadata in links}
        links_urls = [url for url, _ in links]
        
        # 3. æå–æ–‡æœ¬ (æ ¸å¿ƒä¼˜åŒ–ï¼šåŸºäºå—çš„æå–ï¼Œæ”¯æŒæ›´å¤šå†…å®¹ç±»å‹)
        text_blocks = []
        
        # æå–æ ‡é¢˜ï¼ˆä¿ç•™å±‚æ¬¡ç»“æ„ä¿¡æ¯ï¼‰
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = tag.get_text(strip=True, separator=' ')
            if text and len(text) >= 10:  # æ ‡é¢˜å¯ä»¥çŸ­ä¸€äº›
                text_blocks.append(text)
        
        # æå–æ®µè½å’Œä¸»è¦å†…å®¹æ ‡ç­¾
        for tag in soup.find_all(['p', 'article', 'main', 'section', 'div']):
            text = tag.get_text(strip=True, separator=' ')
            
            # è¿‡æ»¤UIçŸ­è¯­
            if self.ui_phrases.search(text):
                continue
            
            # é•¿åº¦æ£€æŸ¥
            if len(text) < self.MIN_LENGTH:
                continue
            
            # ç†µå€¼æ£€æŸ¥
            entropy = self.fast_entropy(text)
            if self.MIN_ENTROPY <= entropy <= self.MAX_ENTROPY:
                text_blocks.append(text)
        
        # æå–åˆ—è¡¨é¡¹ï¼ˆliæ ‡ç­¾ï¼‰- é€šå¸¸åŒ…å«æœ‰ç”¨ä¿¡æ¯
        for tag in soup.find_all(['li']):
            text = tag.get_text(strip=True, separator=' ')
            # åˆ—è¡¨é¡¹å¯ä»¥ç¨çŸ­
            if len(text) >= 20 and len(text) < 500:  # é¿å…è¿‡é•¿çš„åˆ—è¡¨é¡¹
                if self.ui_phrases.search(text):
                    continue
                entropy = self.fast_entropy(text)
                if self.MIN_ENTROPY <= entropy <= self.MAX_ENTROPY:
                    text_blocks.append(text)
        
        # æå–è¡¨æ ¼å†…å®¹ï¼ˆtdæ ‡ç­¾ï¼‰- æŸäº›è¡¨æ ¼å¯èƒ½åŒ…å«é‡è¦æ•°æ®
        for tag in soup.find_all(['td', 'th']):
            text = tag.get_text(strip=True, separator=' ')
            if len(text) >= 15 and len(text) < 300:
                if self.ui_phrases.search(text):
                    continue
                entropy = self.fast_entropy(text)
                if self.MIN_ENTROPY <= entropy <= self.MAX_ENTROPY:
                    text_blocks.append(text)
        
        # æå–ä»£ç å—ä¸­çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆcode, preæ ‡ç­¾ï¼‰
        for tag in soup.find_all(['code', 'pre']):
            text = tag.get_text(strip=True)
            # ä»£ç å—é€šå¸¸è¾ƒé•¿ï¼Œä½†æˆ‘ä»¬åªæå–ç›¸å¯¹çŸ­çš„ä»£ç ç‰‡æ®µæˆ–æ³¨é‡Š
            if len(text) >= 30 and len(text) < 200:
                # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯æ³¨é‡Šæˆ–æ–‡æ¡£
                if '//' in text or '/*' in text or '#' in text or '"""' in text:
                    text_blocks.append(text)
        
        # æå–å—å¼•ç”¨ï¼ˆblockquoteï¼‰- é€šå¸¸åŒ…å«é‡è¦å¼•ç”¨
        for tag in soup.find_all(['blockquote']):
            text = tag.get_text(strip=True, separator=' ')
            if len(text) >= self.MIN_LENGTH:
                entropy = self.fast_entropy(text)
                if self.MIN_ENTROPY <= entropy <= self.MAX_ENTROPY:
                    text_blocks.append(text)

        # å»é‡ä½†ä¿ç•™é¡ºåº
        text_blocks = list(dict.fromkeys(text_blocks))

        # è¿”å›æ ¼å¼ä¸ SmartCrawler å…¼å®¹ï¼Œä½†å¢åŠ é“¾æ¥å…ƒæ•°æ®
        result = {
            "url": url,
            "title": soup.title.string.strip() if soup.title and soup.title.string else "",
            "texts": text_blocks,  # å…³é”®ï¼šä½¿ç”¨ texts è€Œä¸æ˜¯ content_blocks
            "images": images[:5],
            "links": links_urls,  # URLåˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰
            "_links_metadata": links_metadata  # é“¾æ¥å…ƒæ•°æ®ï¼ˆç”¨äºä¼˜å…ˆçº§æ’åºï¼‰
        }
        return result

    async def process_url(self, session, url):
        """å•ä¸ª URL çš„å¤„ç†æµ - æ”¯æŒç¼“å­˜"""
        # è§„èŒƒåŒ–URL
        normalized_url = self._normalize_url(url)
        if not normalized_url:
            return None
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = await self._get_from_cache(normalized_url)
        if cached_result is not None:
            logger.debug(f"Cache hit for {normalized_url}")
            return cached_result
        
        # ç»Ÿè®¡
        self.stats['total_requests'] += 1
        
        html = await self.fetch(session, normalized_url)
        if not html:
            self.stats['failed_requests'] += 1
            return None

        # å°† CPU å¯†é›†å‹çš„è§£æä»»åŠ¡æ”¾åˆ°çº¿ç¨‹æ± ä¸­ï¼Œé¿å…é˜»å¡ Event Loop
        loop = asyncio.get_running_loop()
        try:
            result = await loop.run_in_executor(self.executor, self._parse_sync, html, normalized_url)
            # æ·»åŠ åˆ°ç¼“å­˜
            await self._add_to_cache(normalized_url, result)
            return result
        except Exception as e:
            logger.error(f"Parse error {normalized_url}: {e}")
            self.stats['failed_requests'] += 1
            return None

    def _parse_sync(self, html, url):
        """åŒæ­¥è§£æé€»è¾‘ (è¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­) - å¸¦è§£æå™¨å›é€€"""
        try:
            # å°è¯•ä½¿ç”¨lxmlï¼ˆæ›´å¿«ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°html.parser
            try:
                soup = BeautifulSoup(html, 'lxml')
            except Exception:
                logger.debug(f"lxml parser failed for {url}, falling back to html.parser")
                soup = BeautifulSoup(html, 'html.parser')
            
            return self.extract_content_smart(soup, url)
        except Exception as e:
            logger.error(f"BeautifulSoup parse error for {url}: {e}")
            return None

    async def run(self, urls: List[str]) -> List[Dict]:
        """
        ä¸»å…¥å£ - å¼‚æ­¥æ‰¹é‡å¤„ç†URLåˆ—è¡¨
        Args:
            urls: URLåˆ—è¡¨
        Returns:
            æˆåŠŸçˆ¬å–çš„ç»“æœåˆ—è¡¨
        """
        async with aiohttp.ClientSession() as session:
            tasks = [self.process_url(session, url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è¿‡æ»¤æ‰å¼‚å¸¸å’ŒNone
            valid_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Error processing {urls[i]}: {result}")
                elif result is not None:
                    valid_results.append(result)
            
            return valid_results

    def _calculate_page_quality(self, result: Dict) -> float:
        """
        è®¡ç®—é¡µé¢è´¨é‡åˆ†æ•°ï¼ˆ0-10ï¼‰ï¼Œç”¨äºè‡ªé€‚åº”æ·±åº¦è°ƒæ•´
        åŸºäºæ–‡æœ¬å—æ•°é‡ã€æ–‡æœ¬æ€»é•¿åº¦ã€é“¾æ¥æ•°é‡ç­‰æŒ‡æ ‡
        """
        if not result:
            return 0.0
        
        score = 0.0
        
        # æ–‡æœ¬å—æ•°é‡å’Œé•¿åº¦
        texts = result.get('texts', [])
        if texts:
            text_count = len(texts)
            total_length = sum(len(t) for t in texts)
            
            # æ–‡æœ¬å—è¶Šå¤šè¶Šå¥½ï¼ˆä½†æœ‰é™åˆ¶ï¼‰
            score += min(text_count / 10.0, 3.0)  # æœ€å¤š3åˆ†
            
            # æ€»é•¿åº¦ï¼ˆå†…å®¹è¶Šä¸°å¯Œè¶Šå¥½ï¼‰
            score += min(total_length / 1000.0, 2.0)  # æœ€å¤š2åˆ†
        
        # é“¾æ¥æ•°é‡ï¼ˆé€‚åº¦æœ€å¥½ï¼‰
        links = result.get('links', [])
        link_count = len(links)
        if 5 <= link_count <= 50:
            score += 2.0  # é“¾æ¥æ•°é‡åˆç†
        elif link_count > 50:
            score += 1.0  # é“¾æ¥å¤ªå¤šå¯èƒ½è´¨é‡è¾ƒä½
        
        # æœ‰æ ‡é¢˜åŠ åˆ†
        title = result.get('title', '').strip()
        if title and len(title) > 10:
            score += 1.0
        
        return min(10.0, score)
    
    async def crawl_recursive(self, start_url: str, max_depth: int = 8, max_pages: Optional[int] = None,
                              callback=None, same_domain_only: Optional[bool] = None, 
                              adaptive_depth: bool = True, max_adaptive_depth: int = 2) -> List[Dict]:
        """
        æ·±åº¦é€’å½’çˆ¬å– - ä½¿ç”¨BFSç®—æ³•æŒ‰å±‚çˆ¬å–ï¼Œæ”¯æŒé“¾æ¥ä¼˜å…ˆçº§æ’åºå’Œè‡ªé€‚åº”æ·±åº¦è°ƒæ•´
        
        Args:
            start_url: èµ·å§‹URL
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦ï¼ˆé»˜è®¤8ï¼Œ0è¡¨ç¤ºåªçˆ¬å–èµ·å§‹URLï¼‰
            max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
            callback: å›è°ƒå‡½æ•° callback(count, url, result) åœ¨æ¯ä¸ªé¡µé¢çˆ¬å–å®Œæˆåè°ƒç”¨
            same_domain_only: æ˜¯å¦åªçˆ¬å–åŒä¸€åŸŸåï¼ˆNoneè¡¨ç¤ºä½¿ç”¨åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
            adaptive_depth: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”æ·±åº¦è°ƒæ•´ï¼ˆé»˜è®¤Trueï¼Œæ ¹æ®é¡µé¢è´¨é‡åŠ¨æ€è°ƒæ•´çˆ¬å–ç­–ç•¥ï¼‰
            max_adaptive_depth: è‡ªé€‚åº”æ·±åº¦è°ƒæ•´çš„æœ€å¤§é¢å¤–æ·±åº¦ï¼ˆé»˜è®¤2ï¼Œå³æœ€å¤šå¯ä»¥åˆ° max_depth + 2ï¼‰
        
        Returns:
            æ‰€æœ‰çˆ¬å–ç»“æœåˆ—è¡¨
        """
        if same_domain_only is None:
            same_domain_only = self.same_domain_only
        
        # è§„èŒƒåŒ–èµ·å§‹URL
        start_url = self._normalize_url(start_url)
        if not start_url:
            logger.error(f"Invalid start URL: {start_url}")
            return []
        
        parsed_start = urlparse(start_url)
        start_domain = parsed_start.netloc
        
        visited = set()  # å·²è®¿é—®çš„URLé›†åˆ
        queue = [(start_url, 0)]  # (url, depth) é˜Ÿåˆ—ï¼ŒBFS
        results = []
        count = 0
        
        logger.info(f"ğŸš€ Starting recursive crawl from {start_url} (max_depth={max_depth}, max_pages={max_pages or 'unlimited'})")
        
        async with aiohttp.ClientSession() as session:
            while queue:
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é¡µé¢æ•°é™åˆ¶
                if max_pages and count >= max_pages:
                    logger.info(f"Reached max_pages limit: {max_pages}")
                    break
                
                # è·å–å½“å‰å±‚çš„æ‰€æœ‰URLï¼ˆåŒä¸€æ·±åº¦çš„URLï¼‰
                current_level = []
                current_depth = queue[0][1] if queue else -1
                
                # æ”¶é›†åŒä¸€æ·±åº¦çš„æ‰€æœ‰URL
                while queue and queue[0][1] == current_depth:
                    url, depth = queue.pop(0)
                    if url not in visited:
                        visited.add(url)
                        current_level.append((url, depth))
                
                if not current_level:
                    break
                
                # å¹¶å‘çˆ¬å–å½“å‰å±‚çš„æ‰€æœ‰URL
                tasks = [self.process_url(session, url) for url, _ in current_level]
                level_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # å¤„ç†å½“å‰å±‚çš„ç»“æœ
                for i, (url, depth) in enumerate(current_level):
                    result = level_results[i]
                    
                    if isinstance(result, Exception):
                        logger.error(f"Error processing {url}: {result}")
                        continue
                    
                    if result is None:
                        continue
                    
                    results.append(result)
                    count += 1
                    
                    # è°ƒç”¨å›è°ƒå‡½æ•°
                    if callback:
                        try:
                            callback(count, url, result)
                        except Exception as e:
                            logger.warning(f"Callback error for {url}: {e}")
                    
                    # è®¡ç®—é¡µé¢è´¨é‡
                    page_quality = self._calculate_page_quality(result) if adaptive_depth else 5.0
                    logger.info(f"[{count}] Depth {depth}: {url} - Found {len(result.get('texts', []))} text blocks, {len(result.get('links', []))} links (quality: {page_quality:.1f})")
                    
                    # è‡ªé€‚åº”æ·±åº¦è°ƒæ•´ï¼šå¦‚æœé¡µé¢è´¨é‡é«˜ï¼Œå…è®¸æ›´æ·±çˆ¬å–
                    effective_max_depth = max_depth
                    if adaptive_depth:
                        if page_quality >= 8.0:
                            # éå¸¸é«˜è´¨é‡é¡µé¢ï¼Œå…è®¸æœ€å¤§é¢å¤–æ·±åº¦
                            effective_max_depth = max_depth + max_adaptive_depth
                            logger.debug(f"Very high quality page detected (quality: {page_quality:.1f}), allowing depth up to {effective_max_depth}")
                        elif page_quality >= 6.0:
                            # é«˜è´¨é‡é¡µé¢ï¼Œå…è®¸ä¸­ç­‰é¢å¤–æ·±åº¦
                            effective_max_depth = max_depth + max(1, max_adaptive_depth - 1)
                            logger.debug(f"High quality page detected (quality: {page_quality:.1f}), allowing depth up to {effective_max_depth}")
                        elif page_quality < 2.5:
                            # ä½è´¨é‡é¡µé¢ï¼Œæå‰ç»ˆæ­¢
                            effective_max_depth = depth
                            logger.debug(f"Low quality page detected (quality: {page_quality:.1f}), stopping deep crawl at depth {depth}")
                    
                    # å¦‚æœè¿˜æœ‰æ·±åº¦ï¼Œæ”¶é›†ä¸‹ä¸€å±‚çš„é“¾æ¥
                    if depth < effective_max_depth:
                        links = result.get('links', [])
                        links_metadata = result.get('_links_metadata', {})
                        
                        # æ”¶é›†å¹¶è¯„åˆ†é“¾æ¥
                        candidate_links = []
                        for link in links:
                            # è§„èŒƒåŒ–é“¾æ¥
                            normalized_link = self._normalize_url(link)
                            if not normalized_link:
                                continue
                            
                            # ä½¿ç”¨å¢å¼ºçš„é“¾æ¥è¿‡æ»¤
                            if not self._is_valid_link_for_crawl(normalized_link, start_domain if same_domain_only else None):
                                continue
                            
                            if normalized_link in visited:
                                continue
                            
                            # é¿å…é‡å¤æ·»åŠ åˆ°é˜Ÿåˆ—
                            if any(nl == normalized_link for nl, _ in queue):
                                continue
                            
                            # è·å–é“¾æ¥å…ƒæ•°æ®å¹¶è¯„åˆ†
                            metadata = links_metadata.get(normalized_link, {})
                            link_text = metadata.get('text', '')
                            link_context = metadata.get('context', 'general')
                            
                            # é“¾æ¥è´¨é‡è¯„åˆ†
                            if self.enable_link_prioritization:
                                score = self._score_link_quality(normalized_link, link_text, link_context)
                            else:
                                score = 5.0  # é»˜è®¤åˆ†
                            
                            candidate_links.append((normalized_link, depth + 1, score))
                        
                        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆåˆ†æ•°é«˜çš„ä¼˜å…ˆï¼‰
                        if self.enable_link_prioritization:
                            candidate_links.sort(key=lambda x: x[2], reverse=True)
                            logger.debug(f"Sorted {len(candidate_links)} links by priority for depth {depth + 1}")
                        
                        # æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆæŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰
                        for normalized_link, new_depth, score in candidate_links:
                            queue.append((normalized_link, new_depth))
                
                logger.info(f"Completed depth {current_depth}: processed {len(current_level)} pages, found {len(queue)} URLs for next level")
        
        logger.info(f"âœ… Recursive crawl finished. Processed {count} pages in total.")
        return results

    def parse(self, url: str) -> Optional[Dict]:
        """
        åŒæ­¥æ¥å£ - å…¼å®¹ SmartCrawler.parse()
        ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾›åŒæ­¥æ¥å£
        """
        # è¾“å…¥éªŒè¯
        if not url or not isinstance(url, str):
            logger.error(f"Invalid URL input: {url}")
            return None
        
        if not self._is_valid_url(url):
            logger.error(f"Invalid URL format: {url}")
            return None
        
        try:
            # ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨å¼‚æ­¥æ–¹æ³•
            try:
                loop = asyncio.get_running_loop()
                # å¦‚æœäº‹ä»¶å¾ªç¯å·²ç»åœ¨è¿è¡Œï¼Œä½¿ç”¨çº¿ç¨‹æ± 
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.run([url]))
                    results = future.result(timeout=60)  # æ·»åŠ è¶…æ—¶
                    return results[0] if results else None
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥ä½¿ç”¨asyncio.run
                results = asyncio.run(self.run([url]))
                return results[0] if results else None
        except Exception as e:
            logger.error(f"Error in parse({url}): {e}")
            return None

    def get_stats(self):
        """è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œä½¿ç”¨åŒæ­¥é”ä¿æŠ¤ç¼“å­˜è¯»å–ï¼‰"""
        cache_hit_rate = 0
        if self.stats['total_requests'] + self.stats['cache_hits'] > 0:
            cache_hit_rate = self.stats['cache_hits'] / (self.stats['total_requests'] + self.stats['cache_hits'])
        
        # ä½¿ç”¨åŒæ­¥é”ä¿æŠ¤ç¼“å­˜å¤§å°è¯»å–ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        with self.cache_lock_sync:
            cache_size = len(self.url_cache)
        
        return {
            **self.stats,
            'cache_hit_rate': f"{cache_hit_rate:.2%}",
            'cache_size': cache_size,
            'max_cache_size': self.max_cache_size
        }
    
    async def clear_cache(self):
        """æ¸…ç©ºURLç¼“å­˜ï¼ˆå¼‚æ­¥æ–¹æ³•ï¼‰"""
        async with self.cache_lock:
            self.url_cache.clear()
            logger.info("Cache cleared")
    
    def clear_cache_sync(self):
        """æ¸…ç©ºURLç¼“å­˜ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œç”¨äºå‘åå…¼å®¹ï¼‰"""
        # ä½¿ç”¨åŒæ­¥é”ä¿æŠ¤ï¼Œé¿å…ä¸å¼‚æ­¥æ–¹æ³•äº§ç”Ÿç«æ€æ¡ä»¶
        with self.cache_lock_sync:
            self.url_cache.clear()
            logger.info("Cache cleared")
    
    def close(self):
        """æ˜¾å¼å…³é—­èµ„æºï¼ˆæ¨èä½¿ç”¨ï¼‰"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
    
    def __del__(self):
        """æ¸…ç†èµ„æºï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
        except:
            pass  # å¿½ç•¥æ¸…ç†æ—¶çš„é”™è¯¯
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()
        return False


# ä¸ºäº†å‘åå…¼å®¹ï¼Œé»˜è®¤å¯¼å‡º SmartCrawler
# å¦‚æœé¡¹ç›®éœ€è¦é«˜æ€§èƒ½ï¼Œå¯ä»¥æ”¹ç”¨ OptimizedCrawler
if __name__ == "__main__":
    import time
    
    # æµ‹è¯• SmartCrawler (åŒæ­¥)
    print("=" * 60)
    print("Testing SmartCrawler (Synchronous)")
    print("=" * 60)
    crawler_sync = SmartCrawler()
    start = time.time()
    result = crawler_sync.parse("https://www.tum.de/en/")
    end = time.time()
    
    if result:
        print(f"\nâœ… Crawled in {end - start:.2f} seconds")
        print(f"ğŸ“„ Title: {result.get('title', 'N/A')}")
        print(f"ğŸ“ Text blocks: {len(result['texts'])}")
        print(f"ğŸ–¼ï¸ Images: {len(result['images'])}")
        print(f"ğŸ”— Links: {len(result['links'])}")
        print("\nğŸ“ Sample texts:")
        for i, text in enumerate(result['texts'][:3], 1):
            print(f"   {i}. {text[:80]}...")
    
    # æµ‹è¯• OptimizedCrawler (å¼‚æ­¥)
    print("\n" + "=" * 60)
    print("Testing OptimizedCrawler (Asynchronous)")
    print("=" * 60)
    crawler_async = OptimizedCrawler(concurrency=3)
    
    target_urls = [
        "https://www.tum.de/en/",
        "https://www.tum.de/en/studies/",
        "https://www.tum.de/en/research/"
    ]
    
    start = time.time()
    results = asyncio.run(crawler_async.run(target_urls))
    end = time.time()
    
    print(f"\nâœ… Crawled {len(results)} pages in {end - start:.2f} seconds")
    for result in results:
        if result:
            print(f"\nğŸ“„ {result['url']}")
            print(f"   Texts: {len(result['texts'])} | Images: {len(result['images'])} | Links: {len(result['links'])}")
