"""
ä¼˜åŒ–çš„çˆ¬è™«æ¨¡å— - æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§æ¨¡å¼
å…¼å®¹åŸæœ‰ SmartCrawler æ¥å£ï¼ŒåŒæ—¶æä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥æ‰¹é‡å¤„ç†èƒ½åŠ›
"""
import asyncio
import aiohttp
import math
import logging
import re
import requests
import time
import traceback
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup, Comment
from collections import Counter
from typing import List, Dict, Optional, Set
from concurrent.futures import ThreadPoolExecutor

try:
    from fake_useragent import UserAgent
    HAS_FAKE_USERAGENT = True
except ImportError:
    HAS_FAKE_USERAGENT = False
    logging.warning("fake_useragent not installed, using default User-Agent")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SmartCrawler:
    """
    åŒæ­¥çˆ¬è™«ç±» - ä¿æŒå‘åå…¼å®¹
    è¿™æ˜¯åŸæœ‰æ¥å£ï¼Œç¡®ä¿ç°æœ‰ä»£ç å¯ä»¥æ­£å¸¸å·¥ä½œ
    """
    def __init__(self):
        # ç†µå€¼é˜ˆå€¼ï¼šæ ¹æ®ç»éªŒï¼Œè‹±æ–‡/å¾·æ–‡è‡ªç„¶è¯­è¨€é€šå¸¸åœ¨ 3.5 åˆ° 5.8 ä¹‹é—´
        self.MIN_ENTROPY = 3.5
        self.MAX_ENTROPY = 6.0
        self.MIN_LENGTH = 30

    def calculate_shannon_entropy(self, text):
        """è®¡ç®—æ–‡æœ¬çš„é¦™å†œç†µ (Shannon Entropy)"""
        if not text:
            return 0
        prob = [float(text.count(c)) / len(text) for c in dict.fromkeys(list(text))]
        entropy = -sum([p * math.log(p) / math.log(2.0) for p in prob if p > 0])
        return entropy

    def is_valid_text(self, text):
        """æ ¸å¿ƒè¿‡æ»¤å™¨ï¼šåŸºäºé•¿åº¦å’Œç†µå€¼æ’é™¤æ— æ•ˆæ–‡æœ¬"""
        if len(text) < self.MIN_LENGTH:
            return False, "Too Short"

        entropy = self.calculate_shannon_entropy(text)

        if entropy < self.MIN_ENTROPY:
            return False, f"Low Entropy ({entropy:.2f}) - Likely menu/nav/ad"
        if entropy > self.MAX_ENTROPY:
            return False, f"High Entropy ({entropy:.2f}) - Likely code/hash"

        return True, entropy

    def _normalize_url(self, url):
        """è§„èŒƒåŒ–URLï¼šç§»é™¤fragmentï¼Œå¤„ç†æœ«å°¾æ–œæ ç­‰"""
        if not url:
            return None
        
        # ç§»é™¤fragment
        url = url.split('#')[0]
        
        # è§£æå¹¶é‡å»ºURLä»¥è§„èŒƒåŒ–
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return None
        
        # è§„èŒƒåŒ–è·¯å¾„ï¼ˆç§»é™¤./å’Œ../ï¼‰
        path = parsed.path
        if path:
            parts = path.split('/')
            normalized_parts = []
            for part in parts:
                if part == '..':
                    if normalized_parts:
                        normalized_parts.pop()
                elif part and part != '.':
                    normalized_parts.append(part)
            path = '/' + '/'.join(normalized_parts)
        
        # é‡å»ºURL
        normalized = f"{parsed.scheme}://{parsed.netloc}{path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        
        return normalized
    
    def _is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or len(url) > 2048:  # URLé•¿åº¦é™åˆ¶
            return False
        
        parsed = urlparse(url)
        # åªæ¥å—httpå’Œhttps
        if parsed.scheme not in ['http', 'https']:
            return False
        
        # è¿‡æ»¤æ— æ•ˆåè®®
        if url.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
            return False
        
        return True
    
    def parse(self, url):
        """
        çˆ¬å–å¹¶æ‹†åˆ†å›¾æ–‡ - åŒæ­¥æ¥å£ï¼ˆå‘åå…¼å®¹ï¼‰
        è¿”å›æ ¼å¼: {"url": str, "texts": List[str], "images": List[str], "links": List[str]}
        """
        # è¾“å…¥éªŒè¯
        if not url or not isinstance(url, str):
            logger.error(f"Invalid URL input: {url}")
            return None
        
        if not self._is_valid_url(url):
            logger.error(f"Invalid URL format: {url}")
            return None
        
        # è§„èŒƒåŒ–URL
        url = self._normalize_url(url)
        if not url:
            logger.error(f"Failed to normalize URL: {url}")
            return None
        
        try:
            # æ”¹è¿›çš„HTTP Headersï¼Œæ›´åƒçœŸå®æµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,de;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
            response.raise_for_status()

            # æ”¹è¿›çš„ç¼–ç æ£€æµ‹ï¼šä¼˜å…ˆä½¿ç”¨å“åº”å£°æ˜çš„ç¼–ç ï¼Œå¦åˆ™å°è¯•æ£€æµ‹
            if response.encoding:
                try:
                    html = response.text
                except UnicodeDecodeError:
                    # å¦‚æœå£°æ˜çš„ç¼–ç å¤±è´¥ï¼Œå°è¯•UTF-8
                    html = response.content.decode('utf-8', errors='replace')
            else:
                # å°è¯•å¤šç§å¸¸è§ç¼–ç 
                encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
                html = None
                for encoding in encodings:
                    try:
                        html = response.content.decode(encoding)
                        break
                    except (UnicodeDecodeError, LookupError):
                        continue
                if html is None:
                    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶æ›¿æ¢é”™è¯¯å­—ç¬¦
                    html = response.content.decode('utf-8', errors='replace')

            # å°è¯•ä½¿ç”¨lxmlï¼ˆæ›´å¿«ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°html.parser
            try:
                soup = BeautifulSoup(html, 'lxml')
            except Exception:
                logger.debug(f"lxml parser failed for {url}, falling back to html.parser")
                soup = BeautifulSoup(html, 'html.parser')

            # 1. æå–å›¾åƒ
            images = []
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src')
                if src and not src.startswith('data:'):
                    full_url = urljoin(url, src)
                    # æ”¹è¿›çš„æ‰©å±•åæå–ï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œfragment
                    ext = full_url.split('.')[-1].lower().split('?')[0].split('#')[0]
                    if ext in ['jpg', 'jpeg', 'png', 'webp', 'gif', 'svg']:
                        images.append(full_url)

            # 1.5 Remove Noise (Navigation, Footer, Scripts, etc.)
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'noscript', 'iframe', 'svg']):
                element.decompose()
            
            # Remove elements with specific classes/ids indicating noise
            noise_keywords = ['menu', 'cookie', 'popup', 'banner', 'sidebar', 'search', 'language', 'login', 'copyright']
            for tag in list(soup.find_all(True)):
                if not hasattr(tag, 'attrs') or tag.attrs is None:
                    continue
                    
                # Check class and id
                classes = tag.get('class', [])
                if isinstance(classes, list):
                    classes = ' '.join(classes)
                ids = tag.get('id', '')
                
                combined = (str(classes) + " " + str(ids)).lower()
                if any(keyword in combined for keyword in noise_keywords):
                    tag.decompose()

            # ç§»é™¤æ³¨é‡Š
            for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):
                comment.extract()

            # 2. æå–å¹¶æ¸…æ´—æ–‡æœ¬
            text_blocks = []
            # ä¼˜å…ˆæå–æ­£æ–‡ç›¸å…³çš„æ ‡ç­¾
            for tag in soup.find_all(['p', 'article', 'main', 'section', 'div']):
                text = tag.get_text(strip=True, separator=' ')
                
                # Filter out common UI text
                ui_phrases = [
                    "close menu", "search navigation", "reset search", 
                    "all rights reserved", "privacy policy", "legal notice",
                    "cookie", "accept", "decline", "skip to content"
                ]
                if any(phrase in text.lower() for phrase in ui_phrases):
                    continue
                    
                valid, reason = self.is_valid_text(text)
                if valid:
                    text_blocks.append(text)
            
            # å»é‡ä½†ä¿ç•™é¡ºåº
            text_blocks = list(dict.fromkeys(text_blocks))

            # 3. Extract Links (Recursive Crawling) - æ”¹è¿›ï¼šè¿‡æ»¤æ— æ•ˆé“¾æ¥
            links = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                
                # è¿‡æ»¤æ— æ•ˆåè®®
                if href.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
                    continue
                
                full_url = urljoin(url, href)
                
                # è§„èŒƒåŒ–URL
                parsed = urlparse(full_url)
                if parsed.scheme in ['http', 'https']:
                    # è§„èŒƒåŒ–URL
                    normalized = self._normalize_url(full_url)
                    if normalized and len(normalized) <= 2048:  # URLé•¿åº¦é™åˆ¶
                        links.append(normalized)
            
            # Deduplicate
            links = list(dict.fromkeys(links))

            return {
                "url": url,
                "texts": text_blocks,
                "images": images[:5],  # é™åˆ¶æ¯é¡µæœ€å¤šå–å‰5å¼ å›¾
                "links": links
            }

        except requests.exceptions.Timeout:
            logger.error(f"Timeout for {url}")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error for {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"Crawler Error for {url}: {e}")
            traceback.print_exc()
            return None


class OptimizedCrawler:
    """
    ä¼˜åŒ–çš„å¼‚æ­¥çˆ¬è™«ç±» - æ”¯æŒæ‰¹é‡å¹¶å‘å¤„ç†å’Œæ·±åº¦é€’å½’çˆ¬å–
    æä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥çˆ¬å–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒä¸ SmartCrawler å…¼å®¹çš„è¿”å›æ ¼å¼
    """
    def __init__(self, concurrency=5, timeout=10, delay=1.0, max_rate=None, max_redirects=5, verify_ssl=True, 
                 enable_cache=True, max_cache_size=1000, same_domain_only=True, max_path_depth=None,
                 exclude_static=True, exclude_extensions=None):
        """
        Args:
            concurrency: å¹¶å‘æ•°ï¼Œé˜²æ­¢å°IP
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            delay: è¯·æ±‚ä¹‹é—´çš„æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹
            max_rate: å…¨å±€æœ€å¤§è¯·æ±‚é€Ÿç‡ï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            max_redirects: æœ€å¤§é‡å®šå‘æ·±åº¦ï¼Œé˜²æ­¢æ— é™å¾ªç¯
            verify_ssl: æ˜¯å¦éªŒè¯SSLè¯ä¹¦ï¼ˆé»˜è®¤Trueï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®å¯ç”¨ï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨URLç¼“å­˜ï¼Œé¿å…é‡å¤çˆ¬å–
            max_cache_size: æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆURLæ•°é‡ï¼‰
            same_domain_only: æ˜¯å¦åªçˆ¬å–åŒä¸€åŸŸåï¼ˆæ·±åº¦çˆ¬å–æ—¶ï¼‰
            max_path_depth: æœ€å¤§è·¯å¾„æ·±åº¦é™åˆ¶ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
            exclude_static: æ˜¯å¦æ’é™¤é™æ€èµ„æºæ–‡ä»¶
            exclude_extensions: è¦æ’é™¤çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ˆé»˜è®¤: pdf, jpg, png, gif, css, jsç­‰ï¼‰
        """
        if HAS_FAKE_USERAGENT:
            self.ua = UserAgent()
        else:
            self.ua = None
        
        self.semaphore = asyncio.Semaphore(concurrency)
        self.timeout = aiohttp.ClientTimeout(total=timeout)
        self.executor = ThreadPoolExecutor(max_workers=4)  # ç”¨äºCPUå¯†é›†å‹ä»»åŠ¡
        
        # åçˆ¬è™«ï¼šè¯·æ±‚å»¶è¿Ÿå’Œé€Ÿç‡é™åˆ¶
        self.delay = delay  # è¯·æ±‚ä¹‹é—´çš„æœ€å°å»¶è¿Ÿ
        self.last_request_time = {}  # æŒ‰åŸŸåè®°å½•æœ€åè¯·æ±‚æ—¶é—´
        self.max_rate = max_rate  # å…¨å±€é€Ÿç‡é™åˆ¶
        self.max_redirects = max_redirects  # æœ€å¤§é‡å®šå‘æ·±åº¦
        self.verify_ssl = verify_ssl  # SSLéªŒè¯
        
        # çº¿ç¨‹å®‰å…¨ï¼šä½¿ç”¨é”ä¿æŠ¤å…±äº«çŠ¶æ€
        self._rate_limit_lock = asyncio.Lock()
        self._domain_delay_lock = asyncio.Lock()
        self._last_url_lock = asyncio.Lock()
        
        self.rate_limiter = None
        if max_rate:
            # ä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•å®ç°é€Ÿç‡é™åˆ¶
            self.rate_limiter = {
                'tokens': max_rate,
                'last_update': time.time(),
                'max_tokens': max_rate
            }
        
        # ä¼˜åŒ–åçš„é˜ˆå€¼
        # ç†µå€¼é˜ˆå€¼ï¼šæ ¹æ®ç»éªŒï¼Œè‹±æ–‡/å¾·æ–‡è‡ªç„¶è¯­è¨€é€šå¸¸åœ¨ 3.5 åˆ° 5.8 ä¹‹é—´
        self.MIN_LENGTH = 30
        self.MIN_ENTROPY = 3.5
        self.MAX_ENTROPY = 6.5
        
        # é¢„ç¼–è¯‘æ­£åˆ™ï¼Œæå‡é€Ÿåº¦
        self.noise_pattern = re.compile(
            r'menu|cookie|popup|banner|sidebar|search|language|login|copyright|footer|header', 
            re.IGNORECASE
        )
        self.clean_tags = ['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'noscript', 'iframe', 'svg']
        
        # UIçŸ­è¯­è¿‡æ»¤
        self.ui_phrases = re.compile(
            r'close menu|search navigation|reset search|all rights reserved|privacy policy|legal notice|cookie|accept|decline',
            re.IGNORECASE
        )
        
        # æ·±åº¦çˆ¬å–ç›¸å…³é…ç½®
        self.enable_cache = enable_cache
        self.max_cache_size = max_cache_size
        self.same_domain_only = same_domain_only
        self.max_path_depth = max_path_depth
        self.exclude_static = exclude_static
        
        # é»˜è®¤æ’é™¤çš„é™æ€èµ„æºæ‰©å±•å
        if exclude_extensions is None:
            exclude_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', 
                                  '.css', '.js', '.zip', '.tar', '.gz', '.xml', '.json',
                                  '.mp4', '.mp3', '.avi', '.mov', '.wmv', '.flv',
                                  '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx']
        self.exclude_extensions = set(ext.lower() for ext in exclude_extensions)
        
        # URLç¼“å­˜ï¼ˆç”¨äºé¿å…é‡å¤çˆ¬å–ï¼‰
        self.url_cache = {}  # {url: result}
        self.cache_lock = asyncio.Lock()
        
        # çˆ¬å–ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'failed_requests': 0
        }

    def _get_user_agent(self):
        """è·å–User-Agent"""
        if self.ua:
            try:
                return self.ua.random
            except:
                pass
        return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    
    def _normalize_url(self, url):
        """è§„èŒƒåŒ–URLï¼šç§»é™¤fragmentï¼Œå¤„ç†æœ«å°¾æ–œæ ç­‰"""
        if not url:
            return None
        
        # ç§»é™¤fragment
        url = url.split('#')[0]
        
        # è§£æå¹¶é‡å»ºURLä»¥è§„èŒƒåŒ–
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return None
        
        # è§„èŒƒåŒ–è·¯å¾„ï¼ˆç§»é™¤./å’Œ../ï¼‰
        path = parsed.path
        if path:
            # ç®€å•çš„è·¯å¾„è§„èŒƒåŒ–
            parts = path.split('/')
            normalized_parts = []
            for part in parts:
                if part == '..':
                    if normalized_parts:
                        normalized_parts.pop()
                elif part and part != '.':
                    normalized_parts.append(part)
            path = '/' + '/'.join(normalized_parts)
        
        # é‡å»ºURL
        normalized = f"{parsed.scheme}://{parsed.netloc}{path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        
        return normalized
    
    def _is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        if not url or len(url) > 2048:  # URLé•¿åº¦é™åˆ¶
            return False
        
        parsed = urlparse(url)
        # åªæ¥å—httpå’Œhttps
        if parsed.scheme not in ['http', 'https']:
            return False
        
        # è¿‡æ»¤æ— æ•ˆåè®®
        if url.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
            return False
        
        return True
    
    def _is_valid_link_for_crawl(self, url, start_domain=None):
        """
        æ·±åº¦çˆ¬å–æ—¶çš„é“¾æ¥è¿‡æ»¤ - æ›´ä¸¥æ ¼çš„éªŒè¯
        æ£€æŸ¥é™æ€èµ„æºã€è·¯å¾„æ·±åº¦ã€åŸŸåç­‰
        """
        if not self._is_valid_url(url):
            return False
        
        parsed = urlparse(url)
        
        # åŸŸåè¿‡æ»¤
        if self.same_domain_only and start_domain:
            if parsed.netloc != start_domain:
                return False
        
        # è·¯å¾„æ·±åº¦é™åˆ¶
        if self.max_path_depth is not None:
            path_parts = [p for p in parsed.path.split('/') if p]
            if len(path_parts) > self.max_path_depth:
                return False
        
        # é™æ€èµ„æºè¿‡æ»¤
        if self.exclude_static:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            path_lower = parsed.path.lower()
            for ext in self.exclude_extensions:
                if path_lower.endswith(ext):
                    return False
            
            # æ£€æŸ¥å¸¸è§çš„é™æ€èµ„æºè·¯å¾„æ¨¡å¼
            static_patterns = ['/static/', '/assets/', '/media/', '/files/', 
                             '/downloads/', '/images/', '/img/', '/css/', '/js/']
            if any(pattern in path_lower for pattern in static_patterns):
                return False
        
        return True
    
    async def _get_from_cache(self, url):
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if not self.enable_cache:
            return None
        
        async with self.cache_lock:
            if url in self.url_cache:
                self.stats['cache_hits'] += 1
                return self.url_cache[url]
        
        self.stats['cache_misses'] += 1
        return None
    
    async def _add_to_cache(self, url, result):
        """æ·»åŠ åˆ°ç¼“å­˜"""
        if not self.enable_cache or result is None:
            return
        
        async with self.cache_lock:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆç®€å•çš„FIFOç­–ç•¥ï¼‰
            if len(self.url_cache) >= self.max_cache_size:
                # åˆ é™¤ç¬¬ä¸€ä¸ªï¼ˆæœ€æ—§çš„ï¼‰æ¡ç›®
                if self.url_cache:
                    oldest_url = next(iter(self.url_cache))
                    del self.url_cache[oldest_url]
            
            self.url_cache[url] = result
    
    async def _get_headers(self, url=None):
        """è·å–å®Œæ•´çš„HTTP Headersï¼Œæ›´åƒçœŸå®æµè§ˆå™¨"""
        headers = {
            'User-Agent': self._get_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,de;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        
        # å¦‚æœæœ‰Refererï¼Œæ·»åŠ Refererï¼ˆæ¨¡æ‹Ÿä»å…¶ä»–é¡µé¢è·³è½¬ï¼‰
        async with self._last_url_lock:
            if url and hasattr(self, '_last_url') and self._last_url:
                parsed_current = urlparse(url)
                parsed_last = urlparse(self._last_url)
                if parsed_current.netloc == parsed_last.netloc:
                    headers['Referer'] = self._last_url
        
        return headers
    
    async def _rate_limit(self):
        """å…¨å±€é€Ÿç‡é™åˆ¶ï¼ˆä»¤ç‰Œæ¡¶ç®—æ³•ï¼‰- çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        if not self.max_rate:
            return
        
        async with self._rate_limit_lock:
            now = time.time()
            rate_limiter = self.rate_limiter
            
            # æ›´æ–°ä»¤ç‰Œ
            elapsed = now - rate_limiter['last_update']
            rate_limiter['tokens'] = min(
                rate_limiter['max_tokens'],
                rate_limiter['tokens'] + elapsed * self.max_rate
            )
            rate_limiter['last_update'] = now
            
            # å¦‚æœæ²¡æœ‰ä»¤ç‰Œï¼Œç­‰å¾…
            if rate_limiter['tokens'] < 1:
                wait_time = (1 - rate_limiter['tokens']) / self.max_rate
                # é‡Šæ”¾é”åç­‰å¾…ï¼Œé¿å…é˜»å¡å…¶ä»–è¯·æ±‚
                await asyncio.sleep(wait_time)
                # é‡æ–°è·å–é”å¹¶æ›´æ–°
                async with self._rate_limit_lock:
                    rate_limiter['tokens'] = 0
            
            # æ¶ˆè€—ä¸€ä¸ªä»¤ç‰Œ
            rate_limiter['tokens'] -= 1
    
    async def _domain_delay(self, url):
        """æŒ‰åŸŸåå»¶è¿Ÿï¼Œé˜²æ­¢å¯¹åŒä¸€åŸŸåè¯·æ±‚è¿‡äºé¢‘ç¹ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        if self.delay <= 0:
            return
        
        parsed = urlparse(url)
        domain = parsed.netloc
        
        async with self._domain_delay_lock:
            now = time.time()
            
            if domain in self.last_request_time:
                elapsed = now - self.last_request_time[domain]
                if elapsed < self.delay:
                    wait_time = self.delay - elapsed
                    # é‡Šæ”¾é”åç­‰å¾…
                    await asyncio.sleep(wait_time)
                    # é‡æ–°è·å–é”å¹¶æ›´æ–°
                    async with self._domain_delay_lock:
                        self.last_request_time[domain] = time.time()
                else:
                    self.last_request_time[domain] = now
            else:
                self.last_request_time[domain] = now

    async def fetch(self, session, url, redirect_count=0, redirect_history=None):
        """
        å¼‚æ­¥è·å–é¡µé¢å†…å®¹ï¼Œå¸¦è‡ªåŠ¨é‡è¯•å’Œåçˆ¬è™«æªæ–½
        
        Args:
            session: aiohttpä¼šè¯
            url: ç›®æ ‡URL
            redirect_count: å½“å‰é‡å®šå‘æ·±åº¦
            redirect_history: é‡å®šå‘å†å²ï¼ˆç”¨äºæ£€æµ‹å¾ªç¯ï¼‰
        """
        # è¾“å…¥éªŒè¯
        if not url or not self._is_valid_url(url):
            logger.warning(f"Invalid URL: {url}")
            return None
        
        # è§„èŒƒåŒ–URL
        url = self._normalize_url(url)
        if not url:
            logger.warning(f"Failed to normalize URL: {url}")
            return None
        
        # æ£€æŸ¥é‡å®šå‘æ·±åº¦
        if redirect_count >= self.max_redirects:
            logger.warning(f"Max redirects ({self.max_redirects}) reached for {url}")
            return None
        
        # åˆå§‹åŒ–é‡å®šå‘å†å²
        if redirect_history is None:
            redirect_history = set()
        
        # æ£€æŸ¥é‡å®šå‘å¾ªç¯
        if url in redirect_history:
            logger.warning(f"Redirect loop detected: {url} -> {redirect_history}")
            return None
        
        # åçˆ¬è™«ï¼šé€Ÿç‡é™åˆ¶å’ŒåŸŸåå»¶è¿Ÿ
        await self._rate_limit()
        await self._domain_delay(url)
        
        retries = 3
        for i in range(retries):
            try:
                headers = await self._get_headers(url)
                async with self.semaphore:
                    async with session.get(
                        url, 
                        headers=headers, 
                        timeout=self.timeout, 
                        ssl=self.verify_ssl, 
                        allow_redirects=False
                    ) as response:
                        if response.status == 200:
                            # è®°å½•æœ€åè®¿é—®çš„URLï¼ˆç”¨äºRefererï¼‰
                            async with self._last_url_lock:
                                self._last_url = url
                            # aiohttpä¼šè‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œä½†æ·»åŠ é”™è¯¯å¤„ç†
                            try:
                                return await response.text()
                            except UnicodeDecodeError:
                                # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨è§£ç 
                                content = await response.read()
                                # å°è¯•å¸¸è§ç¼–ç 
                                encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
                                for encoding in encodings:
                                    try:
                                        return content.decode(encoding)
                                    except (UnicodeDecodeError, LookupError):
                                        continue
                                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶æ›¿æ¢é”™è¯¯å­—ç¬¦
                                return content.decode('utf-8', errors='replace')
                        elif response.status in [301, 302, 303, 307, 308]:
                            # å¤„ç†é‡å®šå‘
                            redirect_url = response.headers.get('Location')
                            if redirect_url:
                                logger.info(f"Redirecting {url} -> {redirect_url} (depth: {redirect_count + 1})")
                                # å¤„ç†ç›¸å¯¹å’Œç»å¯¹URL
                                absolute_redirect = urljoin(url, redirect_url)
                                # è§„èŒƒåŒ–é‡å®šå‘URL
                                absolute_redirect = self._normalize_url(absolute_redirect)
                                
                                if absolute_redirect and self._is_valid_url(absolute_redirect):
                                    # æ›´æ–°é‡å®šå‘å†å²
                                    new_history = redirect_history | {url}
                                    # é€’å½’å¤„ç†é‡å®šå‘
                                    return await self.fetch(
                                        session, 
                                        absolute_redirect, 
                                        redirect_count + 1, 
                                        new_history
                                    )
                                else:
                                    logger.warning(f"Invalid redirect URL: {redirect_url}")
                        else:
                            logger.warning(f"Status {response.status} for {url}")
            except asyncio.TimeoutError:
                logger.debug(f"Timeout {i+1}/{retries} for {url}")
            except aiohttp.ClientError as e:
                logger.debug(f"Client error {i+1}/{retries} for {url}: {e}")
            except Exception as e:
                logger.debug(f"Retry {i+1}/{retries} for {url}: {e}")
            
            if i < retries - 1:
                await asyncio.sleep(2 ** i)  # æŒ‡æ•°é€€é¿ç­–ç•¥
        
        return None

    def fast_entropy(self, text):
        """ä¼˜åŒ–çš„é¦™å†œç†µè®¡ç®— (ä½¿ç”¨äº† Counter)"""
        if not text or len(text) < 2:
            return 0
        length = len(text)
        counts = Counter(text)
        probs = (count / length for count in counts.values())
        return -sum(p * math.log2(p) for p in probs if p > 0)

    def clean_dom(self, soup):
        """æ¸…æ´— DOM æ ‘ï¼Œç§»é™¤å™ªå£°èŠ‚ç‚¹"""
        # 1. ç§»é™¤æ— ç”¨æ ‡ç­¾
        for tag in soup(self.clean_tags):
            tag.decompose()
        
        # 2. ç§»é™¤æ³¨é‡Š
        for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):
            comment.extract()

        # 3. åŸºäº Class/ID çš„å¯å‘å¼ç§»é™¤
        for tag in list(soup.find_all(True)):
            attr_str = str(tag.get('class', '')) + " " + str(tag.get('id', ''))
            if self.noise_pattern.search(attr_str):
                tag.decompose()

    def extract_content_smart(self, soup, url):
        """
        æ™ºèƒ½å†…å®¹æå–ï¼šä¿ç•™æ®µè½ç»“æ„ï¼Œè€Œä¸æ˜¯æ‰“æ•£çš„å¥å­
        è¿”å›æ ¼å¼ä¸ SmartCrawler.parse() å…¼å®¹
        """
        self.clean_dom(soup)

        # 1. æå–å›¾ç‰‡
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if not src or src.startswith('data:'):
                continue
            
            full_url = urljoin(url, src)
            # æ”¹è¿›çš„æ‰©å±•åæå–ï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œfragment
            ext = full_url.split('.')[-1].lower().split('?')[0].split('#')[0]
            if ext in ['jpg', 'jpeg', 'png', 'webp', 'gif', 'svg']:
                images.append(full_url)

        # 2. æå–é“¾æ¥ï¼ˆæ”¹è¿›ï¼šä½¿ç”¨å¢å¼ºçš„é“¾æ¥è¿‡æ»¤ï¼‰
        links = set()
        for a in soup.find_all('a', href=True):
            href = a['href']
            
            # è¿‡æ»¤æ— æ•ˆåè®®
            if href.lower().startswith(('javascript:', 'mailto:', 'tel:', 'data:', 'file:')):
                continue
            
            full_link = urljoin(url, href)
            
            # è§„èŒƒåŒ–URL
            normalized = self._normalize_url(full_link)
            if normalized and self._is_valid_url(normalized):
                # ä½¿ç”¨å¢å¼ºçš„é“¾æ¥è¿‡æ»¤ï¼ˆå¦‚æœæä¾›äº†åŸŸåï¼Œä¼šè¿›è¡Œæ›´ä¸¥æ ¼çš„æ£€æŸ¥ï¼‰
                # è¿™é‡ŒåªåšåŸºæœ¬éªŒè¯ï¼Œæ·±åº¦çˆ¬å–æ—¶ä¼šä½¿ç”¨ _is_valid_link_for_crawl
                links.add(normalized)

        # 3. æå–æ–‡æœ¬ (æ ¸å¿ƒä¼˜åŒ–ï¼šåŸºäºå—çš„æå–ï¼Œæ”¯æŒæ›´å¤šå†…å®¹ç±»å‹)
        text_blocks = []
        
        # æå–æ ‡é¢˜ï¼ˆä¿ç•™å±‚æ¬¡ç»“æ„ä¿¡æ¯ï¼‰
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = tag.get_text(strip=True, separator=' ')
            if text and len(text) >= 10:  # æ ‡é¢˜å¯ä»¥çŸ­ä¸€äº›
                text_blocks.append(text)
        
        # æå–æ®µè½å’Œä¸»è¦å†…å®¹æ ‡ç­¾
        for tag in soup.find_all(['p', 'article', 'main', 'section', 'div']):
            text = tag.get_text(strip=True, separator=' ')
            
            # è¿‡æ»¤UIçŸ­è¯­
            if self.ui_phrases.search(text):
                continue
            
            # é•¿åº¦æ£€æŸ¥
            if len(text) < self.MIN_LENGTH:
                continue
            
            # ç†µå€¼æ£€æŸ¥
            entropy = self.fast_entropy(text)
            if self.MIN_ENTROPY <= entropy <= self.MAX_ENTROPY:
                text_blocks.append(text)
        
        # æå–åˆ—è¡¨é¡¹ï¼ˆliæ ‡ç­¾ï¼‰- é€šå¸¸åŒ…å«æœ‰ç”¨ä¿¡æ¯
        for tag in soup.find_all(['li']):
            text = tag.get_text(strip=True, separator=' ')
            # åˆ—è¡¨é¡¹å¯ä»¥ç¨çŸ­
            if len(text) >= 20 and len(text) < 500:  # é¿å…è¿‡é•¿çš„åˆ—è¡¨é¡¹
                if self.ui_phrases.search(text):
                    continue
                entropy = self.fast_entropy(text)
                if self.MIN_ENTROPY <= entropy <= self.MAX_ENTROPY:
                    text_blocks.append(text)
        
        # æå–è¡¨æ ¼å†…å®¹ï¼ˆtdæ ‡ç­¾ï¼‰- æŸäº›è¡¨æ ¼å¯èƒ½åŒ…å«é‡è¦æ•°æ®
        for tag in soup.find_all(['td', 'th']):
            text = tag.get_text(strip=True, separator=' ')
            if len(text) >= 15 and len(text) < 300:
                if self.ui_phrases.search(text):
                    continue
                entropy = self.fast_entropy(text)
                if self.MIN_ENTROPY <= entropy <= self.MAX_ENTROPY:
                    text_blocks.append(text)
        
        # æå–ä»£ç å—ä¸­çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆcode, preæ ‡ç­¾ï¼‰
        for tag in soup.find_all(['code', 'pre']):
            text = tag.get_text(strip=True)
            # ä»£ç å—é€šå¸¸è¾ƒé•¿ï¼Œä½†æˆ‘ä»¬åªæå–ç›¸å¯¹çŸ­çš„ä»£ç ç‰‡æ®µæˆ–æ³¨é‡Š
            if len(text) >= 30 and len(text) < 200:
                # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯æ³¨é‡Šæˆ–æ–‡æ¡£
                if '//' in text or '/*' in text or '#' in text or '"""' in text:
                    text_blocks.append(text)
        
        # æå–å—å¼•ç”¨ï¼ˆblockquoteï¼‰- é€šå¸¸åŒ…å«é‡è¦å¼•ç”¨
        for tag in soup.find_all(['blockquote']):
            text = tag.get_text(strip=True, separator=' ')
            if len(text) >= self.MIN_LENGTH:
                entropy = self.fast_entropy(text)
                if self.MIN_ENTROPY <= entropy <= self.MAX_ENTROPY:
                    text_blocks.append(text)

        # å»é‡ä½†ä¿ç•™é¡ºåº
        text_blocks = list(dict.fromkeys(text_blocks))

        # è¿”å›æ ¼å¼ä¸ SmartCrawler å…¼å®¹
        return {
            "url": url,
            "title": soup.title.string.strip() if soup.title and soup.title.string else "",
            "texts": text_blocks,  # å…³é”®ï¼šä½¿ç”¨ texts è€Œä¸æ˜¯ content_blocks
            "images": images[:5],
            "links": list(links)
        }

    async def process_url(self, session, url):
        """å•ä¸ª URL çš„å¤„ç†æµ - æ”¯æŒç¼“å­˜"""
        # è§„èŒƒåŒ–URL
        normalized_url = self._normalize_url(url)
        if not normalized_url:
            return None
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = await self._get_from_cache(normalized_url)
        if cached_result is not None:
            logger.debug(f"Cache hit for {normalized_url}")
            return cached_result
        
        # ç»Ÿè®¡
        self.stats['total_requests'] += 1
        
        html = await self.fetch(session, normalized_url)
        if not html:
            self.stats['failed_requests'] += 1
            return None

        # å°† CPU å¯†é›†å‹çš„è§£æä»»åŠ¡æ”¾åˆ°çº¿ç¨‹æ± ä¸­ï¼Œé¿å…é˜»å¡ Event Loop
        loop = asyncio.get_running_loop()
        try:
            result = await loop.run_in_executor(self.executor, self._parse_sync, html, normalized_url)
            # æ·»åŠ åˆ°ç¼“å­˜
            await self._add_to_cache(normalized_url, result)
            return result
        except Exception as e:
            logger.error(f"Parse error {normalized_url}: {e}")
            self.stats['failed_requests'] += 1
            return None

    def _parse_sync(self, html, url):
        """åŒæ­¥è§£æé€»è¾‘ (è¿è¡Œåœ¨çº¿ç¨‹æ± ä¸­) - å¸¦è§£æå™¨å›é€€"""
        try:
            # å°è¯•ä½¿ç”¨lxmlï¼ˆæ›´å¿«ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°html.parser
            try:
                soup = BeautifulSoup(html, 'lxml')
            except Exception:
                logger.debug(f"lxml parser failed for {url}, falling back to html.parser")
                soup = BeautifulSoup(html, 'html.parser')
            
            return self.extract_content_smart(soup, url)
        except Exception as e:
            logger.error(f"BeautifulSoup parse error for {url}: {e}")
            return None

    async def run(self, urls: List[str]) -> List[Dict]:
        """
        ä¸»å…¥å£ - å¼‚æ­¥æ‰¹é‡å¤„ç†URLåˆ—è¡¨
        Args:
            urls: URLåˆ—è¡¨
        Returns:
            æˆåŠŸçˆ¬å–çš„ç»“æœåˆ—è¡¨
        """
        async with aiohttp.ClientSession() as session:
            tasks = [self.process_url(session, url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è¿‡æ»¤æ‰å¼‚å¸¸å’ŒNone
            valid_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Error processing {urls[i]}: {result}")
                elif result is not None:
                    valid_results.append(result)
            
            return valid_results

    async def crawl_recursive(self, start_url: str, max_depth: int = 3, max_pages: Optional[int] = None,
                              callback=None, same_domain_only: Optional[bool] = None) -> List[Dict]:
        """
        æ·±åº¦é€’å½’çˆ¬å– - ä½¿ç”¨BFSç®—æ³•æŒ‰å±‚çˆ¬å–
        
        Args:
            start_url: èµ·å§‹URL
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦ï¼ˆ0è¡¨ç¤ºåªçˆ¬å–èµ·å§‹URLï¼‰
            max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
            callback: å›è°ƒå‡½æ•° callback(count, url, result) åœ¨æ¯ä¸ªé¡µé¢çˆ¬å–å®Œæˆåè°ƒç”¨
            same_domain_only: æ˜¯å¦åªçˆ¬å–åŒä¸€åŸŸåï¼ˆNoneè¡¨ç¤ºä½¿ç”¨åˆå§‹åŒ–æ—¶çš„è®¾ç½®ï¼‰
        
        Returns:
            æ‰€æœ‰çˆ¬å–ç»“æœåˆ—è¡¨
        """
        if same_domain_only is None:
            same_domain_only = self.same_domain_only
        
        # è§„èŒƒåŒ–èµ·å§‹URL
        start_url = self._normalize_url(start_url)
        if not start_url:
            logger.error(f"Invalid start URL: {start_url}")
            return []
        
        parsed_start = urlparse(start_url)
        start_domain = parsed_start.netloc
        
        visited = set()  # å·²è®¿é—®çš„URLé›†åˆ
        queue = [(start_url, 0)]  # (url, depth) é˜Ÿåˆ—ï¼ŒBFS
        results = []
        count = 0
        
        logger.info(f"ğŸš€ Starting recursive crawl from {start_url} (max_depth={max_depth}, max_pages={max_pages or 'unlimited'})")
        
        async with aiohttp.ClientSession() as session:
            while queue:
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é¡µé¢æ•°é™åˆ¶
                if max_pages and count >= max_pages:
                    logger.info(f"Reached max_pages limit: {max_pages}")
                    break
                
                # è·å–å½“å‰å±‚çš„æ‰€æœ‰URLï¼ˆåŒä¸€æ·±åº¦çš„URLï¼‰
                current_level = []
                current_depth = queue[0][1] if queue else -1
                
                # æ”¶é›†åŒä¸€æ·±åº¦çš„æ‰€æœ‰URL
                while queue and queue[0][1] == current_depth:
                    url, depth = queue.pop(0)
                    if url not in visited:
                        visited.add(url)
                        current_level.append((url, depth))
                
                if not current_level:
                    break
                
                # å¹¶å‘çˆ¬å–å½“å‰å±‚çš„æ‰€æœ‰URL
                tasks = [self.process_url(session, url) for url, _ in current_level]
                level_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # å¤„ç†å½“å‰å±‚çš„ç»“æœ
                for i, (url, depth) in enumerate(current_level):
                    result = level_results[i]
                    
                    if isinstance(result, Exception):
                        logger.error(f"Error processing {url}: {result}")
                        continue
                    
                    if result is None:
                        continue
                    
                    results.append(result)
                    count += 1
                    
                    # è°ƒç”¨å›è°ƒå‡½æ•°
                    if callback:
                        try:
                            callback(count, url, result)
                        except Exception as e:
                            logger.warning(f"Callback error for {url}: {e}")
                    
                    logger.info(f"[{count}] Depth {depth}: {url} - Found {len(result.get('texts', []))} text blocks, {len(result.get('links', []))} links")
                    
                    # å¦‚æœè¿˜æœ‰æ·±åº¦ï¼Œæ”¶é›†ä¸‹ä¸€å±‚çš„é“¾æ¥
                    if depth < max_depth:
                        links = result.get('links', [])
                        for link in links:
                            # è§„èŒƒåŒ–é“¾æ¥
                            normalized_link = self._normalize_url(link)
                            if not normalized_link:
                                continue
                            
                            # ä½¿ç”¨å¢å¼ºçš„é“¾æ¥è¿‡æ»¤
                            if self._is_valid_link_for_crawl(normalized_link, start_domain if same_domain_only else None):
                                if normalized_link not in visited:
                                    # é¿å…é‡å¤æ·»åŠ åˆ°é˜Ÿåˆ—
                                    if not any(nl == normalized_link for nl, _ in queue):
                                        queue.append((normalized_link, depth + 1))
                
                logger.info(f"Completed depth {current_depth}: processed {len(current_level)} pages, found {len(queue)} URLs for next level")
        
        logger.info(f"âœ… Recursive crawl finished. Processed {count} pages in total.")
        return results

    def parse(self, url: str) -> Optional[Dict]:
        """
        åŒæ­¥æ¥å£ - å…¼å®¹ SmartCrawler.parse()
        ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾›åŒæ­¥æ¥å£
        """
        # è¾“å…¥éªŒè¯
        if not url or not isinstance(url, str):
            logger.error(f"Invalid URL input: {url}")
            return None
        
        if not self._is_valid_url(url):
            logger.error(f"Invalid URL format: {url}")
            return None
        
        try:
            # ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨å¼‚æ­¥æ–¹æ³•
            try:
                loop = asyncio.get_running_loop()
                # å¦‚æœäº‹ä»¶å¾ªç¯å·²ç»åœ¨è¿è¡Œï¼Œä½¿ç”¨çº¿ç¨‹æ± 
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.run([url]))
                    results = future.result(timeout=60)  # æ·»åŠ è¶…æ—¶
                    return results[0] if results else None
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥ä½¿ç”¨asyncio.run
                results = asyncio.run(self.run([url]))
                return results[0] if results else None
        except Exception as e:
            logger.error(f"Error in parse({url}): {e}")
            return None

    def get_stats(self):
        """è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
        cache_hit_rate = 0
        if self.stats['total_requests'] + self.stats['cache_hits'] > 0:
            cache_hit_rate = self.stats['cache_hits'] / (self.stats['total_requests'] + self.stats['cache_hits'])
        
        return {
            **self.stats,
            'cache_hit_rate': f"{cache_hit_rate:.2%}",
            'cache_size': len(self.url_cache),
            'max_cache_size': self.max_cache_size
        }
    
    def clear_cache(self):
        """æ¸…ç©ºURLç¼“å­˜"""
        async with self.cache_lock:
            self.url_cache.clear()
            logger.info("Cache cleared")
    
    def close(self):
        """æ˜¾å¼å…³é—­èµ„æºï¼ˆæ¨èä½¿ç”¨ï¼‰"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
    
    def __del__(self):
        """æ¸…ç†èµ„æºï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
        except:
            pass  # å¿½ç•¥æ¸…ç†æ—¶çš„é”™è¯¯
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()
        return False


# ä¸ºäº†å‘åå…¼å®¹ï¼Œé»˜è®¤å¯¼å‡º SmartCrawler
# å¦‚æœé¡¹ç›®éœ€è¦é«˜æ€§èƒ½ï¼Œå¯ä»¥æ”¹ç”¨ OptimizedCrawler
if __name__ == "__main__":
    import time
    
    # æµ‹è¯• SmartCrawler (åŒæ­¥)
    print("=" * 60)
    print("Testing SmartCrawler (Synchronous)")
    print("=" * 60)
    crawler_sync = SmartCrawler()
    start = time.time()
    result = crawler_sync.parse("https://www.tum.de/en/")
    end = time.time()
    
    if result:
        print(f"\nâœ… Crawled in {end - start:.2f} seconds")
        print(f"ğŸ“„ Title: {result.get('title', 'N/A')}")
        print(f"ğŸ“ Text blocks: {len(result['texts'])}")
        print(f"ğŸ–¼ï¸ Images: {len(result['images'])}")
        print(f"ğŸ”— Links: {len(result['links'])}")
        print("\nğŸ“ Sample texts:")
        for i, text in enumerate(result['texts'][:3], 1):
            print(f"   {i}. {text[:80]}...")
    
    # æµ‹è¯• OptimizedCrawler (å¼‚æ­¥)
    print("\n" + "=" * 60)
    print("Testing OptimizedCrawler (Asynchronous)")
    print("=" * 60)
    crawler_async = OptimizedCrawler(concurrency=3)
    
    target_urls = [
        "https://www.tum.de/en/",
        "https://www.tum.de/en/studies/",
        "https://www.tum.de/en/research/"
    ]
    
    start = time.time()
    results = asyncio.run(crawler_async.run(target_urls))
    end = time.time()
    
    print(f"\nâœ… Crawled {len(results)} pages in {end - start:.2f} seconds")
    for result in results:
        if result:
            print(f"\nğŸ“„ {result['url']}")
            print(f"   Texts: {len(result['texts'])} | Images: {len(result['images'])} | Links: {len(result['links'])}")
