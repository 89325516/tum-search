#!/usr/bin/env python3
"""
è¯¦ç»†æµ‹è¯•çˆ¬è™« - å¸¦æ—¥å¿—è¾“å‡º
"""
import logging
import sys
import traceback

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

print("ğŸ” å¯ç”¨è¯¦ç»†æ—¥å¿—è¿›è¡Œæµ‹è¯•...\n")

from crawler_v2 import SyncCrawlerWrapper

# æµ‹è¯•ä¸€ä¸ªç®€å•çš„URL
test_url = "https://httpbin.org/html"  # ç®€å•çš„HTMLé¡µé¢

print(f"æµ‹è¯•URL: {test_url}\n")

try:
    crawler = SyncCrawlerWrapper(
        enable_robots=False,
        enable_content_dedup=False,  # æš‚æ—¶ç¦ç”¨å»é‡ä»¥ä¾¿æµ‹è¯•
        concurrency=1,
        delay=0.5,
        timeout=10,
        verify_ssl=False  # ç¦ç”¨SSLéªŒè¯
    )
    
    print("å¼€å§‹è§£æ...\n")
    result = crawler.parse(test_url)
    
    if result:
        print(f"\nâœ… æˆåŠŸ!")
        print(f"URL: {result.get('url')}")
        print(f"æ–‡æœ¬å—æ•°: {len(result.get('texts', []))}")
        print(f"é“¾æ¥æ•°: {len(result.get('links', []))}")
        
        if result.get('texts'):
            print(f"\nç¬¬ä¸€ä¸ªæ–‡æœ¬å—:")
            print(result.get('texts')[0][:200])
    else:
        print("\nâŒ è¿”å›None")
        
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
    traceback.print_exc()
