#!/usr/bin/env python3
"""
MediaWiki XML Dump å¤„ç†å·¥å…·
è§£æMediaWiki XML dumpæ–‡ä»¶ï¼Œæå–é¡µé¢å’Œé“¾æ¥å…³ç³»ï¼Œç”ŸæˆèŠ‚ç‚¹å’Œè¾¹çš„CSVæ•°æ®
æ”¯æŒä¸€é”®å¯¼å…¥åˆ°æ•°æ®åº“
"""
import os
import sys
import csv
import argparse
import re
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict
from pathlib import Path

try:
    import mwxml
    import mwparserfromhell
except ImportError:
    print("âŒ ç¼ºå°‘å¿…éœ€çš„ä¾èµ–åº“ã€‚è¯·è¿è¡Œ: pip install mwxml mwparserfromhell")
    sys.exit(1)

# å¯¼å…¥ç³»ç»Ÿç®¡ç†å™¨ç”¨äºæ•°æ®åº“å¯¼å…¥
try:
    from system_manager import SystemManager, SPACE_X, SPACE_R
    from csv_importer import CSVImporter
    DB_AVAILABLE = True
except ImportError:
    print("âš ï¸  æ•°æ®åº“å¯¼å…¥åŠŸèƒ½ä¸å¯ç”¨ï¼Œå°†åªç”ŸæˆCSVæ–‡ä»¶")
    DB_AVAILABLE = False


class MediaWikiDumpProcessor:
    """MediaWiki XML Dumpå¤„ç†å™¨ - æ”¯æŒMediaWikiã€Wikipediaç­‰å¤šç§Wikiæ ¼å¼"""
    
    def __init__(self, base_url: str = "", namespace_filter: Set[int] = None, wiki_type: str = "auto"):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            base_url: Wikiçš„åŸºç¡€URLï¼Œç”¨äºç”Ÿæˆå®Œæ•´URLï¼ˆä¾‹å¦‚: https://wiki.example.comï¼‰
            namespace_filter: è¦å¤„ç†çš„å‘½åç©ºé—´IDé›†åˆï¼ˆNoneè¡¨ç¤ºæ‰€æœ‰ï¼Œé€šå¸¸0æ˜¯ä¸»å‘½åç©ºé—´ï¼‰
            wiki_type: Wikiç±»å‹ ("auto", "mediawiki", "wikipedia", "wikidata")
                      auto: è‡ªåŠ¨æ£€æµ‹ï¼ˆåŸºäºdumpæ–‡ä»¶ä¸­çš„ç«™ç‚¹ä¿¡æ¯ï¼‰
        """
        self.base_url = base_url.rstrip('/')
        self.namespace_filter = namespace_filter or {0}  # é»˜è®¤åªå¤„ç†ä¸»å‘½åç©ºé—´ï¼ˆ0ï¼‰
        self.wiki_type = wiki_type
        
        # Wikiç±»å‹ç‰¹å®šé…ç½®
        self.wiki_configs = {
            "wikipedia": {
                "url_pattern": "{base_url}/wiki/{title}",
                "skip_namespaces": {'File', 'Image', 'Category', 'Template', 'Media', 'User', 'Talk', 'Help', 'Portal'},
                "link_patterns": [r'\[\[([^\]]+)\]\]']
            },
            "mediawiki": {
                "url_pattern": "{base_url}/{title}",
                "skip_namespaces": {'File', 'Image', 'Category', 'Template', 'Media'},
                "link_patterns": [r'\[\[([^\]]+)\]\]']
            },
            "wikidata": {
                "url_pattern": "{base_url}/wiki/{title}",
                "skip_namespaces": {'Property', 'Property talk', 'Item', 'Item talk'},
                "link_patterns": [r'\[\[([^\]]+)\]\]', r'Q\d+', r'P\d+']
            }
        }
        
        self.config = None  # å°†åœ¨å¤„ç†æ—¶è‡ªåŠ¨æ£€æµ‹æˆ–è®¾ç½®
        
        # å­˜å‚¨é¡µé¢æ•°æ®ï¼špage_title -> page_data
        self.pages: Dict[str, Dict] = {}
        
        # å­˜å‚¨é“¾æ¥å…³ç³»ï¼šsource_title -> [target_title1, target_title2, ...]
        self.links: Dict[str, List[str]] = defaultdict(list)
        
        # æ ‡é¢˜åˆ°URLçš„æ˜ å°„
        self.title_to_url: Dict[str, str] = {}
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            'total_pages': 0,
            'processed_pages': 0,
            'skipped_pages': 0,
            'total_links': 0,
            'unique_links': 0
        }
    
    def normalize_title(self, title: str) -> str:
        """è§„èŒƒåŒ–é¡µé¢æ ‡é¢˜"""
        # MediaWikiæ ‡é¢˜è§„èŒƒï¼šé¦–å­—æ¯å¤§å†™ï¼Œç©ºæ ¼ä¿ç•™
        if not title:
            return ""
        # ç§»é™¤å‘½åç©ºé—´å‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
        parts = title.split(':', 1)
        if len(parts) > 1 and parts[0].lower() in ['file', 'image', 'category', 'template']:
            # è·³è¿‡æ–‡ä»¶ã€å›¾åƒã€åˆ†ç±»ã€æ¨¡æ¿ç­‰ç‰¹æ®Šé¡µé¢
            return None
        return title.replace('_', ' ')
    
    def title_to_url_path(self, title: str) -> str:
        """å°†æ ‡é¢˜è½¬æ¢ä¸ºURLè·¯å¾„"""
        # MediaWiki URLæ ¼å¼ï¼šç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        return title.replace(' ', '_')
    
    def _generate_url(self, title: str) -> str:
        """
        æ ¹æ®Wikiç±»å‹ç”ŸæˆURL
        
        Args:
            title: é¡µé¢æ ‡é¢˜
            
        Returns:
            str: å®Œæ•´çš„URL
        """
        if not self.base_url:
            return self.title_to_url_path(title)
        
        title_path = self.title_to_url_path(title)
        
        # æ ¹æ®é…ç½®ç”ŸæˆURL
        if self.config and 'url_pattern' in self.config:
            url_pattern = self.config['url_pattern']
            return url_pattern.format(base_url=self.base_url, title=title_path)
        
        # é»˜è®¤æ ¼å¼ï¼ˆMediaWikiï¼‰
        return f"{self.base_url}/{title_path}"
    
    def extract_links_from_wikicode(self, wikitext: str) -> List[str]:
        """
        ä»MediaWiki wikicodeä¸­æå–å†…éƒ¨é“¾æ¥
        
        MediaWikié“¾æ¥æ ¼å¼ï¼š
        - [[Page Title]]
        - [[Page Title|Display Text]]
        - [[Namespace:Page Title]]
        """
        if not wikitext:
            return []
        
        links = []
        
        try:
            # ä½¿ç”¨mwparserfromhellè§£æwikicode
            wikicode = mwparserfromhell.parse(wikitext)
            
            # æå–æ‰€æœ‰å†…éƒ¨é“¾æ¥
            for link in wikicode.filter_wikilinks():
                target = str(link.title).strip()
                
                # è·³è¿‡å¤–éƒ¨é“¾æ¥ã€æ–‡ä»¶é“¾æ¥ç­‰
                if ':' in target:
                    parts = target.split(':', 1)
                    namespace = parts[0].lower()
                    # è·³è¿‡ç‰¹æ®Šå‘½åç©ºé—´
                    if namespace in ['file', 'image', 'category', 'template', 'media']:
                        continue
                    # å¦‚æœæ˜¯å…¶ä»–å‘½åç©ºé—´ï¼Œå¯ä»¥ä¿ç•™æˆ–è·³è¿‡
                    # è¿™é‡Œæˆ‘ä»¬ä¿ç•™æ‰€æœ‰éç‰¹æ®Šå‘½åç©ºé—´çš„é“¾æ¥
                
                # è§„èŒƒåŒ–æ ‡é¢˜
                normalized = self.normalize_title(target)
                if normalized:
                    links.append(normalized)
        
        except Exception as e:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä½œä¸ºåå¤‡
            print(f"   âš ï¸  Wikicodeè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–: {e}")
            links = self._extract_links_regex(wikitext)
        
        return links
    
    def _extract_links_regex(self, wikitext: str) -> List[str]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–é“¾æ¥ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰"""
        links = []
        # MediaWikié“¾æ¥æ ¼å¼ï¼š[[Page Title]] æˆ– [[Page Title|Display Text]]
        pattern = r'\[\[([^\]]+)\]\]'
        matches = re.findall(pattern, wikitext)
        
        for match in matches:
            # å¤„ç†å¸¦æ˜¾ç¤ºæ–‡æœ¬çš„é“¾æ¥ï¼šPage Title|Display Text
            target = match.split('|')[0].strip()
            
            # è·³è¿‡ç‰¹æ®Šå‘½åç©ºé—´
            if ':' in target:
                parts = target.split(':', 1)
                namespace = parts[0].lower()
                if namespace in ['file', 'image', 'category', 'template', 'media', 'http']:
                    continue
            
            normalized = self.normalize_title(target)
            if normalized:
                links.append(normalized)
        
        return links
    
    def process_dump(self, dump_path: str, max_pages: Optional[int] = None, 
                    progress_callback: Optional[callable] = None):
        """
        å¤„ç†XML dumpæ–‡ä»¶
        
        Args:
            dump_path: XML dumpæ–‡ä»¶è·¯å¾„
            max_pages: æœ€å¤§å¤„ç†é¡µé¢æ•°ï¼ˆNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
        """
        print(f"ğŸ“‚ å¼€å§‹å¤„ç†XML dump: {dump_path}")
        
        if not os.path.exists(dump_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {dump_path}")
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©æ­£ç¡®çš„æ‰“å¼€æ–¹å¼ï¼ˆæ”¯æŒå‹ç¼©æ–‡ä»¶ï¼‰
        dump_path_lower = dump_path.lower()
        if dump_path_lower.endswith('.bz2'):
            import bz2
            file_opener = lambda path: bz2.open(path, 'rt', encoding='utf-8')
            print("ğŸ“¦ æ£€æµ‹åˆ° bzip2 å‹ç¼©æ–‡ä»¶")
        elif dump_path_lower.endswith('.gz'):
            import gzip
            file_opener = lambda path: gzip.open(path, 'rt', encoding='utf-8')
            print("ğŸ“¦ æ£€æµ‹åˆ° gzip å‹ç¼©æ–‡ä»¶")
        else:
            file_opener = lambda path: open(path, 'rb')
        
        # æ‰“å¼€dumpæ–‡ä»¶
        with file_opener(dump_path) as f:
            dump = mwxml.Dump.from_file(f)
            
            # æ˜¾ç¤ºç«™ç‚¹ä¿¡æ¯å¹¶æ£€æµ‹Wikiç±»å‹
            if dump.site_info:
                print(f"ğŸŒ ç«™ç‚¹åç§°: {dump.site_info.name}")
                print(f"ğŸ“¦ æ•°æ®åº“å: {dump.site_info.dbname}")
                
                # è‡ªåŠ¨æ£€æµ‹Wikiç±»å‹
                if self.wiki_type == "auto":
                    site_name = dump.site_info.name.lower()
                    db_name = dump.site_info.dbname.lower()
                    
                    if "wikipedia" in site_name or "wikipedia" in db_name:
                        self.wiki_type = "wikipedia"
                        print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹: Wikipediaæ ¼å¼")
                    elif "wikidata" in site_name or "wikidata" in db_name:
                        self.wiki_type = "wikidata"
                        print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹: Wikidataæ ¼å¼")
                    else:
                        self.wiki_type = "mediawiki"
                        print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹: MediaWikiæ ¼å¼")
                
                # åº”ç”¨Wikié…ç½®
                if self.wiki_type in self.wiki_configs:
                    self.config = self.wiki_configs[self.wiki_type]
                else:
                    self.config = self.wiki_configs["mediawiki"]  # é»˜è®¤
            
            # éå†æ‰€æœ‰é¡µé¢
            page_count = 0
            for page in dump:
                page_count += 1
                
                # æ£€æŸ¥å‘½åç©ºé—´
                if page.namespace not in self.namespace_filter:
                    continue
                
                # é™åˆ¶å¤„ç†æ•°é‡
                if max_pages and self.stats['processed_pages'] >= max_pages:
                    break
                
                # è·å–æœ€æ–°ç‰ˆæœ¬
                revisions = list(page)
                if not revisions:
                    self.stats['skipped_pages'] += 1
                    continue
                
                latest_revision = revisions[-1]
                page_text = latest_revision.text or ""
                
                # è§„èŒƒåŒ–æ ‡é¢˜
                title = self.normalize_title(page.title)
                if not title:
                    self.stats['skipped_pages'] += 1
                    continue
                
                # æå–é“¾æ¥
                page_links = self.extract_links_from_wikicode(page_text)
                
                # ç”ŸæˆURLï¼ˆæ ¹æ®Wikiç±»å‹ä½¿ç”¨ä¸åŒæ ¼å¼ï¼‰
                url = self._generate_url(title)
                
                self.pages[title] = {
                    'title': title,
                    'url': url,
                    'content': page_text,
                    'namespace': page.namespace,
                    'page_id': page.id,
                    'revision_id': latest_revision.id,
                    'timestamp': str(latest_revision.timestamp) if latest_revision.timestamp else None
                }
                
                # å¡«å…… title_to_url æ˜ å°„ï¼ˆç”¨äºè¾¹å¯¼å…¥ï¼‰
                self.title_to_url[title] = url
                
                # å­˜å‚¨é“¾æ¥å…³ç³»
                if page_links:
                    self.links[title] = page_links
                    self.stats['total_links'] += len(page_links)
                
                self.stats['processed_pages'] += 1
                self.stats['total_pages'] = page_count
                
                # è¿›åº¦å›è°ƒ
                if progress_callback and self.stats['processed_pages'] % 100 == 0:
                    progress_callback(
                        self.stats['processed_pages'],
                        self.stats['total_pages'],
                        f"å·²å¤„ç†: {title[:50]}..."
                    )
        
        self.stats['unique_links'] = len(set(
            link for links in self.links.values() for link in links
        ))
        
        print(f"âœ… å¤„ç†å®Œæˆ!")
        print(f"   æ€»é¡µé¢æ•°: {self.stats['total_pages']}")
        print(f"   å¤„ç†é¡µé¢æ•°: {self.stats['processed_pages']}")
        print(f"   è·³è¿‡é¡µé¢æ•°: {self.stats['skipped_pages']}")
        print(f"   æ€»é“¾æ¥æ•°: {self.stats['total_links']}")
        print(f"   å”¯ä¸€é“¾æ¥ç›®æ ‡æ•°: {self.stats['unique_links']}")
    
    def generate_nodes_csv(self, output_path: str):
        """
        ç”ŸæˆèŠ‚ç‚¹CSVæ–‡ä»¶ï¼ˆé¡µé¢æ•°æ®ï¼‰
        
        CSVæ ¼å¼ï¼š
        title, content, url, category
        """
        print(f"ğŸ“ ç”ŸæˆèŠ‚ç‚¹CSV: {output_path}")
        
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['title', 'content', 'url', 'category'])
            
            for title, page_data in self.pages.items():
                # æå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤wikicodeæ ‡è®°ï¼‰
                content = self._extract_text_from_wikicode(page_data['content'])
                
                # åªä¿ç•™æœ‰å†…å®¹çš„é¡µé¢
                if len(content.strip()) < 50:
                    continue
                
                writer.writerow([
                    page_data['title'],
                    content,
                    page_data['url'],
                    'Wiki'  # é»˜è®¤åˆ†ç±»
                ])
        
        print(f"   âœ… å·²ç”Ÿæˆ {len(self.pages)} ä¸ªèŠ‚ç‚¹")
    
    def generate_edges_csv(self, output_path: str):
        """
        ç”Ÿæˆè¾¹CSVæ–‡ä»¶ï¼ˆé“¾æ¥å…³ç³»ï¼‰
        
        CSVæ ¼å¼ï¼š
        source_title, target_title
        """
        print(f"ğŸ”— ç”Ÿæˆè¾¹CSV: {output_path}")
        
        edges_written = 0
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['source_title', 'target_title'])
            
            for source_title, target_titles in self.links.items():
                for target_title in target_titles:
                    # åªå†™å…¥ç›®æ ‡é¡µé¢ä¹Ÿå­˜åœ¨çš„é“¾æ¥ï¼ˆç¡®ä¿æ•°æ®å®Œæ•´æ€§ï¼‰
                    if target_title in self.pages:
                        writer.writerow([source_title, target_title])
                        edges_written += 1
        
        print(f"   âœ… å·²ç”Ÿæˆ {edges_written} æ¡è¾¹")
    
    def _extract_text_from_wikicode(self, wikitext: str) -> str:
        """
        ä»wikicodeä¸­æå–çº¯æ–‡æœ¬å†…å®¹
        """
        if not wikitext:
            return ""
        
        try:
            wikicode = mwparserfromhell.parse(wikitext)
            # è·å–çº¯æ–‡æœ¬ï¼ˆç§»é™¤æ‰€æœ‰wikicodeæ ‡è®°ï¼‰
            text = wikicode.strip_code()
            return text.strip()
        except Exception as e:
            # å¦‚æœè§£æå¤±è´¥ï¼Œç®€å•æ¸…ç†wikicodeæ ‡è®°
            text = wikitext
            # ç§»é™¤å¸¸è§çš„wikicodeæ ‡è®°
            text = re.sub(r'{{[^}]+}}', '', text)  # æ¨¡æ¿
            text = re.sub(r'\[\[([^\]]+)\]\]', r'\1', text)  # é“¾æ¥
            text = re.sub(r'={2,}[^=]+={2,}', '', text)  # æ ‡é¢˜
            text = re.sub(r"''+", '', text)  # ç²—ä½“/æ–œä½“
            return text.strip()
    
    def import_to_database(self, system_manager: SystemManager, 
                          url_prefix: str = "", batch_size: int = 50,
                          import_edges: bool = False, edges_csv_path: str = None,
                          check_db_first: bool = True):
        """
        ä¸€é”®å¯¼å…¥åˆ°æ•°æ®åº“
        
        Args:
            system_manager: SystemManagerå®ä¾‹
            url_prefix: URLå‰ç¼€ï¼ˆè¦†ç›–é¡µé¢URLï¼‰
            batch_size: æ‰¹é‡å¤§å°
            import_edges: æ˜¯å¦åŒæ—¶å¯¼å…¥è¾¹ï¼ˆé“¾æ¥å…³ç³»ï¼‰
            edges_csv_path: è¾¹CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœimport_edgesä¸ºTrueï¼‰
        """
        if not DB_AVAILABLE:
            print("âŒ æ•°æ®åº“å¯¼å…¥åŠŸèƒ½ä¸å¯ç”¨")
            return
        
        print(f"ğŸ“¦ å¼€å§‹å¯¼å…¥åˆ°æ•°æ®åº“...")
        
        from csv_importer import CSVImporter
        importer = CSVImporter(system_manager)
        
        # å‡†å¤‡CSVæ ¼å¼çš„æ•°æ®
        csv_rows = []
        for title, page_data in self.pages.items():
            content = self._extract_text_from_wikicode(page_data['content'])
            if len(content.strip()) < 50:
                continue
            
            url = page_data['url']
            if url_prefix:
                # ä½¿ç”¨é…ç½®çš„URLæ¨¡å¼æˆ–é»˜è®¤æ ¼å¼
                title_path = self.title_to_url_path(title)
                if self.config and 'url_pattern' in self.config:
                    url_pattern = self.config['url_pattern']
                    url = url_pattern.format(base_url=url_prefix, title=title_path)
                else:
                    url = f"{url_prefix}/{title_path}"
            
            csv_rows.append({
                'title': title,
                'content': content,
                'url': url,
                'category': 'Wiki'
            })
        
        # å¯¼å…¥æ•°æ®
        stats = importer.import_csv_batch(
            csv_rows,
            batch_size=batch_size,
            default_url_prefix=url_prefix or self.base_url,
            promote_novel=True,
            check_db_first=check_db_first
        )
        
        print(f"âœ… æ•°æ®åº“å¯¼å…¥å®Œæˆ!")
        print(f"   æ€»è¡Œæ•°: {stats['total']}")
        print(f"   æˆåŠŸå¯¼å…¥: {stats['success']}")
        print(f"   è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {stats.get('skipped', 0)}")
        print(f"   å¤±è´¥: {stats['failed']}")
        print(f"   æ™‹å‡åˆ°Space R: {stats['promoted']}")
        
        # å¯é€‰ï¼šå¯¼å…¥è¾¹
        if import_edges:
            if edges_csv_path and os.path.exists(edges_csv_path):
                print(f"\nğŸ”— å¼€å§‹å¯¼å…¥è¾¹...")
                try:
                    from import_edges import import_edges_from_csv
                    url_prefix_for_edges = url_prefix or self.base_url
                    import_edges_from_csv(edges_csv_path, system_manager, base_url=url_prefix_for_edges)
                except Exception as e:
                    print(f"âš ï¸  è¾¹å¯¼å…¥å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print("âš ï¸  æœªæŒ‡å®šè¾¹CSVæ–‡ä»¶è·¯å¾„ï¼Œè·³è¿‡è¾¹å¯¼å…¥")
        
        return stats


def main():
    parser = argparse.ArgumentParser(
        description='MediaWiki XML Dumpå¤„ç†å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åªç”ŸæˆCSVæ–‡ä»¶
  python xml_dump_processor.py dump.xml --base-url "https://wiki.example.com"
  
  # ç”ŸæˆCSVå¹¶å¯¼å…¥æ•°æ®åº“
  python xml_dump_processor.py dump.xml --base-url "https://wiki.example.com" --import-db
  
  # åªå¤„ç†å‰1000ä¸ªé¡µé¢ï¼ˆæµ‹è¯•ç”¨ï¼‰
  python xml_dump_processor.py dump.xml --max-pages 1000
        """
    )
    
    parser.add_argument('dump_file', help='MediaWiki XML dumpæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--base-url', default='', 
                       help='WikiåŸºç¡€URLï¼ˆä¾‹å¦‚: https://wiki.example.comï¼‰')
    parser.add_argument('--output-dir', default='.', 
                       help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: å½“å‰ç›®å½•ï¼‰')
    parser.add_argument('--nodes-csv', default='wiki_nodes.csv',
                       help='èŠ‚ç‚¹CSVæ–‡ä»¶åï¼ˆé»˜è®¤: wiki_nodes.csvï¼‰')
    parser.add_argument('--edges-csv', default='wiki_edges.csv',
                       help='è¾¹CSVæ–‡ä»¶åï¼ˆé»˜è®¤: wiki_edges.csvï¼‰')
    parser.add_argument('--max-pages', type=int, default=None,
                       help='æœ€å¤§å¤„ç†é¡µé¢æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--import-db', action='store_true',
                       help='ä¸€é”®å¯¼å…¥åˆ°æ•°æ®åº“')
    parser.add_argument('--import-edges', action='store_true',
                       help='åŒæ—¶å¯¼å…¥è¾¹ï¼ˆé“¾æ¥å…³ç³»ï¼‰åˆ°InteractionManager')
    parser.add_argument('--url-prefix', default='',
                       help='æ•°æ®åº“å¯¼å…¥æ—¶çš„URLå‰ç¼€ï¼ˆè¦†ç›–base-urlï¼‰')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='æ•°æ®åº“æ‰¹é‡å¯¼å…¥å¤§å°ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--check-db', action='store_true', default=True,
                       help='å¯¼å…¥å‰æ£€æŸ¥æ•°æ®åº“ï¼Œè·³è¿‡å·²å­˜åœ¨çš„URLï¼ˆé»˜è®¤: Trueï¼‰')
    parser.add_argument('--no-check-db', dest='check_db', action='store_false',
                       help='ç¦ç”¨æ•°æ®åº“æ£€æŸ¥ï¼Œå¼ºåˆ¶å¯¼å…¥æ‰€æœ‰æ•°æ®')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–å¤„ç†å™¨ï¼ˆæ”¯æŒwiki-typeå‚æ•°ï¼Œä½†ç›®å‰å…ˆä½¿ç”¨autoè‡ªåŠ¨æ£€æµ‹ï¼‰
    processor = MediaWikiDumpProcessor(
        base_url=args.base_url,
        wiki_type="auto"  # è‡ªåŠ¨æ£€æµ‹Wikiç±»å‹
    )
    
    # å¤„ç†dumpæ–‡ä»¶
    try:
        processor.process_dump(args.dump_file, max_pages=args.max_pages)
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # ç”ŸæˆCSVæ–‡ä»¶
    nodes_path = os.path.join(args.output_dir, args.nodes_csv)
    edges_path = os.path.join(args.output_dir, args.edges_csv)
    
    processor.generate_nodes_csv(nodes_path)
    processor.generate_edges_csv(edges_path)
    
    print(f"\nâœ… CSVæ–‡ä»¶ç”Ÿæˆå®Œæˆ:")
    print(f"   èŠ‚ç‚¹æ–‡ä»¶: {nodes_path}")
    print(f"   è¾¹æ–‡ä»¶: {edges_path}")
    
    # å¯é€‰ï¼šå¯¼å…¥æ•°æ®åº“
    if args.import_db:
        if not DB_AVAILABLE:
            print("\nâŒ æ•°æ®åº“å¯¼å…¥åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥system_managerå’Œcsv_importeræ¨¡å—")
            sys.exit(1)
        
        print("\nğŸ“¦ å¼€å§‹å¯¼å…¥åˆ°æ•°æ®åº“...")
        try:
            mgr = SystemManager()
            url_prefix = args.url_prefix or args.base_url
            edges_csv_path = edges_path if args.import_edges else None
            processor.import_to_database(
                mgr, 
                url_prefix=url_prefix, 
                batch_size=args.batch_size,
                import_edges=args.import_edges,
                edges_csv_path=edges_csv_path,
                check_db_first=args.check_db
            )
        except Exception as e:
            print(f"âŒ æ•°æ®åº“å¯¼å…¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)


if __name__ == '__main__':
    main()
