#!/usr/bin/env python3
"""
æµ‹è¯•æ–°çˆ¬è™«åŠŸèƒ½
"""
import sys
import traceback
import time
from crawler_v2 import SyncCrawlerWrapper

def test_crawler():
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•æ–°çˆ¬è™« (crawler_v2)")
    print("=" * 60)
    
    # æµ‹è¯•1: åˆ›å»ºçˆ¬è™«å®ä¾‹
    print("\n[æµ‹è¯•1] åˆ›å»ºçˆ¬è™«å®ä¾‹...")
    try:
        crawler = SyncCrawlerWrapper(
            enable_robots=False,  # ç¦ç”¨robots.txtä»¥ä¾¿æµ‹è¯•
            enable_content_dedup=True,
            concurrency=2,
            delay=1.0,
            timeout=10
        )
        print("âœ… çˆ¬è™«å®ä¾‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤±è´¥: {e}")
        traceback.print_exc()
        return False
    
    # æµ‹è¯•2: æµ‹è¯•è§£æä¸€ä¸ªçœŸå®URL
    print("\n[æµ‹è¯•2] æµ‹è¯•è§£æURL (https://www.tum.de/en/)...")
    test_url = "https://www.tum.de/en/"
    
    try:
        start_time = time.time()
        print(f"   å¼€å§‹çˆ¬å–: {test_url}")
        result = crawler.parse(test_url)
        elapsed = time.time() - start_time
        
        if result:
            print(f"âœ… è§£ææˆåŠŸ (è€—æ—¶: {elapsed:.2f}ç§’)")
            print(f"   URL: {result.get('url', 'N/A')}")
            print(f"   æ–‡æœ¬å—æ•°: {len(result.get('texts', []))}")
            print(f"   é“¾æ¥æ•°: {len(result.get('links', []))}")
            print(f"   å›¾ç‰‡æ•°: {len(result.get('images', []))}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡æœ¬å—
            texts = result.get('texts', [])
            if texts:
                print(f"\n   å‰3ä¸ªæ–‡æœ¬å—é¢„è§ˆ:")
                for i, text in enumerate(texts[:3], 1):
                    preview = text[:100].replace('\n', ' ')
                    print(f"   [{i}] {preview}...")
            
            return True
        else:
            print(f"âš ï¸  è§£æè¿”å›None (è€—æ—¶: {elapsed:.2f}ç§’)")
            print("   å¯èƒ½åŸå› :")
            print("   - ç½‘ç«™å†…å®¹è¢«è¿‡æ»¤ï¼ˆç†µå€¼æ£€æŸ¥ï¼‰")
            print("   - ç½‘ç»œè¯·æ±‚å¤±è´¥")
            print("   - robots.txté˜»æ­¢")
            return False
            
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_system_manager():
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•SystemManagerä¸­çš„çˆ¬è™«")
    print("=" * 60)
    
    try:
        print("\n[æµ‹è¯•] å¯¼å…¥SystemManager...")
        from system_manager import SystemManager
        print("âœ… SystemManagerå¯¼å…¥æˆåŠŸ")
        
        print("\n[æµ‹è¯•] åˆ›å»ºSystemManagerå®ä¾‹...")
        # æ³¨æ„ï¼šè¿™ä¼šåˆå§‹åŒ–æ•°æ®åº“è¿æ¥ç­‰ï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
        mgr = SystemManager()
        print("âœ… SystemManagerå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        print(f"\n   çˆ¬è™«ç±»å‹: {type(mgr.crawler).__name__}")
        if hasattr(mgr.crawler, 'async_crawler'):
            print(f"   å†…éƒ¨çˆ¬è™«: AsyncCrawler")
        else:
            print(f"   å†…éƒ¨çˆ¬è™«: SmartCrawler (æ—§ç‰ˆ)")
        
        return True
        
    except Exception as e:
        print(f"âŒ SystemManageræµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("\nğŸš€ å¼€å§‹æµ‹è¯•...\n")
    
    # æµ‹è¯•1: ç›´æ¥æµ‹è¯•çˆ¬è™«
    test1_result = test_crawler()
    
    # æµ‹è¯•2: æµ‹è¯•SystemManagerä¸­çš„çˆ¬è™«
    test2_result = test_system_manager()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"çˆ¬è™«åŠŸèƒ½æµ‹è¯•: {'âœ… é€šè¿‡' if test1_result else 'âš ï¸  éœ€è¦æ£€æŸ¥'}")
    print(f"SystemManageræµ‹è¯•: {'âœ… é€šè¿‡' if test2_result else 'âŒ å¤±è´¥'}")
    
    if test1_result and test2_result:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–°çˆ¬è™«å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        sys.exit(0)
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°è¾“å‡ºã€‚")
        sys.exit(1)
