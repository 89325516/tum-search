import json
import torch
import numpy as np
import random
import sys
import os
import re
from dotenv import load_dotenv

load_dotenv()
from qdrant_client import QdrantClient

# Add root to path
sys.path.append(os.getcwd())
from consistency_engine import ConsistencyEngine

from transformers import CLIPProcessor, CLIPModel
from scipy.stats import rankdata

# ================= é…ç½®åŒº =================
# ğŸ”´ ä½ çš„çœŸå®é…ç½®
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
SPACE_X = "tum_space_x"
# =========================================

print("ğŸ› ï¸Initializing Search Engine...")

# 1. è¿æ¥ Qdrant
print("ğŸ”—Connecting to Qdrant Database...")
client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

# 2. åŠ è½½æœ¬åœ° CLIP (CPUæ¨¡å¼)
print("âš™ï¸Loading local CLIP model (CPU mode)...")
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 3. åˆå§‹åŒ–ä¸€è‡´æ€§å¼•æ“
consistency_engine = ConsistencyEngine()

# --- è¾…åŠ©å‡½æ•°ï¼šé«˜æ–¯ç§©å½’ä¸€åŒ– ---
def gauss_rank_norm(scores):
    if not scores: return []
    ranks = rankdata(scores, method='average')
    return (ranks / len(scores)).tolist()

# --- è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆé«˜äº®æ‘˜è¦ ---
def generate_highlighted_snippet(text: str, query: str, snippet_length: int = 200) -> str:
    """
    ä»æ–‡æœ¬ä¸­æå–åŒ…å«å…³é”®è¯çš„æ‘˜è¦ç‰‡æ®µï¼Œå¹¶ç”¨ç‰¹æ®Šæ ‡è®°åŒ…è£¹å…³é”®è¯ä»¥ä¾¿å‰ç«¯é«˜äº®
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        query: æœç´¢æŸ¥è¯¢ï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªå…³é”®è¯ï¼‰
        snippet_length: æ‘˜è¦ç‰‡æ®µçš„æœ€å¤§é•¿åº¦
        
    Returns:
        åŒ…å«é«˜äº®æ ‡è®°çš„æ‘˜è¦ç‰‡æ®µï¼Œæ ¼å¼ï¼š...å‰æ–‡ [[HIGHLIGHT]]å…³é”®è¯[[/HIGHLIGHT]] åæ–‡...
    """
    if not text or not query:
        return text[:snippet_length] if text else ""
    
    text_lower = text.lower()
    query_lower = query.lower()
    
    # æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤å¸¸è§åœç”¨è¯ï¼‰
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}
    keywords = [word.strip() for word in re.split(r'[\s,\.;:]+', query_lower) 
                if word.strip() and word.strip() not in stop_words and len(word.strip()) > 2]
    
    if not keywords:
        keywords = [query_lower]
    
    # æŸ¥æ‰¾æ‰€æœ‰å…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
    positions = []
    for keyword in keywords:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„æœç´¢
        pattern = re.escape(keyword)
        for match in re.finditer(pattern, text_lower, re.IGNORECASE):
            positions.append((match.start(), match.end(), keyword))
    
    # æŒ‰ä½ç½®æ’åº
    positions.sort(key=lambda x: x[0])
    
    if not positions:
        # å¦‚æœæ²¡æ‰¾åˆ°å…³é”®è¯ï¼Œè¿”å›æ–‡æœ¬å¼€å¤´
        return text[:snippet_length]
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªåŒ¹é…ä½ç½®ä½œä¸ºä¸­å¿ƒï¼ˆæˆ–é€‰æ‹©åŒ…å«æœ€å¤šå…³é”®è¯çš„åŒºåŸŸï¼‰
    center_start, center_end, matched_keyword = positions[0]
    
    # å°è¯•æ‰¾åˆ°ä¸€ä¸ªåŒ…å«æ›´å¤šå…³é”®è¯çš„çª—å£
    # è®¡ç®—ä¸€ä¸ªå¯ä»¥åŒ…å«å¤šä¸ªå…³é”®è¯çš„çª—å£å¤§å°
    best_start = center_start
    best_end = min(len(text), center_start + snippet_length)
    
    # å°è¯•å‘åæ‰©å±•ï¼Œçœ‹çœ‹èƒ½å¦åŒ…å«æ›´å¤šå…³é”®è¯
    for pos_start, pos_end, _ in positions[1:]:
        if pos_end <= center_start + snippet_length:
            best_end = pos_end + snippet_length // 4  # æ‰©å±•ä¸€ç‚¹ä»¥åŒ…å«åé¢çš„å…³é”®è¯
    
    # è®¡ç®—æ‘˜è¦çš„èµ·å§‹ä½ç½®ï¼ˆå‘å‰æ‰©å±•ï¼‰
    snippet_start = max(0, best_start - snippet_length // 2)
    
    # è®¡ç®—æ‘˜è¦çš„ç»“æŸä½ç½®ï¼ˆå‘åæ‰©å±•ï¼‰
    snippet_end = min(len(text), best_end + snippet_length // 4)
    
    # å¦‚æœæ‘˜è¦ä»æ–‡æœ¬ä¸­é—´å¼€å§‹ï¼Œæ·»åŠ çœç•¥å·
    prefix = "..." if snippet_start > 0 else ""
    
    # å¦‚æœæ‘˜è¦æœªåˆ°è¾¾æ–‡æœ¬æœ«å°¾ï¼Œæ·»åŠ çœç•¥å·
    suffix = "..." if snippet_end < len(text) else ""
    
    # æå–æ‘˜è¦ç‰‡æ®µ
    snippet = text[snippet_start:snippet_end]
    
    # åœ¨æ‘˜è¦ä¸­é«˜äº®æ‰€æœ‰å…³é”®è¯
    highlighted_snippet = snippet
    for keyword in keywords:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        highlighted_snippet = pattern.sub(
            lambda m: f"[[HIGHLIGHT]]{m.group()}[[/HIGHLIGHT]]",
            highlighted_snippet
        )
    
    return prefix + highlighted_snippet + suffix

def extract_keywords_from_query(query: str) -> list:
    """
    ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯
    """
    # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œå¸¸è§åœç”¨è¯
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'what', 'where', 'when', 'why', 'how'}
    keywords = [word.strip() for word in re.split(r'[\s,\.;:]+', query.lower()) 
                if word.strip() and word.strip() not in stop_words and len(word.strip()) > 2]
    return keywords if keywords else [query.lower()]

# --- æ ¸å¿ƒæœç´¢å‡½æ•° ---
def search(query_text, top_k=10):
    print(f"\nğŸ” Searching for: '{query_text}' ...")

    # ---------------------------------------------------------
    # Layer 1: Vector Embedding (CLIP)
    # ---------------------------------------------------------
    inputs = clip_processor(text=[query_text], return_tensors="pt", padding=True)
    with torch.no_grad():
        query_vector = clip_model.get_text_features(**inputs)
        query_vector = query_vector / query_vector.norm(p=2, dim=-1, keepdim=True)
        query_vector = query_vector[0].numpy().tolist()

    # ---------------------------------------------------------
    # Layer 2: Qdrant Search (HNSW)
    # ---------------------------------------------------------
    # ç›´æ¥æŸ¥è¯¢ Space X (åŒ…å«æ‰€æœ‰å†…å®¹)
    try:
        hits = client.query_points(
            collection_name=SPACE_X,
            query=query_vector,
            using="clip",
            limit=top_k * 3  # å¤šå–ä¸€äº›ç”¨äºé‡æ’
        ).points
    except Exception as e:
        print(f"âŒ Qdrant search failed: {e}")
        return []

    # ---------------------------------------------------------
    # Layer 3: Fusion & Ranking & Safeguards
    # ---------------------------------------------------------
    results = []
    raw_sims = []
    raw_prs = []
    
    total_candidates = len(hits)

    for rank_idx, hit in enumerate(hits):
        hit_id = hit.id
        sim = hit.score
        payload = hit.payload

        # è·å–æƒå¨åº¦ (PageRank)
        pr = payload.get('pr_score', 0.0)

        # --- ç¬¬å››é“é˜²çº¿ï¼šä¸€è‡´æ€§æ ¡éªŒ (Consistency Check) ---
        # æ£€æŸ¥ CLIP æ’åä¸ DINO æ’åçš„å†²çª (Mock)
        is_consistent, conflict_loss = consistency_engine.check_consistency(
            query_text, payload, rank_idx, total_candidates
        )
        
        if not is_consistent:
            print(f"ğŸ›¡ï¸ [Circuit Breaker] Blocked ID {hit_id}: High Semantic-Visual Conflict (Loss: {conflict_loss:.2f})")
            continue

        raw_sims.append(sim)
        raw_prs.append(pr)

        results.append({
            "id": hit_id,
            "sim": sim,
            "pr": pr,
            "payload": payload,
            "conflict_loss": conflict_loss
        })

    # 4. å½’ä¸€åŒ–å¤„ç† (Gauss Rank)
    norm_sims = gauss_rank_norm(raw_sims)
    norm_prs = gauss_rank_norm(raw_prs)

    final_ranked = []

    # æƒé‡è®¾å®š
    w_sim = 0.7
    w_pr = 0.3

    for i, item in enumerate(results):
        # æœ€ç»ˆæ‰“åˆ†å…¬å¼
        final_score = w_sim * norm_sims[i] + w_pr * norm_prs[i]

        # è§£æå†…å®¹
        p = item['payload']
        content_type = p.get('type', 'unknown')
        url = p.get('url', '#')
        preview = p.get('content_preview', 'No preview')
        if isinstance(preview, list): preview = preview[0]
        
        # è·å–å®Œæ•´æ–‡æœ¬ç”¨äºç”Ÿæˆé«˜äº®æ‘˜è¦ï¼ˆä¼˜å…ˆä½¿ç”¨full_textï¼Œå¦åˆ™ä½¿ç”¨contentæˆ–content_previewï¼‰
        full_text = p.get('full_text', '') or p.get('content', '') or preview
        
        # ç”Ÿæˆé«˜äº®æ‘˜è¦
        highlighted_snippet = generate_highlighted_snippet(
            full_text if isinstance(full_text, str) else str(full_text), 
            query_text, 
            snippet_length=200
        )

        final_ranked.append({
            "score": final_score,
            "type": content_type,
            "url": url,
            "content": preview,
            "highlighted_snippet": highlighted_snippet,  # æ–°å¢ï¼šåŒ…å«é«˜äº®æ ‡è®°çš„æ‘˜è¦
            "id": item['id'],
            "is_exploration": False
        })

    # 5. æ’åº
    final_ranked.sort(key=lambda x: x['score'], reverse=True)
    
    # --- ç¬¬ä¸‰é“é˜²çº¿ (B)ï¼šæ¢ç´¢çº¢åˆ© (Exploration Bonus) ---
    # éšæœºæ’å…¥æ–°å†…å®¹ (Bandit ç®—æ³•)
    if random.random() < 0.05: # 5% æ¦‚ç‡è§¦å‘
        print("ğŸ² [Exploration] Triggering exploration mechanism, injecting new content...")
        # è¿™é‡Œç®€å•æ¨¡æ‹Ÿï¼šéšæœºå–ä¸€ä¸ªä½åˆ†ç»“æœæå‡åˆ°ç¬¬ 2 å
        if len(final_ranked) > 5:
            lucky_idx = random.randint(5, len(final_ranked)-1)
            lucky_item = final_ranked.pop(lucky_idx)
            lucky_item['is_exploration'] = True
            lucky_item['score'] += 0.5 # å¼ºè¡ŒåŠ åˆ†
            final_ranked.insert(1, lucky_item) # æ’å…¥åˆ°ç¬¬äºŒä½

    return final_ranked[:top_k]


# --- ç»“æœå±•ç¤º ---
def display_results(results):
    print("\n" + "=" * 40)
    print("      SEARCH RESULTS (Top 5)      ")
    print("=" * 40)

    if not results:
        print("No results found.")
        return

    for i, res in enumerate(results[:5]):
        icon = "ğŸ“„"
        if res['type'] == 'image':
            icon = "ğŸ–¼ï¸"
        elif res['type'] == 'audio':
            icon = "ğŸµ"
            
        explore_tag = " [ğŸ² EXPLORE]" if res.get('is_exploration') else ""

        print(f"{i + 1}. {icon} [{res['type'].upper()}]{explore_tag} (Score: {res['score']:.4f})")
        print(f"   ğŸ”— URL: {res['url']}")
        content_str = str(res['content'])
        print(f"   ğŸ“ Content: {content_str[:100]}...")
        print("-" * 40)


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    while True:
        q = input("\nè¯·è¾“å…¥æœç´¢å…³é”®è¯ (è¾“å…¥ 'exit' é€€å‡º): ")
        if q.lower() == 'exit': break

        results = search(q)
        display_results(results)